# 17일차 - 250723

# Gradient Boosting: Overview & Explanation

## Boosting

- Boosting은 여러 개의 약한 학습기(weak learner)를 조합하여 강한 학습기를 만드는 앙상블 방법이다.
- 각 학습기는 이전 학습기의 오류를 보완하도록 학습됨.

## Stagewise Additive Model

$$
\[
H(x) = \sum_{m=1}^M \alpha_m h_m(x)
\]
$$

- 반복적으로 학습하며 각 단계에서 기존 모델에 작은 기여(\( \alpha h(x) \))를 더함.
- 그리디 방식(Greedy Algorithm)으로 학습.


## Optimization in Function Space

- 함수 공간에서 손실 \( L(H) \)을 최소화하는 \( H \)를 찾는 것이 목표.
- Gradient Descent를 이용:

$$
\[
H_m = H_{m-1} - \alpha_m \left[ \frac{\partial L}{\partial H} \right]_{H=H_{m-1}}
\]
$$

- 초기 추정 \( h_1(x) \)부터 시작해 점진적으로 개선.


## Gradient Descent: Function Space

- 테일러 급수를 통해 1차 근사:

$$
\[
L(H + \alpha h) \approx L(H) + \left\langle \frac{\partial L}{\partial H}, \alpha h \right\rangle
\]
$$

- $( h )$는 $( -\frac{\partial L}{\partial H} )$와 높은 상관이 있는 방향으로 설정.


## Pseudo Response

- 각 데이터 샘플에 대해 $( -g_n = \frac{\partial L}{\partial H(x_n)} )$
- 이 값들을 최소화하는 새로운 $( h(x) )$를 학습.


## Square Loss에서의 Gradient Boosting

$$
\[
L = \frac{1}{2} \sum_{n=1}^N (y_n - H(x_n))^2
\]
$$

- $( -g_n = y_n - H(x_n) )$: 이는 residual과 동일.
- 이를 이용해 선형회귀 문제로 $( h(x) )$를 근사.


## Gradient Boosting 알고리즘 요약

1. 초기 모델 설정 $( H_0 = 0 )$
2. For $( m = 1 \) to \( M )$:
    - 잔차 계산: $( t_n = - \left[ \frac{\partial L(H(x_n))}{\partial H(x)} \right] )$
    - 회귀 트리 학습: $( h_m = \arg\min_h \sum (t_n - h(x_n))^2 )$
    - 학습률$(( \alpha ))$로 업데이트
    - $( H_m(x) = H_{m-1}(x) + \alpha h_m(x) )$


## Classification with Gradient Boosting

- 회귀 트리를 사용하여 확률적 분류를 수행
- Logit(odds) 변환:

$$
\[
\log(odds) = \log\left(\frac{p}{1 - p}\right)
\]
$$

$$
\[
p = \frac{1}{1 + e^{-\log(odds)}}
\]
$$

## Example: Weather Data

- 초기 확률은 전체 비율로 설정 \( p = 5/9 = 0.56 \)
- 이후 residual 계산: \( y_n - 0.56 \)
- 첫 번째 트리는 humidity로 분리, 두 번째는 wind gust로 분리


## 하이퍼파라미터

- `n_estimators`: 100 (default)
- `learning_rate`: 0.1 (default)
- `max_depth`: 3 (default)


## 장점과 단점

**Pros**
- 미분 가능한 손실 함수에 일반화 가능
- 높은 예측 성능
- 결측값 허용

**Cons**
- 이상치에 민감
- 과적합 위험
- 연산 비용이 높음


## 대표 라이브러리

- **XGBoost**: https://xgboost.readthedocs.io/en/stable/python/python_intro.html
- **LightGBM**: https://lightgbm.readthedocs.io/en/v3.3.2/
- **CatBoost**: https://github.com/catboost/catboost
