# 17일차 - 250723

# Gradient Boosting: Overview & Explanation

## Boosting

- Boosting은 여러 개의 약한 학습기(weak learner)를 조합하여 강한 학습기를 만드는 앙상블 방법이다.
- 각 학습기는 이전 학습기의 오류를 보완하도록 학습됨.

## Stagewise Additive Model

$$
\[
H(x) = \sum_{m=1}^M \alpha_m h_m(x)
\]
$$

- 반복적으로 학습하며 각 단계에서 기존 모델에 작은 기여(\( \alpha h(x) \))를 더함.
- 그리디 방식(Greedy Algorithm)으로 학습.


## Optimization in Function Space

- 함수 공간에서 손실 \( L(H) \)을 최소화하는 \( H \)를 찾는 것이 목표.
- Gradient Descent를 이용:

$$
\[
H_m = H_{m-1} - \alpha_m \left[ \frac{\partial L}{\partial H} \right]_{H=H_{m-1}}
\]
$$

- 초기 추정 \( h_1(x) \)부터 시작해 점진적으로 개선.


## Gradient Descent: Function Space

- 테일러 급수를 통해 1차 근사:

$$
\[
L(H + \alpha h) \approx L(H) + \left\langle \frac{\partial L}{\partial H}, \alpha h \right\rangle
\]
$$

- $( h )$는 $( -\frac{\partial L}{\partial H} )$와 높은 상관이 있는 방향으로 설정.


## Pseudo Response

- 각 데이터 샘플에 대해 $( -g_n = \frac{\partial L}{\partial H(x_n)} )$
- 이 값들을 최소화하는 새로운 $( h(x) )$를 학습.


## Square Loss에서의 Gradient Boosting

$$
\[
L = \frac{1}{2} \sum_{n=1}^N (y_n - H(x_n))^2
\]
$$

- $( -g_n = y_n - H(x_n) )$: 이는 residual과 동일.
- 이를 이용해 선형회귀 문제로 $( h(x) )$를 근사.


## Gradient Boosting 알고리즘 요약

1. 초기 모델 설정 $( H_0 = 0 )$
2. For $( m = 1 \) to \( M )$:
    - 잔차 계산: $( t_n = - \left[ \frac{\partial L(H(x_n))}{\partial H(x)} \right] )$
    - 회귀 트리 학습: $( h_m = \arg\min_h \sum (t_n - h(x_n))^2 )$
    - 학습률$(( \alpha ))$로 업데이트
    - $( H_m(x) = H_{m-1}(x) + \alpha h_m(x) )$


## Classification with Gradient Boosting

- 회귀 트리를 사용하여 확률적 분류를 수행
- Logit(odds) 변환:

$$
\[
\log(odds) = \log\left(\frac{p}{1 - p}\right)
\]
$$

$$
\[
p = \frac{1}{1 + e^{-\log(odds)}}
\]
$$

## Example: Weather Data

- 초기 확률은 전체 비율로 설정 \( p = 5/9 = 0.56 \)
- 이후 residual 계산: \( y_n - 0.56 \)
- 첫 번째 트리는 humidity로 분리, 두 번째는 wind gust로 분리


## Important Hyperparameter(하이퍼 파라미터)

- number of tree
- learning rate
- maximun depth of tree

- `n_estimators`: 100 (default)
- `learning_rate`: 0.1 (default)
- `max_depth`: 3 (default)


## Gradient Boosting(장점과 단점)

**Pros**
- 미분 가능한 손실 함수에 일반화 가능
- 높은 예측 성능
- 결측값 허용

**Cons**
- 이상치에 민감
- 과적합 위험
- 연산 비용이 높음


## 대표 라이브러리

- **XGBoost**: https://xgboost.readthedocs.io/en/stable/python/python_intro.html
- **LightGBM**: https://lightgbm.readthedocs.io/en/v3.3.2/
- **CatBoost**: https://github.com/catboost/catboost


---


## AdaBoost: Adaptive Boosting (for binary classification)

> Yoav Freund and Robert E. Schapire (1997),  
> "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,"  
> *Journal of Computer and System Sciences*


## Stumps as Weak Learners

- **Tree Stump**: A cut-down tree (illustrative image)
- **Decision Stump**: A one-level decision tree used as a weak learner in AdaBoost


## AdaBoost: Example

- Combine multiple weak learners $( h_1(x), h_2(x), h_3(x) )$
- Final hypothesis:
  $$
  h(x) = h_1(x) + h_2(x) + h_3(x)
  $$

Visualization of decision boundaries built sequentially.


## Summary

- Regression and classification trees
- Random forest
- Gradient boosted trees



---

# 📊 Clustering

## 개념 정리

- **Clustering**은 라벨이 없는 데이터(`unlabelled data`)를 다루는 기법이다.
- **Unsupervised** 학습의 한 방법으로 분류할 수 있다.
- 데이터들을 미리 정해진 그룹 수로 나누는데, 같은 그룹의 데이터는 서로 유사하도록 구성된다.

### 그룹 간 유사성 기준:
- **High intra-class similarity** (그룹 내 유사성 높음)
- **Low inter-class similarity** (그룹 간 유사성 낮음)

> 💡 서로 다른 **distance measures (거리 척도)**는 클러스터링 결과에 큰 영향을 준다.


## 📌 수식 표현

예를 들어, 유클리디안 거리 기반의 클러스터 중심 업데이트는 다음과 같다:

$$
\mu_k = \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i
$$


## 🧠 Applications of Clustering

### ✅ 이미지 처리
- **Image Segmentation**: 이미지를 구성하는 영역들을 비슷한 픽셀 값으로 나눈다.
- **SDD Image Clustering**: 구조적 데이터/패턴 기반 클러스터링.

### ✅ 마케팅 & 사회 분석
- **Targeted Marketing**: 고객층을 다양한 성향으로 분류해 마케팅 전략 수립.
- **Community Detection in Social Media**: 유사 관심사를 가진 사용자 그룹 탐색.

---




# 📌 k-Means Clustering
## 🎯 목표

- $k$-means의 목표는 $K$개의 중심 벡터(프로토타입 벡터) $\mu_k$를 찾는 것.
- 데이터 집합: $\{x_n \in \mathbb{R}^D | n=1,...,N\}$
- 각 클러스터 중심 계산:

$$
\mu_k = \frac{1}{N_k} \sum_{n \in C_k} x_n
$$

- 거리 기반 오류 함수 최소화:

$$
\mathcal{J} = \sum_{k=1}^{K} \sum_{n \in C_k} \|x_n - \mu_k\|^2
$$


## 🧠 알고리즘 단계

### 1. 할당 단계 (Assignment step)

- 각 데이터 포인트를 가장 가까운 중심에 할당:

$$
\|x_n - \mu_k\|^2 \text{가 최소인 } k
$$

### 2. 업데이트 단계 (Update step)

- 각 클러스터에 대해 새로운 중심 계산:

$$
\mu_k^{\text{new}} = \frac{1}{N_k} \sum_{n \in C_k} x_n
$$

- 이 두 단계를 수렴할 때까지 반복


## 🔄 대안적 표현 방식

- 클러스터 책임 변수 $r_{k,n}$ 도입:

$$
r_{k,n} = 
\begin{cases}
1, & \text{if } \arg\min_j \|x_n - \mu_j\|^2 = k \\
0, & \text{otherwise}
\end{cases}
$$

- 중심 업데이트 방식:

$$
\mu_k = \frac{\sum_{n=1}^{N} r_{k,n} x_n}{\sum_{n=1}^{N} r_{k,n}}
$$


## 🧩 혼합정수 최적화 (Mixed-Integer Optimization)

- $k$-means는 다음과 같은 최적화 문제로 표현 가능:

$$
\min_{\mathbf{R}} \mathcal{J} = \sum_{k=1}^{K} \sum_{n=1}^{N} r_{k,n} \|x_n - \mu_k\|^2
$$

subject to:

$$
\mathbf{R} = [r_{k,n}] \in \left\{ \mathbf{R} \in \{0,1\}^{K \times N} \middle| \sum_{k=1}^{K} r_{k,n} = 1 \right\}
$$

- 2단계 최적화:
  - $r_{k,n}$ 고정 후 $\mu_k$ 최적화
  - $\mu_k$ 고정 후 $r_{k,n}$ 최적화


## 🧪 한계점 (Limitations)

- 초기 조건에 민감함
- 유클리드 거리 기반으로 볼록(Convex) 형태의 클러스터에만 적합
- $k$ 값 선택에 원칙이 없음


## 🧠 Elbow Method

- 군집 내 제곱합(WCSS: Within-cluster-sum-of-squares)을 기준으로 최적의 $k$ 탐색
- 그래프에서 꺾이는 점이 최적 $k$:

```plaintext
k vs WCSS plot → 꺾이는 지점이 elbow point
```


## 📌 비선형 구조에 대한 한계

- 예: Two Moons 데이터에 $k$-means 적용 시 경계 부정확


## 📚 참고

- Bishop PRML, 다양한 이미지 분할 사례












