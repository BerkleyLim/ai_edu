# 12일차 - 250717

# 📘 Feature (Variable) Selection 정리

Seungjin Choi

---

## 1. 선형 회계와 과적합 문제

* **선형 회계 모델**:

  $$
  $$

y = w\_0 + w\_1 x\_1 + w\_2 x\_2 + \cdots + w\_D x\_D + \varepsilon

$$

- **예측 정확도 문제**  
- 관찰치 수 $N \ll D$ → 모델이 과적합되어 예측 성능 저하
- $D > N$이면 해가 무한히 많아지고, 테스팅세트 성능이 매우 낮아짐

- **모델 해석 가능성 문제**  
- 연관 없는 변수가 많을 경우 모델 해석이 ì \xec96b아지며
- 불필요한 변수를 제거하면 더 직관적인 모델이 가능

---

## 2. Linear Model Selection & Regularization

1. **Subset Selection**  
 - Best Subset Selection  
 - Forward Stepwise Selection  
 - Backward Stepwise Selection

2. **Regularization (Shrinkage)**  
 - Ridge Regression  
 - LASSO Regression

3. **차원 축소**  
 - Principal Component Regression (PCR)  
 - Partial Least Squares (PLS)

---

## 3. 테스트 오차 추정 (Test Error Estimation)

### 1) 간적 추정
- 학습 오차를 기반으로 bias 복정:
- Mallow’s $C_p$
- AIC (Akaike Information Criterion)
- BIC (Bayesian Information Criterion)
- Adjusted $R^2$

### 2) 직접 추정
- Validation Set 또는 Cross Validation 사용

---

## 4. Mallow’s $C_p$

- 정의:
$$

C\_p = \frac{\text{SSE}}{\hat{\sigma}^2} + 2d - N

$$
$$

\text{or} \quad
C\_p = \frac{1}{N} \left( \text{SSE} + 2d \hat{\sigma}^2 \right)

$$

- 용도:
- 학습 오차가 테스트 오차를 가속 추정하는 것을 복정
- $\hat{\sigma}^2$: 전체 변수(full model)에서 추정한 오차 범수

---

## 5. AIC & BIC

- **AIC**:  
$$

\text{AIC} = -2 \log L + 2d

$$
$$

\text{(Gaussian 가정 시)} \quad
\text{AIC} = \frac{\text{SSE}}{\hat{\sigma}^2} + 2d

$$

- **BIC**:  
$$

\text{BIC} = -2 \log L + d \log N

$$

- $L$: 모델의 likelihood 최대값

---

## 6. $R^2$ 및 Adjusted $R^2$

- $R^2$:
$$

R^2 = 1 - \frac{\text{SSE}}{\text{SST}} = \frac{\text{SST} - \text{SSE}}{\text{SST}}

$$

- Adjusted $R^2$:
$$

\bar{R}^2 = R^2 - (1 - R^2) \cdot \frac{D}{N - D - 1}

$$
- 변수 개수가 늘어날때 $R^2$는 무조건 증가하무로 복정 필요

---

# 📊 Subset Selection Algorithms

---

## 7. Best Subset Selection

- 모든 $2^D$ 조합을 계산하여 가장 성능 좋은 모델 선택
- 계산 비용 \ae09증:
- $D=10$: 1024개
- $D=20$: 약 100만
- $D=40$: 약 1조 → 현실적으로 불가능

### 알고리즘 요약

```
1. Null model (변수 없음)로 시작
2. d = 1 ~ D 에 대해:
 - d개 변수 조합 모두 학습
 - 그 중 SSE 가장 낮은 모델 Md 선택
3. Md 중 가장 좋은 모델 1개 선택 (Cp, AIC, BIC, Adjusted R^2 기준)
```

---

## 8. Forward Stepwise Selection

- 변수 없이 시작 → 하나씩 추가
- 매 단계마다 성능 향상이 가장 큰 변수만 추가
- 계산량: 약 $D^2$

### 알고리즘 요약

```
1. Null model로 시작
2. 매 단계마다:
 - 남은 변수 중 하나씩 추가
 - SSE 또는 R^2 가장 감소된 모델 선택
3. 최종 선택: Cp, AIC, BIC 등으로 결정
```

---

## 9. Backward Stepwise Selection

- Full model (모든 변수 포함)에서 시작 → 변수 하나씩 제거
- 성능이 가장 보유되는 변수 제거

### 알고리즘 요약

```
1. Full model(MD)로 시작
2. 매 단계마다:
 - 하나씩 변수 제거
 - SSE 가장 낮은 모델 선택
3. 최종 선택: Cp, AIC, BIC, Adjusted R^2 등으로 판단
```

---

## 🔺 참고 문학

- **Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023)**  
*An Introduction to Statistical Learning, 2nd ed.*

- [ISLR2 Dataset 리뷰](https://r4ds.github.io/bookclub-islr/datasets-provided-in-the-islr2-package.html)

$$
