# 15일차 - 250721

# Decision Tree (의사 결정 나무)
- 지도 학습(Supervised Learning) 중 하나
- 분류 (Classification) or 회귀(Regression)문제 해결할 수 있는 기본적이면서도 강력한 알고리즘


# 🌳 Decision Tree (의사결정나무)

## 📌 1. 개요 (Overview)

Decision Tree는 데이터를 분할하여 예측을 수행하는 트리 기반 모델입니다. 각 **노드**는 하나의 **조건**을 기준으로 데이터를 분기시키며, **리프 노드**는 예측값을 나타냅니다.

- **입력**: 특성 벡터 $x$
- **출력**: 예측값 $\hat{y}$ (분류 or 회귀)


## 🌺 2. 예시: 아이리스 꽃 분류

- 데이터셋: Iris 데이터
- 특성:
  - 꽃잎 길이, 너비
  - 꽃받침 길이, 너비
- 라벨: 품종 (Setosa, Versicolor, Virginica)

```csv
sepal_length, sepal_width, petal_length, petal_width, class
5.1, 3.5, 1.4, 0.2, setosa
...
```


## 🌲 3. 트리 구조 예시

```text
            petal_length <= 2.45
               /        \
           setosa     petal_width <= 1.75
                          /       \
                     versicolor   virginica
```

- 조건에 따라 왼쪽/오른쪽으로 분기
- 리프 노드에 도달하면 예측 종료


## 📈 4. Regression Tree vs Linear Model

- **선형 모델**: 전체 영역을 직선으로 예측
- **회귀 트리**: 각 구간별로 값 예측 (계단형)

> 회귀 트리는 데이터의 비선형 분포도 잘 대응 가능


## 🧠 5. Classification Tree 이론

- 목표: 데이터셋 $\{(x_i, y_i)\}_{i=1}^n$에서 $y_i \in \{1, ..., C\}$
- 트리 분할 기준: **불순도 (Impurity)** 최소화

### 주요 분할 기준

- **Gini 지수**
- **Entropy (정보 이득)**
- **분산 (회귀의 경우)**


## 🧮 6. Gini Impurity (지니 불순도)

클래스 비율 $p_k$에 대해:

$$
\text{Gini}(t) = 1 - \sum_{k=1}^K p_k^2
$$

$$
G(S) = 1 - p^2 - (1 - p)^2 = 2p(1 - p)
$$

- p: 특정 클래스에 속할 확률
- 최대 불순도 p = 0.5 일때 G(S) = 0.5

- 불순도가 낮을수록 하나의 클래스가 우세함
- 트리 학습 시 Gini 값을 최소화하는 분기를 선택


$$
G(S) = \frac{|S_L|}{|S|} G(S_L) + \frac{|S_R|}{|S|} G(S_R)
$$

### 📘 각 기호의 의미

- $( S )$: 전체 데이터 집합  
- $( S_L )$: 왼쪽 서브트리 데이터 집합  
- $( S_R )$: 오른쪽 서브트리 데이터 집합  
- $( |S| )$: 전체 샘플 수  
- $( \frac{|S_L|}{|S|} )$: 왼쪽 서브트리의 비율  
- $( \frac{|S_R|}{|S|} )$: 오른쪽 서브트리의 비율  
- $( G(S_L),\ G(S_R) )$: 각 서브트리의 Gini 불순도  


### 🔍 조건

- \( S = S_L \cup S_R \)  
- \( S_L \cap S_R = \emptyset \) (서브트리는 겹치지 않음)


## 🧪 7. Gini 분할 예시

### Case 1

| 구간 | 클래스 비율 |
|------|--------------|
| 왼쪽 | [3, 3] → Gini = 0.5 |
| 오른쪽 | [4, 0] → Gini = 0 |

→ 가중 평균 Gini 계산 후 가장 낮은 경우 선택


## 🔢 8. 회귀 트리 (Regression Tree)

- 목표: 연속형 변수 예측
- 분기 기준: **평균 제곱 오차(MSE)** 최소화

$$
\text{MSE} = \sum_{i \in \text{left}} (y_i - \bar{y}_{left})^2 + \sum_{i \in \text{right}} (y_i - \bar{y}_{right})^2
$$


## 📦 9. 앙상블 (Ensemble) 기법

의사결정나무는 단독으론 과적합될 수 있음 → 여러 개의 트리를 결합해 **강한 예측기**를 만드는 것이 앙상블 기법입니다.

### 종류

- **Bagging** (ex: Random Forest)
  - 다수의 트리 평균 (분산 감소)
- **Boosting** (ex: XGBoost)
  - 순차적으로 오차 보정 (편향 감소)
- **Stacking**
  - 다른 모델의 결과를 모아서 최종 예측


## ✅ 장점

- 직관적이고 해석 용이
- 수치형/범주형 모두 사용 가능
- 사전 전처리 거의 없음

## ❌ 단점

- 쉽게 과적합됨 (특히 깊은 트리)
- 작은 데이터 변경에도 구조 변화 큼


## 📚 주요 라이브러리

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import DecisionTreeRegressor
```


### 🔚 요약

| 항목 | 설명 |
|------|------|
| 목적 | 분류 / 회귀 |
| 분기 기준 | Gini, Entropy, MSE 등 |
| 장점 | 해석 용이, 빠름 |
| 단점 | 과적합 가능성 |


---



# Ensembles ()