# 16일차 - 250722

# 🌳 Bagging & Random Forest

출처:  
- Leo Breiman (1996), *"Bagging predictors"*, Machine Learning Journal.  
- Leo Breiman (2001), *"Random forests"*, Machine Learning Journal.

## ❓ Question

> **Given a single dataset, how can we construct multiple models?**

## 🔁 Bootstrapping

- 원본 데이터를 **복원 추출**(with replacement)을 통해 여러 부트스트랩 샘플로 나눕니다.
- 각 샘플은 원본과 크기가 같지만 일부 중복된 데이터가 존재합니다.

## 🧩 Bagging (Bootstrap Aggregating)

### 📌 아이디어
> 여러 부트스트랩 샘플로 각각 모델을 학습시키고,  
> **예측 결과를 평균 또는 다수결로 집계(aggregating)** 하여 최종 예측을 생성합니다.

### 🔧 알고리즘

1. 데이터셋 $D$에서 복원 추출하여 $M$개의 데이터셋 $D_1, \ldots, D_M$을 생성  
2. 각 데이터셋 $D_m$에 대해 예측 모델 $h_m(x)$ 학습  
3. 최종 모델:

$$
h(x) = \frac{1}{M} \sum_{m=1}^{M} h_m(x)
$$

### ✅ 효과
- **분산 감소**:
$$
\mathbb{E}_D[(h_D(x) - \bar{h}(x))^2]
$$
는 감소하게 됨  
($M \to \infty$일 때 약한 법칙에 의해 수렴)

## ⚖️ Bias-Variance Tradeoff

- Bagging은 **분산(variance)** 을 줄여줍니다.
- Boosting은 **편향(bias)** 을 줄여줍니다.

> 따라서, Bagging은 분산 감소에 초점을 맞춘 앙상블 방식입니다.

## 🌲 Random Forest

> **Random Forest**는 Bagging 기반의 앙상블 학습법으로,  
> 각 결정트리에 **무작위성(randomness)** 을 추가하여 상호 상관을 줄이는 방식입니다.

### 🧠 핵심 개념

- Random Forest = **Bagged decision trees** + **Random feature subset**

### 🔧 알고리즘

1. 데이터셋 $D$에서 복원 추출하여 $M$개의 데이터셋 $D_1, \ldots, D_M$ 생성
2. 각 $D_m$에 대해 다음을 수행:
    - 전체 특성 중에서 무작위로 $k$개 선택  
      (예: $k \leq \sqrt{d}$)
    - 해당 특성 중에서 best split을 찾아 결정트리 학습
3. 최종 예측은 다음과 같이 집계:

- **회귀 (Regression)**:
$$
h(x) = \frac{1}{M} \sum_{m=1}^{M} h_m(x)
$$

- **분류 (Classification)**:
$$
h(x) = \text{sign} \left( \frac{1}{M} \sum_{m=1}^{M} h_m(x) \right)
$$

### 🧪 기타 특징

- **Out-of-bag (OOB) error**:
  - 각 모델이 사용하지 않은 데이터로 성능 평가 가능

- **결측값 처리 (Missing values)**:
  - 초기값: 평균 또는 중앙값 대입  
  - 개선: Random forest 기반 보정

- **각 트리는 충분한 깊이로 성장**하여 훈련됨



---

# 🌲 Extremely Randomized Trees (Extra Trees)

> Pierre Geurts, Damien Ernst, Louis Wehenkel (2006),  
> *“Extremely randomized trees”*, Machine Learning

## 📌 Extra Trees vs Random Forest

| 항목 | Random Forest | Extra Trees |
|------|---------------|-------------|
| 샘플링 방식 | Bootstrap 샘플 사용 | 전체 원본 데이터 사용 |
| 분할 기준 | 무작위 선택된 피처들 중 최적 기준 선택 | 무작위 피처 + **무작위 임계값** 선택 |
| 분할 최적화 | Gini impurity 또는 entropy 기준 | 분할 조건 자체가 **랜덤** |

## ⚡ 속도 및 성능 특성

- Extra Trees는 **Random Forest보다 빠름**
  - 분할 기준 계산이 단순 (무작위 선택)
- 일반적으로 **더 낮은 분산(Variance)** 을 가짐
- 성능은 Random Forest와 유사하거나 더 좋을 수 있음

## 🔍 예측 불확실성 (Predictive Uncertainty)

> 전체 불확실성 = **Aleatoric + Epistemic**

### 🎲 Aleatoric Uncertainty
- 데이터 자체의 노이즈
- 반복 측정해도 사라지지 않음
- **측정 자체의 한계**로 인한 불확실성

### 🧠 Epistemic Uncertainty
- **모델의 무지(미지의 영역)**
- 학습 데이터 분포 밖에서는 높은 불확실성
- 데이터를 추가 학습시키면 줄어듦

## 📈 예측 평균 및 분산 계산

### ✅ 예측 평균 (Sample Mean)
\[
\hat{f}(x) = \frac{1}{M} \sum_{m=1}^{M} h_m(x)
\]

### 📉 예측 분산 (Epistemic Uncertainty)
\[
\hat{\sigma}^2(x) = \frac{1}{M-1} \sum_{m=1}^{M} \left( h_m(x) - \hat{f}(x) \right)^2
\]

## 🧩 Ensemble of Extra Trees

- 동일한 데이터셋을 기반으로 **여러 개의 ExtraTree** 모델 생성
- 각 트리는 **서로 다른 무작위성(randomness)** 을 갖고 훈련됨
- **앙상블 평균 및 분산**을 통해 예측과 신뢰도 추정 가능

## 📊 시각적 비교

| 모델 | 설명 |
|------|------|
| (a) Random Forest | 예측값과 분산(음영) |
| (b) Extra Trees | 더 랜덤한 분할 기준 |
| (c) Random Forest (복잡한 함수 예측) | 고차 패턴 대응 |
| (d) GP Regression | 베이지안 기반 불확실성 모델 |

> 오렌지색 음영 영역: **예측 분산**을 시각화한 것

---




# 🚀 Boosting

> Yoav Freund and Robert E. Schapire (1997),  
> *“A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting”*,  
> Journal of Computer and System Sciences

## ❓ 핵심 질문

> Can a set of weak learners create a single strong learner?  
> → **Yes, Boosting!** (Robert Schapire)

- **Weak Learners**: 랜덤 추정보다 약간 나은 모델들
- Boosting은 이들을 순차적으로 결합해 **강력한 모델**을 생성함

## 🔁 Boosting 아이디어

- **Additive Ensemble**: 이전 모델의 오차를 보완하는 방식으로 모델을 순차적으로 학습시킴  
- 이전 모델이 **잘못 예측한 샘플**에 더 집중하여 **새로운 모델**을 훈련함  
- 최종 모델은 모든 약한 모델들의 결합 (가중합 또는 평균)

```
Model 1 → Model 2 → Model 3 → ... → Final Strong Model
```

## 🌲 Gradient Boosted Trees for Regression

### 기본 구조

- 초기 모델:  
  \[
  \hat{y}^{(0)} = \arg \min_c \sum (y_i - c)^2
  \]

- 다음 모델은 이전 모델의 **residual(잔차)** 를 학습함:  
  \[
  r_i^{(1)} = y_i - \hat{y}_i^{(0)}
  \]

- 새 모델은 잔차를 예측하고 결과에 반영:  
  \[
  \hat{y}_i^{(1)} = \hat{y}_i^{(0)} + \alpha f_1(x_i)
  \]

## 🧮 Gradient Boosting: Residual 기반 반복 학습

1. 초기 모델 학습:
   \[
   \hat{f}^{(0)}(x) = \arg \min_c \sum L(y_i, c)
   \]

2. 반복:
   - 잔차 계산:
     \[
     r_i^{(m)} = - \left[ \frac{\partial L(y_i, \hat{f}^{(m-1)}(x_i))}{\partial \hat{f}(x_i)} \right]
     \]
   - 잔차를 학습하는 모델 훈련:
     \[
     h_m(x) \approx r_i^{(m)}
     \]
   - 예측 모델 업데이트:
     \[
     \hat{f}^{(m)}(x) = \hat{f}^{(m-1)}(x) + \alpha h_m(x)
     \]

3. 위 과정을 M번 반복하여 최종 모델 완성

## 📊 시각화 예시

- 초기 예측은 단순한 직선 (평균값 등)
- 잔차를 통해 점점 복잡한 구조를 학습
- 각 단계별 예측 보정이 시각적으로 표시됨 (점선: 실제값, 실선: 예측값)

> ✅ Boosting은 단계별로 예측 오차를 줄이는 방식  
> ⛏️ 각 단계는 잔차(residual)를 모델링하여 개선함

---


