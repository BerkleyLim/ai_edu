# 10ì¼ì°¨ - 250715

# Review ë³´ê¸° - Linear Regression
![img.png](250715_1.png)

---

# ðŸ“Œ Maximum Likelihood ê´€ì ì—ì„œì˜ OLS

## ðŸ“˜ ëª¨ë¸ ê°€ì •

- ëª©í‘œ ë³€ìˆ˜ \( y_n \)ì€ ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ëª¨ë¸ë¡œ ìƒì„±ëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤:

$$
y_n = \mathbf{w}^\top \phi(\mathbf{x}_n) + \epsilon_n
$$

- ì—¬ê¸°ì„œ:
  - \( \phi(\mathbf{x}_n) \): ìž…ë ¥ \( \mathbf{x}_n \)ì— ëŒ€í•œ íŠ¹ì„± ë³€í™˜(feature transformation)
  - \( \epsilon_n \sim \mathcal{N}(0, \sigma^2) \): í‰ê·  0, ë¶„ì‚° \( \sigma^2 \)ì¸ ê°€ìš°ì‹œì•ˆ ìž¡ìŒ
  - \( \mathbf{w} \): í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° ë²¡í„°

- ë²¡í„° í˜•íƒœë¡œ í‘œí˜„í•˜ë©´:

$$
\mathbf{y} = \Phi \mathbf{w} + \boldsymbol{\epsilon}
$$

- í™•ë¥  ëª¨ë¸ ê´€ì ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë©ë‹ˆë‹¤:

$$
p(\mathbf{y} | \Phi, \mathbf{w}) = \mathcal{N}(\Phi \mathbf{w}, \sigma^2 \mathbf{I})
$$


## ðŸ“ˆ ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜ (Log-Likelihood)

- ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
\mathcal{L} = \log p(\mathbf{y} | \Phi, \mathbf{w}) = \sum_{n=1}^{N} \log p(y_n | \phi(\mathbf{x}_n), \mathbf{w})
$$

- ì´ë¥¼ ì •ë¦¬í•˜ë©´:

$$
\mathcal{L} = -\frac{N}{2} \log \sigma^2 - \frac{N}{2} \log 2\pi - \frac{1}{2\sigma^2} \mathcal{J}_{LS}
$$

- ì—¬ê¸°ì„œ \( \mathcal{J}_{LS} \): OLSì—ì„œì˜ ì œê³±í•© ì˜¤ì°¨(Sum of Squared Errors)

### ì°¸ì¡°
![img.png](250715_2.png)

### likelihoodê°€ ìžˆëŠ”ë° log-likelihoodë¥¼ ë³„ë„ë¡œ ì“°ëŠ” ì´ìœ ?
![img_1.png](250715_7.png)




## ðŸ§  ìµœëŒ€ìš°ë„ ì¶”ì • (Maximum Likelihood Estimation, MLE)

- MLEëŠ” ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° \( \mathbf{w} \)ë¥¼ ì°¾ëŠ” ê²ƒ:

$$
\mathbf{w}_{ML} = \arg\max_{\mathbf{w}} \log p(\mathbf{y} | \Phi, \mathbf{w})
$$

- ê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì— ë„ë‹¬í•©ë‹ˆë‹¤:

$$
\mathbf{w}_{ML} = \mathbf{w}_{OLS}
$$

> âœ… ì¦‰, **ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ê°€ì •** í•˜ì—ì„œ MLEëŠ” OLS í•´ì™€ ë™ì¼í•©ë‹ˆë‹¤.


---


# ðŸ“Œ Regularization

- ëª¨ë“  ë”¥ëŸ¬ë‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ ê¹”ê³  ì‹œìž‘í•œë‹¤!!
- **Ridge Regression**: $\ell_2$ norm regularization  
- **LASSO**: $\ell_1$ norm regularization


## â“ Why Regularization?

- ëª©ì : ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ë‚˜ì˜¨ê²ƒ!!

> ë³µìž¡í•œ ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©(overfitting)ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.  
> Regularizationì€ ì´ëŸ° **ê³¼ì í•© ë¬¸ì œë¥¼ ì™„í™”**í•˜ì—¬ ë” **ì¼ë°˜í™” ê°€ëŠ¥í•œ ëª¨ë¸**ì„ í•™ìŠµí•˜ê²Œ ë„ì™€ì¤ë‹ˆë‹¤.

ì˜ˆì‹œë¡œ, ê³ ì°¨ ë‹¤í•­ì‹ì„ ì‚¬ìš©í•œ ëª¨ë¸ì€ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì™„ë²½í•˜ê²Œ ë§žì§€ë§Œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” ì˜ˆì¸¡ì´ í¬ê²Œ ë²—ì–´ë‚  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì ì ˆí•œ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ì€ ì•½ê°„ì˜ ì˜¤ì°¨ë¥¼ ê°ìˆ˜í•˜ë”ë¼ë„ ë³´ë‹¤ ë¶€ë“œëŸ½ê³  ì¼ë°˜í™”ëœ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ê²Œ ë©ë‹ˆë‹¤.


![img.png](250715_3.png)
> Regularizationì€ í•™ìŠµëœ ëª¨ë¸ì˜ **ì¼ë°˜í™” ì„±ëŠ¥**(generalization)ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.



## âš™ï¸ Regularization ê°œë…

### ðŸ§  ëª©ì 
- ìž…ë ¥ $\mathbf{x}$ì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ ì¶”ë¡ í•˜ê³ ìž í•©ë‹ˆë‹¤.
- í•™ìŠµ ë°ì´í„°:  
  $$
  \mathcal{D} = \{ (\mathbf{x}_n, y_n) \}_{n=1}^{N}
  $$

### ðŸ§® ì†ì‹¤ í•¨ìˆ˜
- ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜: $\ell(f(\mathbf{x}_n; \mathbf{w}), y_n)$  
- ì˜ˆ: Least Squares (LS) ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\sum_{n=1}^N \ell(f(\mathbf{x}_n; \mathbf{w}), y_n) = \frac{1}{2N} \|\mathbf{y} - \Phi \mathbf{w} \|^2
$$


## ðŸ§© Regularizer ì¶”ê°€

ëª¨ë¸ì˜ **ë³µìž¡ë„(complexity)** ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´, ì†ì‹¤ í•¨ìˆ˜ì— íŒ¨ë„í‹° í•­ì„ ì¶”ê°€í•©ë‹ˆë‹¤:

$$
\underbrace{\sum_{n=1}^N \ell(f(\mathbf{x}_n; \mathbf{w}), y_n)}_{\text{loss}} + \lambda \underbrace{R(f)}_{\text{regularizer}}
$$

- $\lambda$: ì •ê·œí™” í•­ì˜ ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°**


## ðŸ” ëª¨ë¸ ë³µìž¡ë„ì™€ ê³„ìˆ˜ ë³€í™” ì˜ˆì‹œ

- ë‹¤ìŒ ê·¸ë¦¼ì€ ëª¨ë¸ ì°¨ìˆ˜ $M$ì— ë”°ë¥¸ íšŒê·€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:

| ëª¨ë¸ ì°¨ìˆ˜ | ì˜ˆì‹œ |
|-----------|------|
| $M = 1$   | (a) ì„ í˜• íšŒê·€ (ê³¼ì†Œì í•©) |
| $M = 3$   | (b) ì ì ˆí•œ ëª¨ë¸ |
| $M = 9$   | (c) ê³¼ì í•© |
| ê³„ìˆ˜ ë³€í™” | (d) íšŒê·€ ê³„ìˆ˜ í¬ê¸°ê°€ ë§¤ìš° ì»¤ì§ |

![img.png](250715_4.png)

## ðŸ“Š íšŒê·€ ê³„ìˆ˜ ë¹„êµ (ì˜ˆì‹œ)

| ê³„ìˆ˜ | $M=0$ | $M=1$ | $M=3$ | $M=9$ |
|------|-------|--------|--------|--------|
| $w_0$ | 0.1861 | 1.0977 | 0.0880 | -8.1 |
| $w_1$ |       | -1.8913 | 9.9135 | 401.2 |
| $w_2$ |       |        | -29.8721 | -6326.3 |
| $w_3$ |       |        | 20.1642 | 49778.9 |
| $w_4$ |       |        |         | -222555.2 |
| $w_5$ |       |        |         | 599603.0 |
| $w_6$ |       |        |         | -990507.7 |
| $w_7$ |       |        |         | 980248.7 |
| $w_8$ |       |        |         | -523726.3 |
| $w_9$ |       |        |         | 122122.1 |

> ë†’ì€ ì°¨ìˆ˜ì¼ìˆ˜ë¡ ê³„ìˆ˜ì˜ í¬ê¸°ê°€ ê¸‰ê²©ížˆ ì»¤ì§€ë©°, ì´ëŠ” ê³¼ì í•©(overfitting)ì˜ ì›ì¸ì´ ë©ë‹ˆë‹¤.


---


# ðŸ“˜ Ridge Regression

![img.png](250715_5.png)

- ëžŒë‹¤ $ \lambda $ ê°€ ì»¤ì§ˆìˆ˜ë¡ ê°•í•œ regularization

## ðŸ“€ ì •ì˜

Ridge Regressionì€ \$\ell\_2\$ norm ì •ê·œí™”ë¥¼ ì ìš©í•œ ì„ í˜• íšŒê·€ìž…ë‹ˆë‹¤.

ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 + \frac{\lambda}{2} \|\mathbf{w}\|^2
$$

í•´ë‹¹ ì‹ìœ¼ë¡œë¶€í„° í•´ëŠ” ì•„ëž˜ì™€ ê°™ì´ ìœ ë„ë©ë‹ˆë‹¤:

$$
\mathbf{w}_{\text{ridge}} = \left( \Phi^\top \Phi + \lambda I \right)^{-1} \Phi^\top \mathbf{y}
$$

ë˜ëŠ” ë‹¤ìŒê²ƒê³¼ ê°™ì€ ì œì•½ì¡°ê±´ í•˜ì˜ ìµœì í™” ë¬¸ì œë¡œ í‘œí˜„í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 \quad \text{subject to} \quad \|\mathbf{w}\|^2 \leq \delta
$$

* \$\lambda\$: ì •ê·œí™” ê°•ë„ (í¬ë©´ ì œì•½ì´ ê°•í•¨)
* \$\delta\$: \$\ell\_2\$ ì œì•½ ì¡°ê±´ì˜ ê²½ê³„

![img.png](250715_6.png)

## ðŸ“ˆ ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ ëª¨ë¸ ë³€í™”

* **$ \log\_{10} \lambda = \infty $**: ë‹¨ìˆœí•œ ì§€ì„  (\uac00ìš´ì¹˜ ê±°ì˜ 0)
* **$ \log\_{10} \lambda = -10 $**: \uc77cë°˜ì ì¸ \uacf5ì„ 
* **$ \lambda = 0 $**: ì •ê·œí™” ì—†ìŒ â†’ ê³¼ì í•© ë°œìƒ


## íŒŒë¼ë¯¸í„° ê°’ ë¹„êµ (Ridge ì ìš© ì „í›„)

| í•­ëª©       | OLS ì¶”ì •ì¹˜   | Ridge ì¶”ì •ì¹˜ |
| -------- | --------- | --------- |
| \$w\_0\$ | -8.1      | -0.2059   |
| \$w\_1\$ | 401.2     | 0.6986    |
| \$w\_2\$ | -6326.3   | -0.8355   |
| \$w\_3\$ | 49778.9   | 0.1882    |
| \$w\_4\$ | -222555.2 | -0.0394   |
| \$w\_5\$ | 599603.0  | -0.1488   |
| \$w\_6\$ | -990507.7 | -0.0717   |
| \$w\_7\$ | 980248.7  | 0.1706    |
| \$w\_8\$ | -523726.3 | -0.0984   |
| \$w\_9\$ | 122122.1  | 0.0165    |

> Ridgeë¥¼ ì ìš©í•˜ë©´ ê¸°ë³¸ê¸° ê³„ìˆ˜ ê°’ì´ ê°€ë³€í•´ì§„ë‹¤.


## í˜¸í™˜ì  ì„¤ëª…

RidgeëŠ” ë‹¤ìŒ ë‘ ê²ƒì„ ë™ì‹œì— ìµœì†Œí™”í•©ë‹ˆë‹¤:

* LS ì†ì‹¤: $ |\mathbf{y} - \Phi \mathbf{w}|^2 $
* ì •ê·œí™” í•­: $ |\mathbf{w}|^2 $

ê·¸ëŠ” **ë™ê³ ì„ (Contour)** í˜•íƒœë¡œ ì‹¤ì‹œê°„í™”ë  ìˆ˜ ìžˆê³ ,\nìš©ì  ì¡°ê±´ ë‚´ë¶€ì—ì„œ ì†ì‹¤ì´ ìµœì†Œì¸ ì§€ì ì„ ì°¾ëŠ” ë°©ì‹ìž…ë‹ˆë‹¤.


![img.png](250715_8.png)

## í•™ìŠµ ì•Œê³ ë¦¬ë“œë§ (Pseudocode)

```
Algorithm 1: Ridge Regression

Input: X âˆˆ â„^{nÃ—d}, y âˆˆ â„^n, Î» âˆˆ â„

1. Initialize Î¦ â† X,  y â† y
2. Compute w â† (Î¦áµ€Î¦ + Î»I)â»Â¹ Î¦áµ€y
3. Return w
```


## íŒŒì¼ ì„¤ëª…: Prostate Cancer Data

* íƒ€ê²Ÿ: PSA(ì „ë§ì„  íŠ¹ì´ í• ì•„ì›ƒ) ìˆ˜ì¹˜ ì˜ˆì¸¡
* íŠ¹ì„±:

  * `lcavol`: log cancer volume
  * `lweight`: log prostate weight
  * `age`: age of patient
  * `lbph`: log of benign prostatic hyperplasia
  * `svi`: seminal vesicle invasion
  * `lcp`: log of capsular penetration
  * `gleason`: Gleason score
  * `pgg45`: percent of Gleason 4 or 5

í¬í•¨ ë°ì´í„° ì¡°ì§:

* [http://statweb.stanford.edu/\~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
* [https://hastie.su.domains/CASI\_files/DATA/prostate.data](https://hastie.su.domains/CASI_files/DATA/prostate.data)


## ðŸ“ˆ Regularization Path [Hastie, Tibshirani, Friedman]

* \$\lambda\$ì˜ ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ ê° ê³„ìˆ˜ \$w\_i\$ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´
* Ridge regressionì˜ ê°€ìš´ì¹˜ ì´ˆê³¼ ê²°ê³¼ ê°„ì˜ ê´€ê³„ ì‹œê°í™”

![img.png](250715_9.png)

![img.png](250715_10.png)


## ðŸ” Cross-Validation for Choosing $Î»$

![img.png](250715_11.png)

> \$\lambda\$ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ëª©í‘œì— ê´€ê°í•˜ê¸° ë•Œë¬¸ì—,
> ëŒ€ì²´ì ìœ¼ë¡œ **k-fold êµì°¨ ê²€ì‚¬**ë¡œ ì ì ˆí•œ ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.

ë³µì†Œ:

* ë°ì´í„°ë¥¼ Kê°œ ë¶„í• 
* í•˜ë‚˜ëŠ” validation, ë‚˜ë¨¸ì§€ëŠ” training
* ë‹¤ì–‘í•œ \$\lambda\$ ê°’ì— ëŒ€í•´ validation error ê³„ì‚°
* errorê°€ ê°€ìž¥ ì ì€ ê°’ì„ ì„ íƒ

ê²°ê³¼:

```
Training:  1 2 3 4 5
Fold 1 â†’ Train: 2~5, Test: 1
Fold 2 â†’ Train: 1,3~5, Test: 2
...
```

ìµœì¢… ì„ íƒ: ê²€ì¦ ì˜¤ë¥˜ê°€ ìµœì†Œì¸ \$\lambda\$

---


# ðŸ“˜ LASSO (Least Absolute Shrinkage and Selection Operator)

## âœ¨ í•µì‹¬ ì•„ì´ë””ì–´

LASSOëŠ” ì•„ëž˜ì˜ ëª©ì  í•¨ìˆ˜ë¡œ ì •ì˜ë©ë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 + \lambda \|\mathbf{w}\|_1
$$

ë˜ëŠ” ë‹¤ìŒ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìµœì í™” ë¬¸ì œë¡œ ì“¸ ìˆ˜ ìžˆìŠµë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 \quad \text{s.t.} \quad \|\mathbf{w}\|_1 \leq \delta
$$

* \$\ell\_1\$ ì •ê·œí™”ë¥¼ í†µí•´ í¬ì†Œì„±(sparsity)ì„ ìœ ë„í•©ë‹ˆë‹¤.

![img.png](250715_12.png)


## ðŸ” í•´ì„ì  ìž¥ì 

* **ì˜ˆì¸¡ ì„±ëŠ¥**: ëª¨ë¸ ë³µìž¡ë„ë¥¼ ì œí•œí•˜ì—¬ ê³¼ì í•© ë°©ì§€
* **í•´ì„ ê°€ëŠ¥ì„±**: ì¼ë¶€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë³€ìˆ˜ ì„ íƒì´ ê°€ëŠ¥í•¨

## ðŸ“ ì‹œê°ì  ì„¤ëª…

* LASSOëŠ” ì œì•½ ì¡°ê±´ì´ ë‹¤ì´ì•„ëª¬ë“œ ëª¨ì–‘ (\$\ell\_1\$ norm ball)
* ê²½ê³„ì—ì„œ ìµœì ê°’ì´ ë°œìƒ â†’ ì¼ë¶€ ê³„ìˆ˜ëŠ” ì •í™•ížˆ 0ì´ ë¨

## ðŸ“ˆ Regularization Path ë¹„êµ

![img.png](250715_13.png)

* Ridge: ì—°ì†ì ìœ¼ë¡œ ìˆ˜ë ´, ëª¨ë“  ë³€ìˆ˜ í¬í•¨
* LASSO: ì¼ë¶€ ê³„ìˆ˜ê°€ ì •í™•ížˆ 0ìœ¼ë¡œ ìˆ˜ë ´ â†’ ë³€ìˆ˜ ì„ íƒ ê¸°ëŠ¥ í¬í•¨

## ðŸ§® Gradient ê³„ì‚°

ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\nabla_w \left( \frac{1}{2} \|y - Xw\|^2 + \lambda \|w\|_1 \right)
$$

* \$|w|\_1\$ì€ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ì§€ì  ì¡´ìž¬ â†’ coordinate descent ì‚¬ìš©

## ðŸ” Coordinate Descent

ê° íŒŒë¼ë¯¸í„° \$w\_j\$ì— ëŒ€í•´ ë°˜ë³µì ìœ¼ë¡œ ìµœì í™”:

$$
w_j \leftarrow S\left( \frac{1}{N} \sum_{i=1}^N x_{ij} (y_i - \hat{y}^{(-j)}_i), \lambda \right)
$$

## ðŸ”» Soft Thresholding

$$
S(z, \lambda) = \text{sign}(z) \cdot \max(|z| - \lambda, 0)
$$

* ìž…ë ¥ê°’ \$z\$ê°€ \$\lambda\$ë³´ë‹¤ ìž‘ìœ¼ë©´ 0ìœ¼ë¡œ ìˆ˜ì¶•
* í° ê°’ì€ \$\lambda\$ë§Œí¼ ê°ì†Œ

## ðŸ§® Subgradient ê³„ì‚°

\$\ell\_1\$ normì˜ ì„œë¸Œê·¸ëž˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\partial |w_j| = \begin{cases}
1 & w_j > 0 \\
-1 & w_j < 0 \\
[-1, 1] & w_j = 0
\end{cases}
$$

ì´ ê°œë…ì„ í†µí•´ soft-thresholdingì´ ë„ì¶œë¨

## ðŸ”« Shooting Algorithm

LASSOì˜ coordinate descentëŠ” shooting ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³ ë„ ë¶ˆë¦¼

**Papers**:

* Fu, W. (1998). "Penalized regression: The bridge versus the LASSO"
* Wu and Lange (2008). "Coordinate descent algorithms for LASSO penalized regression"

Pseudocode (Shooting Algorithm):

```
Algorithm: Coordinate Descent for Sparse Regression
Input: X, y, Î», initialize w = 0
repeat until convergence:
  for j = 1 to p:
    compute partial residual r^{(j)}
    update w_j â† S( ... ) using soft-threshold
return w
```

---



# ë²ˆì™¸: ðŸ§¬ Elastic Net

## ðŸ’¡ ì •ì˜

Elastic Netì€ Ridge(\$\ell\_2\$)ì™€ LASSO(\$\ell\_1\$)ë¥¼ ê²°í•©í•œ ì •ê·œí™” ë°©ë²•ìž…ë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 + \lambda_1 \|\mathbf{w}\|_1 + \lambda_2 \|\mathbf{w}\|_2^2
$$

* \$\lambda\_1\$: LASSO (í¬ì†Œì„± ìœ ë„)
* \$\lambda\_2\$: Ridge (ê³„ìˆ˜ ì•ˆì •í™”)

## âœ… ìž¥ì 

* ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity)ì— ê°•ê±´í•¨
* ë³€ìˆ˜ ì„ íƒê³¼ ê³„ìˆ˜ ì¶•ì†Œë¥¼ ë™ì‹œì— ìˆ˜í–‰ ê°€ëŠ¥
* LASSOê°€ ë³€ìˆ˜ ìˆ˜ë³´ë‹¤ ê´€ì¸¡ì¹˜ê°€ ì ì„ ë•Œ ë¶ˆì•ˆì •í•œ ì  ë³´ì™„

## ðŸ” í‘œí˜„ ë°©ì‹ (í˜¼í•© ë¹„ìœ¨)

ë³´í†µ í•˜ë‚˜ì˜ ì •ê·œí™” ê³„ìˆ˜ \$\lambda\$ì™€ í˜¼í•© ë¹„ìœ¨ \$\alpha\$ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œí˜„:

$$
\lambda \left( \alpha \|\mathbf{w}\|_1 + (1 - \alpha) \|\mathbf{w}\|_2^2 \right)
$$

* \$\alpha = 1\$ â†’ LASSO
* \$\alpha = 0\$ â†’ Ridge
* \$0 < \alpha < 1\$ â†’ Elastic Net

## ðŸ“ˆ ì‚¬ìš© ì˜ˆì‹œ

* sklearnì˜ `ElasticNetCV`ëŠ” êµì°¨ ê²€ì¦ìœ¼ë¡œ ìµœì ì˜ \$\lambda\$, \$\alpha\$ë¥¼ ì°¾ìŒ

## ðŸ“Œ ì •ë¦¬

| ê¸°ë²•          | ì •ê·œí™” í•­                                       | íŠ¹ì§•            |
| ----------- | ------------------------------------------- | ------------- |
| Ridge       | $\|w\|\_2^2\$                               | ê³„ìˆ˜ ì „ì²´ ì¶•ì†Œ, ì•ˆì •í™” |
| LASSO       | $\|w\|\_1\$                                 | í¬ì†Œì„±, ë³€ìˆ˜ ì„ íƒ    |
| Elastic Net | \$\alpha \|w\|\_1 + (1-\alpha) \|w\|\_2^2\$ | ë‘ ìž¥ì  ê²°í•©       |





