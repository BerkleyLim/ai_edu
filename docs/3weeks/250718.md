# 13ì¼ì°¨ - 250718

# ğŸ“˜ Reviews ì •ë¦¬ - Feature Selection
![img.png](250718_1.png)



# Quantile Regression

## Point Prediction vs Prediction Sets/Intervals

* **Point prediction(í¬ì¸íŠ¸ ì˜ˆì¸¡)**: e.g. `50 mins` to airport (no uncertainty). 
  - ê³µí•­ê¹Œì§€ 50ë¶„ - ë¶ˆí™•ì‹¤ì„± ì—†ìŒ
* **Prediction intervals(ì˜ˆì¸¡ êµ¬ê°„)**: e.g. `50 \pm 5 mins`, gives uncertainty.
  -  $50 \pm 5$ë¶„ â€” ë¶ˆí™•ì‹¤ì„± í¬í•¨.
* **Point prediction** returns a real value or label.
  - í•œê°œì˜ ê°’ or ë ˆì´ë¸” ì˜ˆì¸¡
* **Prediction intervals/sets** return a set of labels or an interval.
  - ì˜ˆì¸¡ êµ¬ê°„/ì…‹íŠ¸ëŠ” ê°’ì˜ ë²”ìœ„ë‚˜ ì—¬ëŸ¬ ê°œì˜ ê°€ëŠ¥ì„± ìˆëŠ” ê°’ì„ ë°˜í™˜í•¨


## OLS ì™€ Quantile Regression ë¹„êµ

![img.png](250718_2.png)

* **OLS (ìµœì†Œì œê³±ë²•)**: Estimates conditional **mean** of response variable.
  - ë°˜ì‘ ë³€ìˆ˜ì˜ ì¡°ê±´ë¶€ í‰ê· ì„ ì¶”ì •
* **Quantile Regression (ë¶„ìœ„ìˆ˜íšŒê·€)**: Estimates conditional **quantiles** (e.g., median).
  - ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ (ì˜ˆ: ì¤‘ì•™ê°’) ì¶”ì •

  * Provides prediction **intervals** (e.g., 5% & 95% quantiles for 90% interval).
    -  5%, 95% ë¶„ìœ„ìˆ˜ë¥¼ êµ¬í•´ 90% ì˜ˆì¸¡ êµ¬ê°„ì„ êµ¬ì„± ê°€ëŠ¥

## `QuantileRegressor` Code (sklearn)

```python
from sklearn.linear_model import QuantileRegressor

quantiles = [0.05, 0.5, 0.95]
predictions = {}
out_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool)

for quantile in quantiles:
    qr = QuantileRegressor(quantile=quantile, alpha=0)
    y_pred = qr.fit(X, y_normal).predict(X)
    predictions[quantile] = y_pred

    if quantile == min(quantiles):
        out_bounds_predictions = np.logical_or(out_bounds_predictions, y_pred >= y_normal)
    elif quantile == max(quantiles):
        out_bounds_predictions = np.logical_or(out_bounds_predictions, y_pred <= y_normal)
```


## What is a Quantile?

* Divides ordered data into equal-sized groups. 
  - (ë°ì´í„° ì •ë ¬ í›„ ë™ì¼ í¬ê¸°ì˜ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ”)
* Helps understand **spread** and **distribution** beyond averages. 
  - (í‰ê·  ë„˜ì–´ì„œ ë¶„í¬ì˜ êµ¬ì¡°ë‚˜ í¼ì§ì„ íŒŒì•…í•˜ëŠ”ë° ìœ ìš©í•¨)


## Types of Quantiles

* **Quartiles(4ë¶„ìœ„ìˆ˜)**: 4 parts

  * Q1: 25th percentile
  * Q2: 50th percentile (median)
  * Q3: 75th percentile
* **Deciles(10ë¶„ìœ„ìˆ˜)**: 10 parts (10%, ..., 90%) - 10ê°œì˜ ë¶„ìœ„ìˆ˜
* **Percentiles(100ë¶„ìœ„ìˆ˜)**: 100 parts - 100ê°œì˜ ë¶„ìœ„ìˆ˜


## How to Calculate a Quantile (ë¶„ìœ„ìˆ˜ ê³„ì‚° ë°©ë²•)

**Step 1: Order the data(ë°ì´í„° ì •ë ¬)**

Example: \$n = 9\$ values â†’ {3, 6, 7, 8, 8, 10, 13, 15, 16}

**Step 2: Calculate Rank (ë¶„ìœ„ìˆ˜ ìœ„ì¹˜ ê³„ì‚°)**

$\text{Rank} = p \times (n + 1)$

For \$p = 0.25\$ (1st quartile):

$\text{Rank} = 0.25 \times (9 + 1) = 2.5$

**Why \$n+1\$?** Because data divides into \$n+1\$ intervals.

**Step 3: Determine Value (ìœ„ì¹˜ì— ë”°ë¼ ë³´ê°„ë²•(inperpolate) ì ìš©)**

* Interpolate between 2nd and 3rd values (2ë²ˆì§¸ì˜ 3ë²ˆì§¸ ê°’ ì‚¬ì´ ë³´ê°„):

$6 + 0.5 \times (7 - 6) = 6.5$

So, Q1 = 6.5


## Formal Definition (ìˆ˜ì‹ì  ì •ì˜)

- $X$: random variable(ëœë¤ ë³€ìˆ˜)
- $F(x)$: CDF(ëˆ„ì ë¶„í¬í•¨ìˆ˜)
- $Q(p)$: quantile function (ë¶„ìœ„ìˆ˜ í•¨ìˆ˜)

$F(x) = P(X \leq x)$

$Q(p) = F^{-1}(p) = \inf \{x \in \mathbb{R} \mid F(x) \geq p\}$


## Quantile Regression: Motivation (ë¶„ìœ„ìˆ˜ íšŒê¸° ì¶œì—° ë™ê¸°)

* More **robust** to outliers, non-Gaussian noise
  - ì´ìƒì¹˜ì— ê°•ê±´í•¨ (outlierì— ì˜í–¥ ì ìŒ)
* Gives **prediction intervals**, not just points
  - ì˜ˆì¸¡ êµ¬ê°„ì„ ì§ì ‘ ì œê³µ ê°€ëŠ¥
* Works with **heteroscedastic** data
  - ì´ë¶„ì‚°ì„±(heteroscedastic)ì´ ìˆëŠ” ë°ì´í„°ì— ì í•©


## Quantile Regression vs OLS

### OLS(ìµœì†Œì œê³±ë²•):

$E[Y \mid X = x] = w_0 + w_1x_1 + \cdots + w_dx_d$

* Loss: **squared loss**

### Quantile Regression (ë¶„ìœ„ìˆ˜íšŒê·€):

$Q_\alpha(Y \mid X = x) = w_0(\alpha) + w_1(\alpha)x_1 + \cdots + w_d(\alpha)x_d$

* Loss: **pinball loss**


## Pinball Loss (í•€ë³¼ ì†ì‹¤ í•¨ìˆ˜)

For scalar $z$:

$$
\ell_\alpha(z) = \begin{cases}
  (\alpha - 1)z & \text{if } z \leq 0 \\
  \alpha z & \text{if } z > 0
\end{cases}
$$

![img.png](250718_3.png)


## Error Function in Quantile Regression (ë¶„ìœ„ìˆ˜ íšŒê·€ì˜ ì˜¤ì°¨ í•¨ìˆ˜)

$$
J(w_\alpha) = \sum_{n: y_n \geq w_\alpha^\top x_n} \alpha |y_n - w_\alpha^\top x_n| +
\sum_{n: y_n < w_\alpha^\top x_n} (1 - \alpha) |y_n - w_\alpha^\top x_n|
$$

* Minimization is via **linear programming** 
  - ì„ í˜•ê³„íšë²•(linear programming)ìœ¼ë¡œ ìµœì†Œí™”í•¨
* For \$\alpha = 0.5\$, reduces to **LAD (Least Absolute Deviation)**
  - a = 0.5ì¼ ê²½ìš° ìµœì†Œ ì ˆëŒ€ í¸ì°¨(LAD)ë¡œ ê·€ê²°ë¨


## Pinball Loss Intuition (ì§ê´€ì  ì´í•´: í•€ë³¼ ì†ì‹¤)

Let $q$: prediction(ì˜ˆì¸¡ê°’ q), $y$: true value (ì‹¤ì œê°’ y)

$$
\ell_{0.9}(q, y) = \begin{cases}
  0.9(y - q), & \text{if } y \geq q \\
  0.1(q - y), & \text{if } y < q
\end{cases}
$$

* Underprediction is penalized more when \$\alpha\$ is high
  - aê°€ í´ìˆ˜ë¡ ê³¼ì†Œì˜ˆì¸¡(predicting too low)ì— ëŒ€í•œ íŒ¨ë„í‹°ê°€ ì»¤ì§
* Loss minimized when:
  - ì†ì‹¤ì€ ë‹¤ìŒ ì¡°ê±´ì—ì„œìµœì†Œí™” ë¨

$P(Y \leq q) = \alpha$

---

## Example: Food Expenditure vs Income

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

mod = smf.quantreg("foodexp ~ income", data)

quantiles = np.arange(0.05, 0.96, 0.1)

models = [
    [q, *mod.fit(q=q).params.values]
    for q in quantiles
]
```

* **OLS**: overestimates for low income
* **LAD**: slightly better
* **QR**: shows varying quantile lines (fan-shape)


## Outlier Examples

* **OLS**: strongly affected by outliers
* **LAD**: more stable
* **QR**: remains stable and spreads by quantile


## Heteroscedastic Noise

* Noise increases with input \$x\$
* **Spline regression** fits average trend
* **Quantile regression** captures uncertainty via multiple quantiles


## Important Note

**QR is not automatically valid on test data. Calibration is needed.**
