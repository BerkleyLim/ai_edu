# 13일차 - 250718

# 📘 Reviews 정리 - Feature Selection
![img.png](250718_1.png)



# Quantile Regression

## Point Prediction vs Prediction Sets/Intervals

* **Point prediction(포인트 예측)**: e.g. `50 mins` to airport (no uncertainty). 
  - 공항까지 50분 - 불확실성 없음
* **Prediction intervals(예측 구간)**: e.g. `50 \pm 5 mins`, gives uncertainty.
  -  $50 \pm 5$분 — 불확실성 포함.
* **Point prediction** returns a real value or label.
  - 한개의 값 or 레이블 예측
* **Prediction intervals/sets** return a set of labels or an interval.
  - 예측 구간/셋트는 값의 범위나 여러 개의 가능성 있는 값을 반환함


## OLS 와 Quantile Regression 비교

![img.png](250718_2.png)

* **OLS (최소제곱법)**: Estimates conditional **mean** of response variable.
  - 반응 변수의 조건부 평균을 추정
* **Quantile Regression (분위수회귀)**: Estimates conditional **quantiles** (e.g., median).
  - 조건부 분위수 (예: 중앙값) 추정

  * Provides prediction **intervals** (e.g., 5% & 95% quantiles for 90% interval).
    -  5%, 95% 분위수를 구해 90% 예측 구간을 구성 가능

## `QuantileRegressor` Code (sklearn)

```python
from sklearn.linear_model import QuantileRegressor

quantiles = [0.05, 0.5, 0.95]
predictions = {}
out_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool)

for quantile in quantiles:
    qr = QuantileRegressor(quantile=quantile, alpha=0)
    y_pred = qr.fit(X, y_normal).predict(X)
    predictions[quantile] = y_pred

    if quantile == min(quantiles):
        out_bounds_predictions = np.logical_or(out_bounds_predictions, y_pred >= y_normal)
    elif quantile == max(quantiles):
        out_bounds_predictions = np.logical_or(out_bounds_predictions, y_pred <= y_normal)
```


## What is a Quantile?

* Divides ordered data into equal-sized groups.
* Helps understand **spread** and **distribution** beyond averages.


## Types of Quantiles

* **Quartiles**: 4 parts

  * Q1: 25th percentile
  * Q2: 50th percentile (median)
  * Q3: 75th percentile
* **Deciles**: 10 parts (10%, ..., 90%)
* **Percentiles**: 100 parts


## How to Calculate a Quantile

**Step 1: Order the data**

Example: \$n = 9\$ values → {3, 6, 7, 8, 8, 10, 13, 15, 16}

**Step 2: Calculate Rank**

$\text{Rank} = p \times (n + 1)$

For \$p = 0.25\$ (1st quartile):

$\text{Rank} = 0.25 \times (9 + 1) = 2.5$

**Why \$n+1\$?** Because data divides into \$n+1\$ intervals.

**Step 3: Determine Value**

* Interpolate between 2nd and 3rd values:

$6 + 0.5 \times (7 - 6) = 6.5$

So, Q1 = 6.5


## Formal Definition

Let \$X\$: random variable, \$F(x)\$: CDF, \$Q(p)\$: quantile function

$F(x) = P(X \leq x)$

$Q(p) = F^{-1}(p) = \inf \{x \in \mathbb{R} \mid F(x) \geq p\}$


## Motivation for Quantile Regression

* More **robust** to outliers, non-Gaussian noise
* Gives **prediction intervals**, not just points
* Works with **heteroscedastic** data


## Quantile Regression vs OLS

### OLS:

$E[Y \mid X = x] = w_0 + w_1x_1 + \cdots + w_dx_d$

* Loss: **squared loss**

### Quantile Regression:

$Q_\alpha(Y \mid X = x) = w_0(\alpha) + w_1(\alpha)x_1 + \cdots + w_d(\alpha)x_d$

* Loss: **pinball loss**


## Pinball Loss

For scalar \$z\$:

$$
\ell_\alpha(z) = \begin{cases}
  (\alpha - 1)z & \text{if } z \leq 0 \\
  \alpha z & \text{if } z > 0
\end{cases}
$$


## Error Function in Quantile Regression

$$
J(w_\alpha) = \sum_{n: y_n \geq w_\alpha^\top x_n} \alpha |y_n - w_\alpha^\top x_n| +
\sum_{n: y_n < w_\alpha^\top x_n} (1 - \alpha) |y_n - w_\alpha^\top x_n|
$$

* Minimization is via **linear programming**
* For \$\alpha = 0.5\$, reduces to **LAD (Least Absolute Deviation)**


## Pinball Loss Intuition

Let \$q\$: prediction, \$y\$: true value

$$
\ell_{0.9}(q, y) = \begin{cases}
  0.9(y - q), & \text{if } y \geq q \\
  0.1(q - y), & \text{if } y < q
\end{cases}
$$

* Underprediction is penalized more when \$\alpha\$ is high
* Loss minimized when:

$P(Y \leq q) = \alpha$

---

## Example: Food Expenditure vs Income

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

mod = smf.quantreg("foodexp ~ income", data)

quantiles = np.arange(0.05, 0.96, 0.1)

models = [
    [q, *mod.fit(q=q).params.values]
    for q in quantiles
]
```

* **OLS**: overestimates for low income
* **LAD**: slightly better
* **QR**: shows varying quantile lines (fan-shape)


## Outlier Examples

* **OLS**: strongly affected by outliers
* **LAD**: more stable
* **QR**: remains stable and spreads by quantile


## Heteroscedastic Noise

* Noise increases with input \$x\$
* **Spline regression** fits average trend
* **Quantile regression** captures uncertainty via multiple quantiles


## Important Note

**QR is not automatically valid on test data. Calibration is needed.**
