# 18일차 - 250724


# 📊 Clustering

- **Clustering**은 **라벨이 없는 데이터(unlabeled data)** 를 다루는 **비지도 학습(unsupervised learning)** 방법이다.
- 데이터를 유사한 특성을 가진 그룹으로 분할한다.
  - **Intra-class similarity**는 높고, **Inter-class similarity**는 낮다.
- 사용되는 **distance measure** (거리 측정 방식)에 따라 클러스터링 결과가 달라진다.


## 🧠 Clustering의 대표 응용

- **Image Segmentation**
- **Targeted Marketing**
- **SDD Image Clustering**
- **Community Detection in Social Media**


# 📌 k-Means Clustering

## 🎯 목표

- 데이터 $\{x_n \in \mathbb{R}^D \mid n = 1, \dots, N\}$ 를 $K$개의 클러스터 $C_k$ 로 분할.
- 각 클러스터 중심 벡터 $\mu_k$ 는 다음과 같이 정의됨:

$$
\mu_k = \frac{1}{N_k} \sum_{n \in C_k} x_n
$$

- 목적 함수 $J$는 각 포인트와 클러스터 중심 간의 유클리드 거리의 제곱합:

$$
J = \sum_{k=1}^{K} \sum_{n \in C_k} \| x_n - \mu_k \|^2
$$


## 🧾 알고리즘 (k-means)

1. **Assignment Step**: 모든 포인트를 가장 가까운 클러스터에 할당

   $$\arg \min_k \|x_n - \mu_k\|^2$$

2. **Update Step**: 각 클러스터의 중심 $\mu_k$를 다시 계산

   $$
   \mu_k^{\text{new}} = \frac{1}{N_k} \sum_{n \in C_k} x_n
   $$


## ⚙️ Alternative Formulation

- Cluster indicator 변수 $r_{k,n}$ 사용:

$$
r_{k,n} =
\begin{cases}
1, & \text{if } \arg \min_j \|x_n - \mu_j\|^2 = k \\
0, & \text{otherwise}
\end{cases}
$$

- 업데이트 식:

$$
\mu_k = \frac{\sum_{n=1}^{N} r_{k,n} x_n}{\sum_{n=1}^{N} r_{k,n}}
$$


## 🧮 Mixed-Integer Optimization으로의 해석

$$
\arg\min J = \sum_{k=1}^{K} \sum_{n=1}^{N} r_{k,n} \|x_n - \mu_k\|^2
$$

subject to

$$
R = [r_{k,n}] \in \left\{ R \in \{0, 1\}^{K \times N} \mid \sum_{k=1}^{K} r_{k,n} = 1 \right\}
$$


## ⚠️ k-means의 한계

- 초기 중심값에 민감
- **Convex** 형태의 클러스터에만 적합
- 적절한 $k$ 값을 고르는 기준이 없음


## 💡 Elbow Method

- WCSS (Within-Cluster-Sum-of-Squares) 기반으로 $k$ 값 결정
- 그래프에서 "팔꿈치(elbow)" 모양이 생기는 지점을 최적 $k$로 선택

---

# Mixture of Gaussians (MoG)

MoG는 데이터를 여러 개의 가우시안 분포로 구성된 혼합 모델로 표현합니다. 확률 기반 접근을 사용합니다.

## k-Means와 비교
| 항목 | k-Means | MoG |
|------|---------|-----|
| 확률 기반 | 아니오 | 예 |
| 유연성 | 낮음 | 높음 |
| 확장성 | 높음 | 낮음 |

## 개념
- 각 데이터는 여러 개의 가우시안 중 하나에서 생성됨
- `π_k`: 가우시안 k에 속할 확률 (mixing coefficient)
- `μ_k`, `Σ_k`: 각 가우시안의 평균과 공분산

## 수식 요약
- 전체 확률:  
  `p(x) = Σ_k π_k N(x | μ_k, Σ_k)`
- 최대우도 추정(MLE)로 파라미터 업데이트

## 알고리즘 (EM)
1. **E-step**: 각 데이터가 어떤 가우시안에 속할 확률 계산
2. **M-step**: 각 가우시안의 파라미터(μ, Σ, π) 업데이트

## 예시
- 가우시안 수를 늘릴수록 분포를 더 정밀하게 모델링
- `σ^2 → 0`일 때 k-means와 동일한 형태로 수렴

## 클러스터 수 결정 방법
- AIC, BIC (정보 기준) 사용
- Elbow method: 클러스터 수에 따른 비용 함수 확인

---

# 🌀 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

- **밀도 기반 클러스터링** 알고리즘
- 밀도가 높은 영역은 클러스터로 간주, 밀도가 낮은 영역은 노이즈로 간주


## 📌 주요 개념

- **eps (ε)**: 점들 간에 이 거리 이내에 있으면 '이웃'으로 간주
- **min_samples**: 핵심 포인트(core point)가 되기 위한 최소 이웃 개수
- **core point**: ε 반경 내에 이웃이 min_samples 이상인 점
- **border point**: core point의 이웃이지만 자신은 core point가 아닌 점
- **noise point**: 어느 클러스터에도 속하지 않는 이상치


## 🌟 포인트 유형

1. **Core Point**: $\varepsilon$ 내에 min_samples 이상의 포인트가 있음
2. **Border Point**: $\varepsilon$ 내에 있지만 min_samples 미만이고, core point의 이웃
3. **Noise Point**: 위 두 조건에 해당하지 않음 (클러스터 라벨 -1)


## ⛓️ DBSCAN 알고리즘 순서

1. 코어 포인트를 임의로 선택하여 클러스터 시작
2. 이웃 포인트를 모두 클러스터에 포함
3. 확장할 수 없으면 다른 코어 포인트로 새 클러스터 생성
4. 반복


## 📈 Two Moons Dataset

- k-means는 두 개의 반달 모양을 나누기 어렵지만
- DBSCAN은 복잡한 형태의 클러스터도 잘 탐지

## 클러스터링 과정
1. 임의의 점에서 시작하여 core point인지 확인
2. core point이면 해당 점을 기준으로 연결된 이웃을 하나의 클러스터로 확장
3. border point는 core point에 속하지만 클러스터 확장에는 사용되지 않음
4. 반복하여 모든 점을 방문

## 장점
- 클러스터 개수를 미리 알 필요 없음
- 이상치(outlier) 자동 탐지
- 복잡한 모양의 클러스터에도 강함

## 예시
- 달 모양 데이터(two moons)도 효과적으로 구분
- Iris 데이터에서는 `eps`, `min_samples`에 따라 결과가 다름


## 📊 Iris 데이터 예시

```python
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.4, min_samples=10)
dbscan.fit(df[['sepal_length', 'sepal_width']])
```

- 두 개의 outlier (-1 라벨) 존재
- 적절한 파라미터 조정으로 노이즈와 클러스터를 잘 분리 가능


