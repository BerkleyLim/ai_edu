
# 🆕 강화학습 추가/보강 내용 (Reinforcement Learning – Supplementary Notes)

## 1. 최적 제어와 강화학습 비교 (Optimal Control vs RL)
- **강화학습 (Reinforcement Learning)**: Agent, Environment, Action 구조, reward 기반 학습  
- **최적 제어 (Optimal Control)**: Controller, Plant, Control Signal 구조, 시스템 모델 기반  
- 차이점: RL은 모델 불필요 (model-free 가능), Optimal Control은 모델 필요  

---

## 2. 강화학습 Taxonomy (RL Taxonomy)
- **Model-based RL**: 환경 모델 $$p(s',r|s,a)$$ 학습 후 planning 수행  
- **Value-based RL**: $$Q^*(s,a)$$ 학습, $$\pi^*(s)=rg\max_a Q^*(s,a)$$ (Q-Learning, SARSA, DQN)  
- **Policy-based RL**: $$\pi_	heta(a|s)$$ 직접 최적화 (REINFORCE, PPO, TRPO)  
- **Hybrid (Actor-Critic)**: Policy + Value function 결합 (A2C, A3C, PPO)  

---

## 3. 계획 (Planning)
- **Offline Planning**: 전체 MDP 모델 기반 Policy Iteration, Value Iteration  
- **Online Planning**: 현재 상태에서 Lookahead (MCTS, RTDP)  

---

## 4. Monte Carlo (MC) 학습 방법
- **First-visit MC**: episode 내 첫 방문 시 업데이트  
- **Every-visit MC**: 방문할 때마다 업데이트  
- **Incremental MC**:  
  $$ V(s) \leftarrow V(s) + lpha (G_t - V(s)) $$  

---

## 5. Temporal Difference (TD) 학습
- 업데이트:  
  $$ V(S_t) \leftarrow V(S_t) + lpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) $$  
- 특징: Bootstrapping 기반, episode 끝나기 전에도 학습 가능  

---

## 6. SARSA vs Q-Learning
- **SARSA (On-policy)**: 실제 수행한 행동 기준 업데이트 → 안전/보수적  
- **Q-Learning (Off-policy)**: 최적 행동 기준 업데이트 → 공격적/빠른 수렴  
- **Cliff Walking 예제**: SARSA는 안전 경로, Q-Learning은 위험하지만 최단 경로 선택  

---

## 7. 함수 근사 (Function Approximation)
- Large state space → Table 불가 → Approximation 필요  
- 선형 근사: $$ V_	heta(s) = 	heta^	op \phi(s) $$  
- 신경망 기반: Deep RL로 일반화 가능  

---

## 8. 심층 Q-네트워크 (Deep Q-Network, DQN)
- **구조**: CNN + FC Layer로 $$Q_	heta(s,a)$$ 근사  
- **Loss Function**:  
  $$ J(	heta) = (r + \gamma \max_{a'} Q_{	heta^-}(s',a') - Q_	heta(s,a))^2 $$  
- **안정화 기법**: Experience Replay, Target Network  
- **실험**: Atari 게임 (픽셀 입력, 보상=점수 변화)  

---

## 9. 보충 개념 (Additional Insights)
- **Exploration vs Exploitation**: ε-greedy, Softmax, UCB  
- **Sample Efficiency 문제**: 데이터 요구량 큼 → Model-based RL로 개선 가능  
- **Policy Gradient Variance Reduction**: Baseline (예: $$V^\pi(s)$$), Advantage Actor-Critic (A2C)  

---

## 10. 결론 (Conclusion)
- handout17에서 보강된 핵심 포인트:
  1. RL vs Optimal Control 비교  
  2. RL Taxonomy 구체화  
  3. Planning (Offline vs Online, MCTS)  
  4. Monte Carlo 학습 방식 보강  
  5. TD 학습 vs MC 비교  
  6. SARSA vs Q-Learning 차이 + Cliff Walking 예제  
  7. 함수 근사 및 DQN 메커니즘  
  8. Exploration 전략 및 Variance Reduction 기법
