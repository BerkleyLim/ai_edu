
# ğŸ†• ê°•í™”í•™ìŠµ ì¶”ê°€/ë³´ê°• ë‚´ìš© (Reinforcement Learning â€“ Supplementary Notes)

## 1. ìµœì  ì œì–´ì™€ ê°•í™”í•™ìŠµ ë¹„êµ (Optimal Control vs RL)
- **ê°•í™”í•™ìŠµ (Reinforcement Learning)**: Agent, Environment, Action êµ¬ì¡°, reward ê¸°ë°˜ í•™ìŠµ  
- **ìµœì  ì œì–´ (Optimal Control)**: Controller, Plant, Control Signal êµ¬ì¡°, ì‹œìŠ¤í…œ ëª¨ë¸ ê¸°ë°˜  
- ì°¨ì´ì : RLì€ ëª¨ë¸ ë¶ˆí•„ìš” (model-free ê°€ëŠ¥), Optimal Controlì€ ëª¨ë¸ í•„ìš”  

---

## 2. ê°•í™”í•™ìŠµ Taxonomy (RL Taxonomy)
- **Model-based RL**: í™˜ê²½ ëª¨ë¸ $$p(s',r|s,a)$$ í•™ìŠµ í›„ planning ìˆ˜í–‰  
- **Value-based RL**: $$Q^*(s,a)$$ í•™ìŠµ, $$\pi^*(s)=rg\max_a Q^*(s,a)$$ (Q-Learning, SARSA, DQN)  
- **Policy-based RL**: $$\pi_	heta(a|s)$$ ì§ì ‘ ìµœì í™” (REINFORCE, PPO, TRPO)  
- **Hybrid (Actor-Critic)**: Policy + Value function ê²°í•© (A2C, A3C, PPO)  

---

## 3. ê³„íš (Planning)
- **Offline Planning**: ì „ì²´ MDP ëª¨ë¸ ê¸°ë°˜ Policy Iteration, Value Iteration  
- **Online Planning**: í˜„ì¬ ìƒíƒœì—ì„œ Lookahead (MCTS, RTDP)  

---

## 4. Monte Carlo (MC) í•™ìŠµ ë°©ë²•
- **First-visit MC**: episode ë‚´ ì²« ë°©ë¬¸ ì‹œ ì—…ë°ì´íŠ¸  
- **Every-visit MC**: ë°©ë¬¸í•  ë•Œë§ˆë‹¤ ì—…ë°ì´íŠ¸  
- **Incremental MC**:  
  $$ V(s) \leftarrow V(s) + lpha (G_t - V(s)) $$  

---

## 5. Temporal Difference (TD) í•™ìŠµ
- ì—…ë°ì´íŠ¸:  
  $$ V(S_t) \leftarrow V(S_t) + lpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) $$  
- íŠ¹ì§•: Bootstrapping ê¸°ë°˜, episode ëë‚˜ê¸° ì „ì—ë„ í•™ìŠµ ê°€ëŠ¥  

---

## 6. SARSA vs Q-Learning
- **SARSA (On-policy)**: ì‹¤ì œ ìˆ˜í–‰í•œ í–‰ë™ ê¸°ì¤€ ì—…ë°ì´íŠ¸ â†’ ì•ˆì „/ë³´ìˆ˜ì   
- **Q-Learning (Off-policy)**: ìµœì  í–‰ë™ ê¸°ì¤€ ì—…ë°ì´íŠ¸ â†’ ê³µê²©ì /ë¹ ë¥¸ ìˆ˜ë ´  
- **Cliff Walking ì˜ˆì œ**: SARSAëŠ” ì•ˆì „ ê²½ë¡œ, Q-Learningì€ ìœ„í—˜í•˜ì§€ë§Œ ìµœë‹¨ ê²½ë¡œ ì„ íƒ  

---

## 7. í•¨ìˆ˜ ê·¼ì‚¬ (Function Approximation)
- Large state space â†’ Table ë¶ˆê°€ â†’ Approximation í•„ìš”  
- ì„ í˜• ê·¼ì‚¬: $$ V_	heta(s) = 	heta^	op \phi(s) $$  
- ì‹ ê²½ë§ ê¸°ë°˜: Deep RLë¡œ ì¼ë°˜í™” ê°€ëŠ¥  

---

## 8. ì‹¬ì¸µ Q-ë„¤íŠ¸ì›Œí¬ (Deep Q-Network, DQN)
- **êµ¬ì¡°**: CNN + FC Layerë¡œ $$Q_	heta(s,a)$$ ê·¼ì‚¬  
- **Loss Function**:  
  $$ J(	heta) = (r + \gamma \max_{a'} Q_{	heta^-}(s',a') - Q_	heta(s,a))^2 $$  
- **ì•ˆì •í™” ê¸°ë²•**: Experience Replay, Target Network  
- **ì‹¤í—˜**: Atari ê²Œì„ (í”½ì…€ ì…ë ¥, ë³´ìƒ=ì ìˆ˜ ë³€í™”)  

---

## 9. ë³´ì¶© ê°œë… (Additional Insights)
- **Exploration vs Exploitation**: Îµ-greedy, Softmax, UCB  
- **Sample Efficiency ë¬¸ì œ**: ë°ì´í„° ìš”êµ¬ëŸ‰ í¼ â†’ Model-based RLë¡œ ê°œì„  ê°€ëŠ¥  
- **Policy Gradient Variance Reduction**: Baseline (ì˜ˆ: $$V^\pi(s)$$), Advantage Actor-Critic (A2C)  

---

## 10. ê²°ë¡  (Conclusion)
- handout17ì—ì„œ ë³´ê°•ëœ í•µì‹¬ í¬ì¸íŠ¸:
  1. RL vs Optimal Control ë¹„êµ  
  2. RL Taxonomy êµ¬ì²´í™”  
  3. Planning (Offline vs Online, MCTS)  
  4. Monte Carlo í•™ìŠµ ë°©ì‹ ë³´ê°•  
  5. TD í•™ìŠµ vs MC ë¹„êµ  
  6. SARSA vs Q-Learning ì°¨ì´ + Cliff Walking ì˜ˆì œ  
  7. í•¨ìˆ˜ ê·¼ì‚¬ ë° DQN ë©”ì»¤ë‹ˆì¦˜  
  8. Exploration ì „ëµ ë° Variance Reduction ê¸°ë²•
