
# ğŸ¤– ê°•í™”í•™ìŠµ (Reinforcement Learning)

## 1. ê¸°ë³¸ ê°œë… (Fundamentals)
- **ì—ì´ì „íŠ¸ (Agent):** í•™ìŠµì ë˜ëŠ” ì˜ì‚¬ê²°ì •ì  
- **í™˜ê²½ (Environment):** ì—ì´ì „íŠ¸ ì™¸ë¶€, ìƒí˜¸ì‘ìš© ëŒ€ìƒ  
- ìƒí˜¸ì‘ìš© ê³¼ì •:  
  $$ (S_1, A_1, R_2, S_2, A_2, R_3, \ldots, S_T) $$  

- ëª©í‘œ: ëˆ„ì  ë³´ìƒ (Cumulative Reward) ìµœëŒ€í™”  

---

## 2. ë¹„êµ (Bandit vs RL)
| êµ¬ë¶„ | ë‹¤ì¤‘ ìŠ¬ë¡¯ë¨¸ì‹  (Multi-Armed Bandit) | ì»¨í…ìŠ¤ì¶”ì–¼ ë°´ë”§ (Contextual Bandit) | ê°•í™”í•™ìŠµ (RL) |
|------|----------------------------------|-----------------------------------|-------------|
| ìƒíƒœ | ì—†ìŒ | Context ê´€ì°° | State ê´€ì°° |
| ë³´ìƒ | í–‰ë™ë§Œ ì˜ì¡´ | Context + Action ì˜ì¡´ | State + Action + Next State ì˜ì¡´ |
| íƒí—˜ | Arms íƒìƒ‰ | Context ë‚´ Action íƒìƒ‰ | State + Action ëª¨ë‘ íƒí—˜ |

---

## 3. ë§ˆë¥´ì½”í”„ ê²°ì • ê³¼ì • (Markov Decision Process, MDP)
- ì •ì˜: $$ \langle S, A, P, R, \gamma 
angle $$  
  - $$ S $$: ìƒíƒœ ì§‘í•©  
  - $$ A $$: í–‰ë™ ì§‘í•©  
  - $$ P(s'|s,a) $$: ìƒíƒœ ì „ì´ í™•ë¥   
  - $$ R(s,a) $$: ë³´ìƒ í•¨ìˆ˜  
  - $$ \gamma $$: í• ì¸ìœ¨ (discount factor)  

- ë°˜í™˜ (Return):  
  $$ G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} $$  

---

## 4. ê°€ì¹˜ í•¨ìˆ˜ (Value Functions)
- ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ (State-value):  
  $$ V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ] $$  
- í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜ (Action-value):  
  $$ Q^\pi(s,a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ] $$  
- ì–´ë“œë°´í‹°ì§€ í•¨ìˆ˜ (Advantage):  
  $$ A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$  

---

## 5. ë²¨ë§Œ ë°©ì •ì‹ (Bellman Equations)
- ê¸°ëŒ€ ë²¨ë§Œ ë°©ì •ì‹ (Bellman Expectation):  
  $$ V^\pi(s) = \sum_a \pi(a|s) \Big[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \Big] $$  

- ìµœì  ê°€ì¹˜ (Optimal Value):  
  $$ V^*(s) = \max_\pi V^\pi(s) $$  
  $$ Q^*(s,a) = \max_\pi Q^\pi(s,a) $$  

- ìµœì  ì •ì±… (Optimal Policy):  
  $$ \pi^*(s) = rg\max_a Q^*(s,a) $$  

---

## 6. ë™ì  í”„ë¡œê·¸ë˜ë° (Dynamic Programming)
- ì •ì±… ë°˜ë³µ (Policy Iteration): Policy Evaluation â†” Policy Improvement  
- ê°€ì¹˜ ë°˜ë³µ (Value Iteration):  
  $$ V_{k+1}(s) = \max_a \Big[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \Big] $$  

---

## 7. ê²½í—˜ìœ¼ë¡œë¶€í„° í•™ìŠµ (Learning from Experience)
- ëª¬í…Œì¹´ë¥¼ë¡œ ë°©ë²• (Monte Carlo Methods)  
  - ì „ì²´ episode ì™„ë£Œ í›„ í‰ê·  return ê¸°ë°˜  
- ì‹œê°„ì°¨ í•™ìŠµ (Temporal Difference, TD)  
  - ë§¤ step ì—…ë°ì´íŠ¸  
  - $$ V(S_t) \leftarrow V(S_t) + lpha ig( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) ig) $$  

---

## 8. ê°€ì¹˜ ê¸°ë°˜ ë°©ë²• (Value-Based Methods)
- SARSA (On-policy):  
  $$ Q(s,a) \leftarrow Q(s,a) + lpha ig( r + \gamma Q(s',a') - Q(s,a) ig) $$  

- Q-Learning (Off-policy):  
  $$ Q(s,a) \leftarrow Q(s,a) + lpha ig( r + \gamma \max_{a'} Q(s',a') - Q(s,a) ig) $$  

- ì°¨ì´: SARSA = ì•ˆì „/ë³´ìˆ˜ì , Q-Learning = ìµœì /ê³µê²©ì   

---

## 9. í•¨ìˆ˜ ê·¼ì‚¬ (Function Approximation)
- ìƒíƒœ ê³µê°„ì´ ë§¤ìš° í´ ë•Œ, í•¨ìˆ˜ ê·¼ì‚¬ ì‚¬ìš©  
- ì„ í˜• ëª¨ë¸ (Linear Models)  
  $$ V_	heta(s) = 	heta^	op \phi(s) $$  
- ì‹ ê²½ë§ ê¸°ë°˜ (Deep Q-Network, DQN)  
  $$ Q_	heta(s,a) pprox Q^*(s,a) $$  
- Experience Replay + Target Network ì•ˆì •í™” ê¸°ë²• ì ìš©  

---

## 10. ì‹¬ì¸µ ê°•í™”í•™ìŠµ (Deep Reinforcement Learning)
- DQN (Deep Q-Network)  
  - Atari ê²Œì„ í”Œë ˆì´ (í”½ì…€ â†’ ìƒíƒœ)  
  - ë³´ìƒ ê¸°ë°˜ìœ¼ë¡œ CNN í•™ìŠµ  
- ì •ì±… ê¸°ë°˜ (Policy Gradient)  
  - ì§ì ‘ ì •ì±… $$ \pi_	heta(a|s) $$ í•™ìŠµ  
- ì•¡í„°-í¬ë¦¬í‹± (Actor-Critic)  
  - Policy + Value function ë³‘í•© (ì˜ˆ: PPO, TRPO)  
- **ê²Œì„ (Games):** Atari, Go, StarCraft II  
- **ë¡œë³´í‹±ìŠ¤ (Robotics):** Motor control, Navigation  
- **ë¹„ì¦ˆë‹ˆìŠ¤ (Business):** ì¬ê³  ê´€ë¦¬, ì¶”ì²œ ì‹œìŠ¤í…œ  
- 
---

## 11. íƒí—˜ ì „ëµ (Exploration Strategies)
- **Îµ-greedy**: í™•ë¥  Îµë¡œ ëœë¤, 1âˆ’Îµë¡œ greedy  
- **Softmax Policy**:  
  $$ \pi(a|s) = rac{\exp(Q(s,a)/	au)}{\sum_{a'} \exp(Q(s,a')/	au)} $$  
  (Ï„ = Temperature, í´ìˆ˜ë¡ íƒí—˜ â†‘)  

---

## 12. On-policy vs Off-policy
- **On-policy (SARSA):** ì‹¤ì œ ì •ì±…ì´ íƒí—˜ì„ í¬í•¨ â†’ ì•ˆì •ì , ì•ˆì „ì„± ê³ ë ¤  
- **Off-policy (Q-learning):** ìµœì  ì •ì±… ê¸°ì¤€ í•™ìŠµ â†’ ë¹ ë¥¸ ìˆ˜ë ´, ê³µê²©ì   

---

## 13. ì‹¬ì¸µ Q-ë„¤íŠ¸ì›Œí¬ (Deep Q-Network, DQN)
- Qí•¨ìˆ˜ë¥¼ CNNìœ¼ë¡œ ê·¼ì‚¬  
- í•µì‹¬ ê¸°ë²•:  
  1. **Experience Replay** (ìƒ˜í”Œ decorrelation)  
  2. **Target Network** (ì•ˆì •ì„± í™•ë³´)  
- ì†ì‹¤ í•¨ìˆ˜:  
  $$ J(	heta) = (r + \gamma \max_{a'} Q_{	heta^-}(s',a') - Q_	heta(s,a))^2 $$  

---

## 14. ì •ì±… ê¸°ë°˜ ë°©ë²• (Policy Gradient)
- ì •ì±…ì„ ì§ì ‘ ìµœì í™”:  
  $$ 
abla_	heta J(	heta) = \mathbb{E}_\pi [ 
abla_	heta \log \pi_	heta(a|s) G_t ] $$  
- REINFORCE ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜  

---

## 15. ì•¡í„°-í¬ë¦¬í‹± (Actor-Critic)
- Policy (Actor) + Value function (Critic) ê²°í•©  
- Criticì´ Advantage $$ A(s,a) $$ ì¶”ì • â†’ Actor ì—…ë°ì´íŠ¸ ì•ˆì •í™”  
- ëŒ€í‘œ ì•Œê³ ë¦¬ì¦˜: **A2C, A3C, PPO, TRPO**  

---

## 16. ê²°ë¡  (Conclusion)
- RL = ìƒíƒœ, í–‰ë™, ë³´ìƒ ê¸°ë°˜ì˜ í•™ìŠµ  
- MDPì™€ ë²¨ë§Œ ë°©ì •ì‹ì´ ì´ë¡ ì  í† ëŒ€  
- ê°•í™”í•™ìŠµ = ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° ë³´ìƒ ìµœëŒ€í™” í•™ìŠµ  
- í† ëŒ€: MDP + Bellman Equation  
- ì „í†µì  ë°©ë²•: MC, TD, SARSA, Q-Learning  
- í˜„ëŒ€ì  ë°©ë²•: DQN, Policy Gradient, Actor-Critic  
- ì‘ìš© ë¶„ì•¼ í™•ì¥: ë¡œë³´í‹±ìŠ¤, ê²Œì„, ê¸ˆìœµ, ìš´ì˜ ìµœì í™”  
