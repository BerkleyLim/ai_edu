
# 🤖 강화학습 (Reinforcement Learning)

## 1. 기본 개념 (Fundamentals)
- **에이전트 (Agent):** 학습자 또는 의사결정자  
- **환경 (Environment):** 에이전트 외부, 상호작용 대상  
- 상호작용 과정:  
  $$ (S_1, A_1, R_2, S_2, A_2, R_3, \ldots, S_T) $$  

- 목표: 누적 보상 (Cumulative Reward) 최대화  

---

## 2. 비교 (Bandit vs RL)
| 구분 | 다중 슬롯머신 (Multi-Armed Bandit) | 컨텍스추얼 밴딧 (Contextual Bandit) | 강화학습 (RL) |
|------|----------------------------------|-----------------------------------|-------------|
| 상태 | 없음 | Context 관찰 | State 관찰 |
| 보상 | 행동만 의존 | Context + Action 의존 | State + Action + Next State 의존 |
| 탐험 | Arms 탐색 | Context 내 Action 탐색 | State + Action 모두 탐험 |

---

## 3. 마르코프 결정 과정 (Markov Decision Process, MDP)
- 정의: $$ \langle S, A, P, R, \gamma 
angle $$  
  - $$ S $$: 상태 집합  
  - $$ A $$: 행동 집합  
  - $$ P(s'|s,a) $$: 상태 전이 확률  
  - $$ R(s,a) $$: 보상 함수  
  - $$ \gamma $$: 할인율 (discount factor)  

- 반환 (Return):  
  $$ G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1} $$  

---

## 4. 가치 함수 (Value Functions)
- 상태 가치 함수 (State-value):  
  $$ V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ] $$  
- 행동 가치 함수 (Action-value):  
  $$ Q^\pi(s,a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ] $$  
- 어드밴티지 함수 (Advantage):  
  $$ A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$  

---

## 5. 벨만 방정식 (Bellman Equations)
- 기대 벨만 방정식 (Bellman Expectation):  
  $$ V^\pi(s) = \sum_a \pi(a|s) \Big[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s') \Big] $$  

- 최적 가치 (Optimal Value):  
  $$ V^*(s) = \max_\pi V^\pi(s) $$  
  $$ Q^*(s,a) = \max_\pi Q^\pi(s,a) $$  

- 최적 정책 (Optimal Policy):  
  $$ \pi^*(s) = rg\max_a Q^*(s,a) $$  

---

## 6. 동적 프로그래밍 (Dynamic Programming)
- 정책 반복 (Policy Iteration): Policy Evaluation ↔ Policy Improvement  
- 가치 반복 (Value Iteration):  
  $$ V_{k+1}(s) = \max_a \Big[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s') \Big] $$  

---

## 7. 경험으로부터 학습 (Learning from Experience)
- 몬테카를로 방법 (Monte Carlo Methods)  
  - 전체 episode 완료 후 평균 return 기반  
- 시간차 학습 (Temporal Difference, TD)  
  - 매 step 업데이트  
  - $$ V(S_t) \leftarrow V(S_t) + lpha ig( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) ig) $$  

---

## 8. 가치 기반 방법 (Value-Based Methods)
- SARSA (On-policy):  
  $$ Q(s,a) \leftarrow Q(s,a) + lpha ig( r + \gamma Q(s',a') - Q(s,a) ig) $$  

- Q-Learning (Off-policy):  
  $$ Q(s,a) \leftarrow Q(s,a) + lpha ig( r + \gamma \max_{a'} Q(s',a') - Q(s,a) ig) $$  

- 차이: SARSA = 안전/보수적, Q-Learning = 최적/공격적  

---

## 9. 함수 근사 (Function Approximation)
- 상태 공간이 매우 클 때, 함수 근사 사용  
- 선형 모델 (Linear Models)  
  $$ V_	heta(s) = 	heta^	op \phi(s) $$  
- 신경망 기반 (Deep Q-Network, DQN)  
  $$ Q_	heta(s,a) pprox Q^*(s,a) $$  
- Experience Replay + Target Network 안정화 기법 적용  

---

## 10. 심층 강화학습 (Deep Reinforcement Learning)
- DQN (Deep Q-Network)  
  - Atari 게임 플레이 (픽셀 → 상태)  
  - 보상 기반으로 CNN 학습  
- 정책 기반 (Policy Gradient)  
  - 직접 정책 $$ \pi_	heta(a|s) $$ 학습  
- 액터-크리틱 (Actor-Critic)  
  - Policy + Value function 병합 (예: PPO, TRPO)  
- **게임 (Games):** Atari, Go, StarCraft II  
- **로보틱스 (Robotics):** Motor control, Navigation  
- **비즈니스 (Business):** 재고 관리, 추천 시스템  
- 
---

## 11. 탐험 전략 (Exploration Strategies)
- **ε-greedy**: 확률 ε로 랜덤, 1−ε로 greedy  
- **Softmax Policy**:  
  $$ \pi(a|s) = rac{\exp(Q(s,a)/	au)}{\sum_{a'} \exp(Q(s,a')/	au)} $$  
  (τ = Temperature, 클수록 탐험 ↑)  

---

## 12. On-policy vs Off-policy
- **On-policy (SARSA):** 실제 정책이 탐험을 포함 → 안정적, 안전성 고려  
- **Off-policy (Q-learning):** 최적 정책 기준 학습 → 빠른 수렴, 공격적  

---

## 13. 심층 Q-네트워크 (Deep Q-Network, DQN)
- Q함수를 CNN으로 근사  
- 핵심 기법:  
  1. **Experience Replay** (샘플 decorrelation)  
  2. **Target Network** (안정성 확보)  
- 손실 함수:  
  $$ J(	heta) = (r + \gamma \max_{a'} Q_{	heta^-}(s',a') - Q_	heta(s,a))^2 $$  

---

## 14. 정책 기반 방법 (Policy Gradient)
- 정책을 직접 최적화:  
  $$ 
abla_	heta J(	heta) = \mathbb{E}_\pi [ 
abla_	heta \log \pi_	heta(a|s) G_t ] $$  
- REINFORCE 알고리즘 기반  

---

## 15. 액터-크리틱 (Actor-Critic)
- Policy (Actor) + Value function (Critic) 결합  
- Critic이 Advantage $$ A(s,a) $$ 추정 → Actor 업데이트 안정화  
- 대표 알고리즘: **A2C, A3C, PPO, TRPO**  

---

## 16. 결론 (Conclusion)
- RL = 상태, 행동, 보상 기반의 학습  
- MDP와 벨만 방정식이 이론적 토대  
- 강화학습 = 에이전트가 환경과 상호작용하며 보상 최대화 학습  
- 토대: MDP + Bellman Equation  
- 전통적 방법: MC, TD, SARSA, Q-Learning  
- 현대적 방법: DQN, Policy Gradient, Actor-Critic  
- 응용 분야 확장: 로보틱스, 게임, 금융, 운영 최적화  
