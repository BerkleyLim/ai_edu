# 25일차 - 250801

# Recurrent Neural Networks (RNNs) 정리

## 1. RNN 개요

RNN은 시퀀스 데이터 처리를 위한 딥러닝 모델로, 시간적 또는 순차적 데이터에서 장기적 의존성 학습이 가능함.

* 응용 분야: 음성인식, 기계번역, 감정 분석, 음악 생성, 영상 분류, 이름 개체 인식 등

## 2. RNN의 기본 구조

### Vanilla RNN

* 현재 시점(hidden state)은 이전 시점의 정보와 입력 데이터를 함께 사용하여 생성
* 시퀀스의 길이가 변해도 사용 가능하며, 파라미터 공유로 효율적 학습 가능

## 3. Backpropagation Through Time (BPTT)

* 시계열 데이터를 다룰 때의 역전파 알고리즘
* 기울기 소멸 및 폭발 문제 존재

### 기울기 소멸 및 폭발 해결법

* 기울기 클리핑 (Gradient Clipping)
* ReLU 활성화 함수 사용
* LSTM이나 GRU와 같은 특수한 구조 사용

## 4. Bidirectional RNN

* 순방향과 역방향 정보를 모두 활용하여 시퀀스 데이터의 양방향 맥락을 학습

## 5. Long Short-Term Memory (LSTM)

* Vanilla RNN의 장기 의존성 문제를 해결하는 게이트 메커니즘 도입

  * Input gate: 입력 제어
  * Forget gate: 오래된 정보 제거
  * Output gate: 출력 정보 제어
* 더 긴 시퀀스에 대한 의존성 학습 가능

## 6. LSTM Autoencoder

* 시퀀스 데이터를 압축된 표현으로 인코딩한 후 이를 다시 디코딩하여 원본 데이터를 복원하는 구조
* Unsupervised learning에 활용 가능하며, 동영상 등 복잡한 데이터에서 효과적

## 7. Training 방법

### Teacher Forcing

* 학습 시 모델의 예측 대신 실제 값을 입력으로 사용하여 학습 속도를 높임
* 단점: 실제 예측 상황과 괴리 발생

### Scheduled Sampling

* 학습 초기엔 실제 값을 주로 사용하고, 점진적으로 모델 예측값을 사용하는 비율을 늘려 실제 상황과의 괴리 문제 해결

## 8. Attention 메커니즘

* 모델이 전체 입력 데이터에서 중요한 부분에 집중할 수 있도록 함
* 예시: 기계 번역, 이미지 캡션 생성 등에서 활용

## 9. Sequence-to-Sequence 모델

* 입력 시퀀스를 하나의 고정된 벡터로 압축하고, 이를 기반으로 새로운 시퀀스를 생성하는 Encoder-Decoder 구조
* Attention 메커니즘을 통해 각 디코딩 단계에서 입력의 중요한 부분에 집중 가능

## 10. 응용 사례

* 음성 인식, 기계 번역, 감정 분석, 이미지 캡션 생성, 시계열 예측 등 다양한 영역에서 널리 사용됨.
