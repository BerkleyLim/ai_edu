# 22ì¼ì°¨ - 250729

# Optimization & Regularization ì •ë¦¬

## Adam Optimizer

* **í•µì‹¬ ê°œë…**: 1ì°¨ ë° 2ì°¨ ëª¨ë©˜íŠ¸ë¥¼ ì´ìš©í•˜ì—¬ ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ê°œë³„ í•™ìŠµë¥ ì„ ì¡°ì •í•¨.
* **êµ¬ì„±ìš”ì†Œ**:

  * Momentum (1ì°¨ ëª¨ë©˜íŠ¸)
  * RMSProp (2ì°¨ ëª¨ë©˜íŠ¸)
  * Bias Correction (íŽ¸í–¥ ë³´ì •)
* **ì—…ë°ì´íŠ¸ ê³µì‹**:
  $\theta_t = \theta_{t-1} - \alpha \frac{\hat{v}_t}{\sqrt{\hat{r}_t} + \epsilon}$

## SGD + $\ell_2$ Regularization

* **$\ell_2$** íŒ¨ë„í‹°ëŠ” í° ê°€ì¤‘ì¹˜ ê°’ì„ ì–µì œí•˜ëŠ” ë° ë„ì›€.
* **ì—…ë°ì´íŠ¸ ê³µì‹**:
  $\theta_t = (1 - \alpha \lambda)\theta_{t-1} - \alpha \nabla J(\theta_{t-1})$
* $\lambda$: ì •ê·œí™” ê³„ìˆ˜ (weight decay ì—­í• )

## Adam with $\ell_2$ Regularization

* ì¼ë°˜ Adamì—ì„œëŠ” $r_t$ì´ í¬ë©´ ì •ê·œí™” íš¨ê³¼ê°€ ì¤„ì–´ë“œëŠ” ë¬¸ì œ ë°œìƒ
* **AdamW** ì œì•ˆ: í•™ìŠµë¥ ê³¼ ì •ê·œí™”ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë³´ë‹¤ ì•ˆì •ì ì¸ ì¼ë°˜í™” ì„±ëŠ¥ ì œê³µ
* **AdamW ì—…ë°ì´íŠ¸ ê³µì‹**:
  $\theta_t = \theta_{t-1} - \alpha \left( \frac{\hat{v}_t}{\sqrt{\hat{r}_t} + \epsilon} + \lambda \theta_{t-1} \right)$

## Learning Rate ì „ëžµ

* **ì¢‹ì€ í•™ìŠµë¥ **: ë„ˆë¬´ ìž‘ìœ¼ë©´ ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¼, ë„ˆë¬´ í¬ë©´ ë°œì‚° ê°€ëŠ¥
* **Decay ì „ëžµ**:

  * ê³ ì • ë¹„ìœ¨ ê°ì†Œ: $\alpha_t = 0.95^t \alpha_0$
  * ë£¨íŠ¸ ê°ì†Œ: $\alpha_t = \frac{k}{\sqrt{t}}$
  * ìŠ¤í… ê°ì†Œ: ì¼ì • ì£¼ê¸°ë¡œ $\alpha$ë¥¼ ê¸‰ê²©ížˆ ë‚®ì¶¤
  * ì£¼ê¸°ì  ê°ì†Œ(Cyclical): ì‚¼ê°í˜• í˜•íƒœë¡œ ë°˜ë³µ

## Dropout

* **ë¬¸ì œ**: Deep netì€ co-adaptationê³¼ overfitting ë°œìƒ
* **í•´ê²°**: í•™ìŠµ ì‹œ ì¼ë¶€ ë‰´ëŸ°ì„ í™•ë¥ ì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ë‹¤ì–‘í•œ ì„œë¸Œ ëª¨ë¸ í•™ìŠµ íš¨ê³¼ ìœ ë„
* **í…ŒìŠ¤íŠ¸ ì‹œ**: ì „ì²´ ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©í•˜ë˜ ê°€ì¤‘ì¹˜ë¥¼ $p$ë§Œí¼ ì¤„ì—¬ ì˜ˆì¸¡ì„ í‰ê· í™”í•œ íš¨ê³¼

## Batch Normalization (BN)

* **ëª©í‘œ**: Internal Covariate Shift ë°©ì§€ â†’ ë¹ ë¥¸ ìˆ˜ë ´
* **ê³¼ì •**:

  1. ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê· , ë¶„ì‚° ê³„ì‚°
  2. ì •ê·œí™” í›„ scale($\gamma$)ê³¼ shift($\beta$) ì ìš©
* **ì¶”ë¡  ì‹œ ì²˜ë¦¬**: ì „ì²´ í‰ê· ê³¼ ë¶„ì‚°ì„ ì´ìš©í•´ ì •ê·œí™”
* **íš¨ê³¼**:

  * ë” í° í•™ìŠµë¥  ì‚¬ìš© ê°€ëŠ¥
  * Dropout ì—†ì´ë„ ì¢‹ì€ ì¼ë°˜í™” ê°€ëŠ¥
  * $\ell_2$ ì •ê·œí™” í•„ìš”ì„± ê°ì†Œ


### ì°¸ê³ ë¬¸í—Œ

* D. P. Kingma & J. Ba, 2015, "Adam: A Method for Stochastic Optimization"
* I. Loshchilov & F. Hutter, 2019, "Decoupled Weight Decay Regularization"
* S. Ioffe & C. Szegedy, 2015, "Batch Normalization"
* G. Hinton et al., 2014, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"

---




# ðŸ“š Layer Normalization ì •ë¦¬ (Markdown íŽ¸ì§‘ê¸°ìš©)

## ðŸ” ì™œ Batch Normalizationì´ í•­ìƒ ì ì ˆí•˜ì§€ ì•Šì€ê°€?

* **BNì€ ìž…ë ¥ í†µê³„ëŸ‰ì˜ ì´ë™ í‰ê· ì„ ìœ ì§€í•´ì•¼ í•¨**
* **Feed-forward ë„¤íŠ¸ì›Œí¬**ì—ì„œëŠ” ì¸µë§ˆë‹¤ í†µê³„ë¥¼ ë”°ë¡œ ì €ìž¥í•˜ê¸° ì‰¬ì›€
* **RNNì— ì ìš© ì–´ë ¤ì›€**: ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ ìž…ë ¥ í•©ê³„ê°€ ë‹¬ë¼ì ¸ì„œ ì‹œê°„ ë‹¨ìœ„ë§ˆë‹¤ ë‹¤ë¥¸ í†µê³„ë¥¼ ìš”êµ¬í•¨
* **ë°°ì¹˜ í¬ê¸° í•˜í•œ ì¡´ìž¬**: BNì€ ì˜¨ë¼ì¸ í•™ìŠµì—ì„œëŠ” ì ì ˆí•˜ì§€ ì•ŠìŒ


## âœ… Layer Normalizationì˜ ê°œë…

* ìž…ë ¥ í†µê³„ëŸ‰ì˜ ë¶„í¬ ë³€í™”(covariate shift)ë¥¼ ì¤„ì´ê¸° ìœ„í•´ í•œ ì¸µ ë‚´ ëª¨ë“  ë‰´ëŸ° ë‹¨ìœ„ì—ì„œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì •ê·œí™”í•¨

### ê³„ì‚° ë°©ë²•

* í•œ ì¸µ ë‚´ hidden unitë“¤ì˜ í‰ê·  ë° ë¶„ì‚° ê³„ì‚°:

  $\mu_t^{(l)} = \frac{1}{H} \sum_{i=1}^{H} a_{i,t}^{(l)} \quad \text{and} \quad \sigma_t^{(l)} = \sqrt{\frac{1}{H} \sum_{i=1}^{H} (a_{i,t}^{(l)} - \mu_t^{(l)})^2}$

* ì •ê·œí™” ë° í™œì„±í™”:

  $\mathbf{h}_t = \varphi\left( \frac{\gamma}{\sigma_t^{(l)}} (\mathbf{a}_t - \mu_t^{(l)}\mathbf{1}) + \mathbf{b} \right)$


## ðŸ”„ BatchNorm vs LayerNorm ì‹œê°ì  ë¹„êµ

* **BN**: ë°°ì¹˜ ê¸°ì¤€ í†µê³„ë¡œ ì •ê·œí™”
* **LN**: ë‰´ëŸ° ê¸°ì¤€ í†µê³„ë¡œ ì •ê·œí™”

ì´ë¯¸ì§€ ì°¸ê³ : ê°ê¸° ë‹¤ë¥¸ ë°©í–¥ìœ¼ë¡œ ì •ê·œí™” ì¶•ì´ ì„¤ì •ë¨


## ðŸ”¬ MS COCO ì‹¤í—˜ ê²°ê³¼ ìš”ì•½

* ì‹¤í—˜ ëŒ€ìƒ: ì´ë¯¸ì§€ì™€ ë¬¸ìž¥ì„ ì¡°í•©í•œ joint embedding í•™ìŠµ
* **LN-LSTM**ì€ **BN-LSTM** ëŒ€ë¹„ ìˆ˜ë ´ì€ ëŠë¦¬ì§€ë§Œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ìš°ìˆ˜í•¨


## ðŸ§® CNNì—ì„œì˜ ì •ê·œí™” ë°©ë²• ë¹„êµ

ìž…ë ¥ í…ì„œ: $\mathbf{a} \in \mathbb{R}^{B \times C \times W \times H}$

### Batch Normalization

* í‰ê· : $\mu_{BN} = \frac{1}{BWH} \sum_{bwh} a_{bcwh}$
* ë¶„ì‚°: $\sigma_{BN}^2 = \frac{1}{BWH} \sum_{bwh} (a_{bcwh} - \mu_{BN})^2$

### Layer Normalization

* í‰ê· : $\mu_{LN} = \frac{1}{CWH} \sum_{cwh} a_{bcwh}$
* ë¶„ì‚°: $\sigma_{LN}^2 = \frac{1}{CWH} \sum_{cwh} (a_{bcwh} - \mu_{LN})^2$

### Instance Normalization

* í‰ê· : $\mu_{IN} = \frac{1}{WH} \sum_{wh} a_{bcwh}$
* ë¶„ì‚°: $\sigma_{IN}^2 = \frac{1}{WH} \sum_{wh} (a_{bcwh} - \mu_{IN})^2$


> **ì°¸ê³ ë¬¸í—Œ**: J. L. Ba, J. R. Kiros, and G. E. Hinton (2016), "Layer normalization," arXiv:1607.06450.
