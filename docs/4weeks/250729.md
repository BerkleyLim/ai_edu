# 22일차 - 250729

# Optimization & Regularization 정리

## Adam Optimizer

* **핵심 개념**: 1차 및 2차 모멘트를 이용하여 각 파라미터마다 개별 학습률을 조정함.
* **구성요소**:

  * Momentum (1차 모멘트)
  * RMSProp (2차 모멘트)
  * Bias Correction (편향 보정)
* **업데이트 공식**:
  $\theta_t = \theta_{t-1} - \alpha \frac{\hat{v}_t}{\sqrt{\hat{r}_t} + \epsilon}$

## SGD + $\ell_2$ Regularization

* **$\ell_2$** 패널티는 큰 가중치 값을 억제하는 데 도움.
* **업데이트 공식**:
  $\theta_t = (1 - \alpha \lambda)\theta_{t-1} - \alpha \nabla J(\theta_{t-1})$
* $\lambda$: 정규화 계수 (weight decay 역할)

## Adam with $\ell_2$ Regularization

* 일반 Adam에서는 $r_t$이 크면 정규화 효과가 줄어드는 문제 발생
* **AdamW** 제안: 학습률과 정규화를 분리하여 보다 안정적인 일반화 성능 제공
* **AdamW 업데이트 공식**:
  $\theta_t = \theta_{t-1} - \alpha \left( \frac{\hat{v}_t}{\sqrt{\hat{r}_t} + \epsilon} + \lambda \theta_{t-1} \right)$

## Learning Rate 전략

* **좋은 학습률**: 너무 작으면 수렴 속도가 느림, 너무 크면 발산 가능
* **Decay 전략**:

  * 고정 비율 감소: $\alpha_t = 0.95^t \alpha_0$
  * 루트 감소: $\alpha_t = \frac{k}{\sqrt{t}}$
  * 스텝 감소: 일정 주기로 $\alpha$를 급격히 낮춤
  * 주기적 감소(Cyclical): 삼각형 형태로 반복

## Dropout

* **문제**: Deep net은 co-adaptation과 overfitting 발생
* **해결**: 학습 시 일부 뉴런을 확률적으로 제거하여 다양한 서브 모델 학습 효과 유도
* **테스트 시**: 전체 네트워크 사용하되 가중치를 $p$만큼 줄여 예측을 평균화한 효과

## Batch Normalization (BN)

* **목표**: Internal Covariate Shift 방지 → 빠른 수렴
* **과정**:

  1. 배치 단위로 평균, 분산 계산
  2. 정규화 후 scale($\gamma$)과 shift($\beta$) 적용
* **추론 시 처리**: 전체 평균과 분산을 이용해 정규화
* **효과**:

  * 더 큰 학습률 사용 가능
  * Dropout 없이도 좋은 일반화 가능
  * $\ell_2$ 정규화 필요성 감소


### 참고문헌

* D. P. Kingma & J. Ba, 2015, "Adam: A Method for Stochastic Optimization"
* I. Loshchilov & F. Hutter, 2019, "Decoupled Weight Decay Regularization"
* S. Ioffe & C. Szegedy, 2015, "Batch Normalization"
* G. Hinton et al., 2014, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"

---




# 📚 Layer Normalization 정리 (Markdown 편집기용)

## 🔍 왜 Batch Normalization이 항상 적절하지 않은가?

* **BN은 입력 통계량의 이동 평균을 유지해야 함**
* **Feed-forward 네트워크**에서는 층마다 통계를 따로 저장하기 쉬움
* **RNN에 적용 어려움**: 시퀀스 길이에 따라 입력 합계가 달라져서 시간 단위마다 다른 통계를 요구함
* **배치 크기 하한 존재**: BN은 온라인 학습에서는 적절하지 않음


## ✅ Layer Normalization의 개념

* 입력 통계량의 분포 변화(covariate shift)를 줄이기 위해 한 층 내 모든 뉴런 단위에서 평균과 분산을 정규화함

### 계산 방법

* 한 층 내 hidden unit들의 평균 및 분산 계산:

  $\mu_t^{(l)} = \frac{1}{H} \sum_{i=1}^{H} a_{i,t}^{(l)} \quad \text{and} \quad \sigma_t^{(l)} = \sqrt{\frac{1}{H} \sum_{i=1}^{H} (a_{i,t}^{(l)} - \mu_t^{(l)})^2}$

* 정규화 및 활성화:

  $\mathbf{h}_t = \varphi\left( \frac{\gamma}{\sigma_t^{(l)}} (\mathbf{a}_t - \mu_t^{(l)}\mathbf{1}) + \mathbf{b} \right)$


## 🔄 BatchNorm vs LayerNorm 시각적 비교

* **BN**: 배치 기준 통계로 정규화
* **LN**: 뉴런 기준 통계로 정규화

이미지 참고: 각기 다른 방향으로 정규화 축이 설정됨


## 🔬 MS COCO 실험 결과 요약

* 실험 대상: 이미지와 문장을 조합한 joint embedding 학습
* **LN-LSTM**은 **BN-LSTM** 대비 수렴은 느리지만 일반화 성능이 우수함


## 🧮 CNN에서의 정규화 방법 비교

입력 텐서: $\mathbf{a} \in \mathbb{R}^{B \times C \times W \times H}$

### Batch Normalization

* 평균: $\mu_{BN} = \frac{1}{BWH} \sum_{bwh} a_{bcwh}$
* 분산: $\sigma_{BN}^2 = \frac{1}{BWH} \sum_{bwh} (a_{bcwh} - \mu_{BN})^2$

### Layer Normalization

* 평균: $\mu_{LN} = \frac{1}{CWH} \sum_{cwh} a_{bcwh}$
* 분산: $\sigma_{LN}^2 = \frac{1}{CWH} \sum_{cwh} (a_{bcwh} - \mu_{LN})^2$

### Instance Normalization

* 평균: $\mu_{IN} = \frac{1}{WH} \sum_{wh} a_{bcwh}$
* 분산: $\sigma_{IN}^2 = \frac{1}{WH} \sum_{wh} (a_{bcwh} - \mu_{IN})^2$


> **참고문헌**: J. L. Ba, J. R. Kiros, and G. E. Hinton (2016), "Layer normalization," arXiv:1607.06450.
