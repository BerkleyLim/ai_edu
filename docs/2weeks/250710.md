# 6ì¼ì°¨ - 250710

## 5ì¼ì°¨ - 250709 ì´ì–´ì„œ ì‘ì„±
## (Shannon Entropy) - ì„¸ë„Œ ì—”íŠ¸ë¡œí”¼
- Average information (í‰ê·  ì •ë³´)
- 
$$
\begin{aligned}
H(p) &= \mathbb{E}_p[I(x)] \\
     &= \mathbb{E}_p[-\log p(x)] \\
     &= -\sum_{x \in \mathcal{X}} p(x) \log p(x)
\end{aligned}
$$

![img.png](250709_7.png)


### Information
![img.png](250709_8.png)


### Bit (binary digit)
![img.png](250709_9.png)


### Nat (Natural unit of information)
![img.png](250709_10.png)


### Excample: Entropo 1 - Sunny 50%, Rainy 50% í™•ë¥ 
$$
H(p) = -\left( \frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{2} \log_2 \frac{1}{2} \right)
= -\left( \frac{1}{2} \cdot (-1) + \frac{1}{2} \cdot (-1) \right)
= 1 \text{ bit}
$$


### Example: Entropy (Cont'd) - Sunny 75%, Rainy 25% í™•ë¥ 
$$
H(p) = -\left( \frac{3}{4} \log_2 \frac{3}{4} + \frac{1}{4} \log_2 \frac{1}{4} \right)
= -\left( \frac{3}{4} \log_2 \frac{3}{4} + \frac{1}{4} \cdot (-2) \right)
\approx 0.81 \text{ bit}
$$


### Information Theory
1.	**ìµœëŒ€ ë°ì´í„° ì••ì¶•(ultimate data compression)**ì€?
   â†’ ì—”íŠ¸ë¡œí”¼ H
2.	**í†µì‹ ì˜ ìµœëŒ€ ì „ì†¡ë¥ (ultimate transmission rate)**ì€?
   â†’ ì±„ë„ ìš©ëŸ‰ C

3. í™œìš©ìš©ë„
   - ìƒíƒœí•™ì  ê°ê° ì²˜ë¦¬ ì´ë¡ (ecological sensory processing) ì„¤ëª…ì—ë„ ìœ ìš©
   - ë¹„ì§€ë„ í•™ìŠµ(unsupervised learning) ëª©í‘œë¥¼ ì„¤ëª…í•˜ëŠ” ë°ë„ í•µì‹¬ì ì¸ ì—­í• ì„ í•¨


### Information and Entropy

- ğŸ”¹ ì •ë³´(Information)ì˜ ì •ì˜
	- ì •ë³´ëŠ” ë†€ëŒ, ë¶ˆí™•ì‹¤ì„±, ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŒ

$$
I = -\log p_i
$$

	- ì—¬ê¸°ì„œ p_iëŠ” ì‚¬ê±´ iê°€ ì¼ì–´ë‚  í™•ë¥ 
    - ì¦‰:
	    - ë“œë¬¸ ì‚¬ê±´: ë†’ì€ ì •ë³´ëŸ‰ (surprise â†‘)
	    - ìì£¼ ì¼ì–´ë‚˜ëŠ” ì‚¬ê±´: ë‚®ì€ ì •ë³´ëŸ‰ (surprise â†“)

- ğŸ”¹ ì—”íŠ¸ë¡œí”¼(Entropy)ì˜ ì •ì˜
	- ì •ë³´ì˜ í‰ê· ê°’ (Average Information)

$$
H = \mathbb{E}[I] = - \sum_{i=1}^{N} p_i \log p_i
$$

	- ì¦‰, ì—”íŠ¸ë¡œí”¼ëŠ” í™•ë¥  ë¶„í¬ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œì´ë©°, ì •ë³´ ì´ë¡ ì˜ í•µì‹¬ ê°œë…ì…ë‹ˆë‹¤.

# Shannon Entropy
## ì •ì˜

ì£¼ì–´ì§„ ì´ì‚° í™•ë¥  ë³€ìˆ˜ \( X \)ì™€ ê·¸ ê°’ë“¤ì˜ ì§‘í•© \( \mathcal{X} \)ì— ëŒ€í•´,  
**Shannon Entropy**ëŠ” í‰ê·  ì •ë³´ëŸ‰(ë¶ˆí™•ì‹¤ì„±ì˜ ì²™ë„)ìœ¼ë¡œ ì •ì˜ëœë‹¤.

$$
H(p) = - \sum_{x \in \mathcal{X}} p(x) \log p(x) = \mathbb{E}_p[-\log p(x)]
$$


## ì„±ì§ˆ (Properties)

- \( H(p) \geq 0 \)  
  (ëª¨ë“  í•­ì´ 0 ì´ìƒì˜ ê°’ì„ ê°€ì§€ë¯€ë¡œ)

- \( H(p) = 0 \)  
  â†³ ì–´ë–¤ \( x \in \mathcal{X} \)ì— ëŒ€í•´ \( \mathbb{P}(X = x) = 1 \)ì¼ ë•Œë§Œ ê°€ëŠ¥

- ì—”íŠ¸ë¡œí”¼ëŠ” **ëª¨ë“  ê²°ê³¼ê°€ ê· ë“±í•  ë•Œ ìµœëŒ€**


## ì‹œê°ì  ì˜ˆì‹œ

- ë¶„í¬ \( p = [0.8, 0.1, 0.1] \) â†’ ë¶ˆê· í˜• â†’ \( H(p) \) ì‘ìŒ  
- ë¶„í¬ \( q = [\frac{1}{3}, \frac{1}{3}, \frac{1}{3}] \) â†’ ê· ë“± â†’ \( H(q) \) í¼

$$
H(p) < H(q)
$$


### ğŸ ì˜ˆì œ: ë§ ê²½ì£¼ì™€ ì—”íŠ¸ë¡œí”¼

8ë§ˆë¦¬ì˜ ë§ì´ ê²½ì£¼ì— ì°¸ì—¬í•˜ê³ , ê° ë§ì´ ì´ê¸¸ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ë§¤ìš° ë¶ˆê· ë“±í•˜ë‹¤:

$$
\left( \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64}, \frac{1}{64} \right)
$$

ì´ì œ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

> "ì´ê¸´ ë§ì„ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ì „ë‹¬í•˜ë ¤ë©´ ëª‡ ë¹„íŠ¸ê°€ í•„ìš”í• ê¹Œ?"


#### ğŸ”º ê· ë“±í•˜ê²Œ ìƒê°í•˜ë©´ í‹€ë¦° ë‹µì„ ë‚¸ë‹¤

ëª¨ë“  ë§ì´ ê°™ì€ í™•ë¥ ë¡œ ì´ê¸´ë‹¤ê³  ê°€ì •í•˜ë©´,  
ì´ 8ê°€ì§€ ê²½ìš° â†’ \( \log_2 8 = 3 \)ë¹„íŠ¸ í•„ìš”.

í•˜ì§€ë§Œ ì‹¤ì œ í™•ë¥ ì€ **í¸ì¤‘ë˜ì–´ ìˆìŒ**  
â†’ ì¦‰, **ë” ìì£¼ ì¼ì–´ë‚˜ëŠ” ë§ì€ ë” ì§§ê²Œ**,  
â†’ **ë“œë¬¼ê²Œ ì´ê¸°ëŠ” ë§ì€ ë” ê¸¸ê²Œ** í‘œí˜„í•˜ëŠ” ê²Œ ìœ ë¦¬.


#### âš™ï¸ íš¨ìœ¨ì  í‘œí˜„ ë°©ì‹ (Variable-length Encoding)

í™•ë¥ ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë¹„íŠ¸ ë¬¸ìì—´ì„ ë°°ì •í•˜ë©´:

```
ë§ 1: 0
ë§ 2: 10
ë§ 3: 110
ë§ 4: 1110
ë§ 5~8: 111100, 111101, 111110, 111111
```

ì´ë ‡ê²Œ í•˜ë©´ í‰ê·  í‘œí˜„ ê¸¸ì´ëŠ” **2ë¹„íŠ¸** ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤.  
ì´ëŠ” ê³ ì • 3ë¹„íŠ¸ ë°©ì‹ë³´ë‹¤ íš¨ìœ¨ì ì´ë‹¤.


#### ğŸ“‰ í‰ê·  ì •ë³´ëŸ‰ (ì—”íŠ¸ë¡œí”¼ ê³„ì‚°)

ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤:

$$
\begin{aligned}
H &= - \left( 
\frac{1}{2} \log_2 \frac{1}{2}
+ \frac{1}{4} \log_2 \frac{1}{4}
+ \frac{1}{8} \log_2 \frac{1}{8}
+ \frac{1}{16} \log_2 \frac{1}{16}
+ 4 \cdot \frac{1}{64} \log_2 \frac{1}{64}
\right) \\
&= 2 \text{ bits}
\end{aligned}
$$


#### ğŸ§  í•µì‹¬ ë©”ì‹œì§€

- **ì—”íŠ¸ë¡œí”¼ëŠ” í‰ê·  ë¹„íŠ¸ ìˆ˜ì˜ ì´ë¡ ì  ìµœì†Œê°’**
- ì¦‰, ì•„ë¬´ë¦¬ ì½”ë”©ì„ ì˜í•´ë„ í‰ê· ì ìœ¼ë¡œ **2ë¹„íŠ¸ë³´ë‹¤ ë” ì ê²Œ** í‘œí˜„í•  ìˆ˜ëŠ” ì—†ìŒ
- ì´ëŠ” "ìŠ¤ë¬´ê³ ê°œ"ì™€ ê°™ì€ ë¬¸ì œì—ì„œ **ìµœì†Œ ì§ˆë¬¸ ìˆ˜ì˜ í•˜í•œì„ **ìœ¼ë¡œë„ í•´ì„ ê°€ëŠ¥


---

## ğŸ“Š Differential Entropy

ì—°ì† í™•ë¥  ë³€ìˆ˜ \( X \)ì— ëŒ€í•´,  
**Differential Entropy**ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

$$
H(p) = - \int p(x) \log p(x) \, dx = \mathbb{E}_p[-\log p(x)]
$$


### ğŸ“Œ ì£¼ìš” íŠ¹ì„± (Properties)

- ê²½ìš°ì— ë”°ë¼ **ìŒìˆ˜ê°€ ë  ìˆ˜ë„ ìˆìŒ**
- **ê³ ì •ëœ ë¶„ì‚°**ì„ ê°€ì§„ ë¶„í¬ ì¤‘ì—ì„œëŠ”  
  â†’ **ê°€ìš°ì‹œì•ˆ ë¶„í¬(Gaussian)**ê°€ **ìµœëŒ€ ì—”íŠ¸ë¡œí”¼**ë¥¼ ê°€ì§
- ë‹¨ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ \( X \sim \mathcal{N}(\mu, \sigma^2) \)ì¼ ë•Œ:

  $$
  H(X) = \frac{1}{2} \log (2\pi e \sigma^2)
  $$

- ë‹¤ë³€ëŸ‰ ê°€ìš°ì‹œì•ˆ \( X \sim \mathcal{N}(\mu, \Sigma) \)ì¼ ë•Œ:

  $$
  H(X) = \frac{1}{2} \log \left( (2\pi e)^d \cdot |\Sigma| \right)
       = \frac{1}{2} \log \left( 2\pi e \right)^d |\Sigma|
       = \frac{1}{2} \log |2\pi e \Sigma|
  $$



### ğŸ§® ê³„ì‚° ì˜ˆì‹œ: \( H(\mathcal{N}(\mu, \Sigma)) \)

ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ì˜ PDF:

$$
p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} 
\exp\left( -\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu) \right)
$$


#### ğŸ”½ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° ì ˆì°¨:

$$
\begin{aligned}
H(X) 
&= \mathbb{E}[-\log p(x)] \\
&= \mathbb{E}\left[ \frac{1}{2} \log |2\pi \Sigma| + \frac{1}{2}(x - \mu)^T \Sigma^{-1}(x - \mu) \right] \\
&= \frac{1}{2} \log |2\pi \Sigma| + \frac{1}{2} \operatorname{tr}\left( \Sigma^{-1} \mathbb{E}[(x - \mu)(x - \mu)^T] \right) \\
&= \frac{1}{2} \log |2\pi \Sigma| + \frac{1}{2} \operatorname{tr}(\Sigma^{-1} \Sigma) \\
&= \frac{1}{2} \log |2\pi \Sigma| + \frac{1}{2} \cdot d \\
&= \frac{1}{2} \log \left( (2\pi e)^d |\Sigma| \right)
\end{aligned}
$$

#### âœ… ê²°ë¡ 

$$
H(\mathcal{N}(\mu, \Sigma)) = \frac{1}{2} \log |2\pi e \Sigma|
$$

â†’ ê°€ìš°ì‹œì•ˆ ë¶„í¬ëŠ” ì£¼ì–´ì§„ ê³µë¶„ì‚° ë‚´ì—ì„œ **ìµœëŒ€ differential entropy**ë¥¼ ê°€ì§„ë‹¤.




### ğŸ”— Joint Entropy (ê²°í•© ì—”íŠ¸ë¡œí”¼)

ë‘ ê°œì˜ ì´ì‚° í™•ë¥  ë³€ìˆ˜ \( X \), \( Y \)ê°€ ìˆì„ ë•Œ,  
ê·¸ ê²°í•© ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
H(X, Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log p(x, y)
$$

ì´ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ì „ê°œë©ë‹ˆë‹¤:


#### ğŸ“ ì „ê°œ (Chain Rule ìœ ë„ ê³¼ì •)

1. **ì—°ì‡„ë²•ì¹™ ì ìš©ì„ ìœ„í•´ ë¶„ë¦¬**:

$$
p(x, y) = p(x) \cdot p(y \mid x)
$$

ë”°ë¼ì„œ,

$$
\begin{aligned}
H(X, Y)
&= - \sum_{x} \sum_{y} p(x, y) \log \left( p(x) \cdot p(y \mid x) \right) \\
&= - \sum_{x} \sum_{y} p(x, y) \log p(x) - \sum_{x} \sum_{y} p(x, y) \log p(y \mid x) \\
&= - \sum_{x} p(x) \log p(x) - \sum_{x} p(x) \sum_{y} p(y \mid x) \log p(y \mid x) \\
&= H(X) + \sum_{x} p(x) H(Y \mid X = x)
\end{aligned}
$$


#### âœ… Chain Rule for Entropy

ê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ **ì—°ì‡„ ë²•ì¹™(chain rule)**ì´ ì„±ë¦½í•©ë‹ˆë‹¤:

$$
H(X, Y) = H(X) + H(Y \mid X)
$$


#### ğŸ“ ì„±ì§ˆ (Properties)

- \( H(X, Y) \leq H(X) + H(Y) \)  
  â†’ ë…ë¦½ì¼ ê²½ìš° ë“±í˜¸ ì„±ë¦½

- \( H(Y \mid X) \leq H(Y) \)  
  â†’ ì¡°ê±´ë¶€ ì •ë³´ëŠ” í•­ìƒ ì „ì²´ ì •ë³´ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ë‹¤

> ğŸ’¡ ìœ„ ë‘ ë¶€ë“±ì‹ì€ ìˆ˜í•™ì ìœ¼ë¡œë„ ì¤‘ìš”í•œ ë¶ˆí‰ë“±ì´ë©°,  
> ì •ë³´ ì´ë¡ ì˜ ì£¼ìš” ì •ë¦¬ì¸ **subadditivity**ì™€ **conditioning reduces entropy**ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

---


# Cross Entropy

## ğŸ“Œ ì •ì˜

ë¶„í¬ \( p(x) \) (true distribution)ì— ëŒ€í•´,  
\( q(x) \) (estimated distribution)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¸ì½”ë”©í–ˆì„ ë•Œ í•„ìš”í•œ **í‰ê·  ì •ë³´ëŸ‰**ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

$$
H(p, q) = \mathbb{E}_p[-\log q(x)] = - \sum_x p(x) \log q(x)
$$

- ì´ëŠ” "ì‹¤ì œëŠ” \( p \), í•˜ì§€ë§Œ ëª¨ë¸ì´ \( q \)ë¼ê³  ë¯¿ê³  ìˆì„ ë•Œ"  
  ìš°ë¦¬ê°€ **ì–¼ë§ˆë‚˜ ì˜ëª» ì¸ì½”ë”©í–ˆëŠ”ê°€?**ë¥¼ ì¸¡ì •í•˜ëŠ” ê°œë…ì´ë‹¤.

---

# ğŸŒ¤ï¸ ì˜ˆì‹œ: ë‚ ì”¨ ì˜ˆì¸¡

### ì‹¤ì œ í™•ë¥  (True \( p \)):

- Sunny: 75%  
- Rainy: 25%

### ëª¨ë¸ ì˜ˆì¸¡ (Estimated \( q \)):

- Sunny: 50%  
- Rainy: 50%

---

### ğŸ”¸ Entropy (ìì²´ ì—”íŠ¸ë¡œí”¼):

$$
H(p) = - \left( \frac{3}{4} \log_2 \frac{3}{4} + \frac{1}{4} \log_2 \frac{1}{4} \right)
= \frac{3}{4} \log_2 \frac{4}{3} + \frac{1}{4} \log_2 4
\approx 0.81 \text{ bit}
$$

---

### ğŸ”¹ Cross Entropy:

$$
H(p, q) = - \left( \frac{3}{4} \log_2 \frac{1}{2} + \frac{1}{4} \log_2 \frac{1}{2} \right)
= \frac{3}{4} \log_2 2 + \frac{1}{4} \log_2 2 = 1 \text{ bit}
$$

â†’ ëª¨ë¸ì´ ì˜ëª»ëœ ë¶„í¬ \( q \)ë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ì •ë³´ëŸ‰(ë¹„íŠ¸ ìˆ˜)ì´ ì¦ê°€.

---

# ğŸ§  ì˜ˆì‹œ: Cross Entropy Error in Classification

### ë¬¸ì œ: 3-class classification  
(target = Tiger, Lion, Cat)

---

## ì˜ˆì‹œ 1

- ì‹¤ì œ í´ë˜ìŠ¤: Tiger â†’ \( y = [1, 0, 0] \)
- ëª¨ë¸ ì¶œë ¥ \( \hat{y} = [0.7,\ 0.2,\ 0.1] \)

### Cross Entropy Error:

$$
- \sum_{i=1}^3 y_i \log \hat{y}_i = - \log 0.7 \approx 0.36
$$

---

## ì˜ˆì‹œ 2

- ì‹¤ì œ í´ë˜ìŠ¤: Tiger â†’ \( y = [1, 0, 0] \)
- ëª¨ë¸ ì¶œë ¥ \( \hat{y} = [0.5,\ 0.3,\ 0.2] \)

### Cross Entropy Error:

$$
- \log 0.5 = 0.69
$$

â†’ ì •ë‹µ í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡ lossëŠ” ì»¤ì§


#### âœ… í•µì‹¬ ìš”ì•½

- Cross EntropyëŠ” ëª¨ë¸ì´ **í‹€ë¦° ë¶„í¬ë¡œ ì–¼ë§ˆë‚˜ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ì¸ì½”ë”©í–ˆëŠ”ê°€**ë¥¼ ì¸¡ì •
- DNNì—ì„œëŠ” **ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ \( \hat{y} \)**ì™€ **ì‹¤ì œ ì •ë‹µ ë¶„í¬ \( y \)** ê°„ì˜ ê±°ë¦¬ë¡œ í•´ì„
- ì •í™•íˆ ì˜ˆì¸¡í• ìˆ˜ë¡ (ì˜ˆ: ì •ë‹µ í™•ë¥ ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡) cross-entropyëŠ” **0ì— ê°€ê¹Œì›Œì§„ë‹¤**





---

# ğŸ”€ KL Divergence (Kullback-Leibler Divergence)

## ğŸ“˜ ê°œë…

- KL DivergenceëŠ” ë‘ í™•ë¥  ë¶„í¬ \( p(x) \), \( q(x) \) ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•
- 1951ë…„, Kullbackê³¼ Leiblerì— ì˜í•´ ì œì•ˆë¨
- ì£¼ë¡œ "ì •ë‹µì€ \( p \), ì˜ˆì¸¡ì€ \( q \)"ì¸ ìƒí™©ì—ì„œ \( q \)ê°€ \( p \)ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ë‚˜íƒ€ëƒ„


## ğŸ§® ì •ì˜

### ì´ì‚° í™•ë¥  ë³€ìˆ˜ì˜ ê²½ìš°:
$$
D_{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)}
$$

### ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ê²½ìš°:
$$
D_{KL}(p \| q) = \int p(x) \log \frac{p(x)}{q(x)} dx
$$


## ğŸ“Œ ì£¼ìš” ì„±ì§ˆ

- ë¹„ëŒ€ì¹­: \( D_{KL}(p \| q) \neq D_{KL}(q \| p) \)
- í•­ìƒ 0 ì´ìƒ (Gibbs' inequality): \( D_{KL}(p \| q) \geq 0 \)
- \( D_{KL}(p \| q) = 0 \) âŸº \( p = q \)
- í™•ë¥  ë¶„í¬ ê³µê°„ì—ì„œ convex í•¨ìˆ˜

---

# ğŸ“Š ì—”íŠ¸ë¡œí”¼ì™€ KL Divergence ê´€ê³„

ì—”íŠ¸ë¡œí”¼ëŠ” ê· ë“±ë¶„í¬ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë²—ì–´ë‚¬ëŠ”ì§€ë¥¼ KL Divergenceë¡œ í•´ì„ ê°€ëŠ¥:

$$
\begin{aligned}
H(p)
&= \sum_x p(x) \log \frac{1}{p(x)} \\
&= \log |\mathcal{X}| - D_{KL}\left(p \| \text{unif} \right)
\end{aligned}
$$

- ì¦‰, **ë¶„í¬ê°€ ê· ë“±í• ìˆ˜ë¡ ì—”íŠ¸ë¡œí”¼ëŠ” ìµœëŒ€**  
- \( p(x) \)ê°€ ì¹˜ìš°ì¹ ìˆ˜ë¡ \( H(p) \)ëŠ” ì‘ì•„ì§€ê³ , \( D_{KL} \)ëŠ” ì»¤ì§


## ğŸ” KL Divergence ì§ê´€ ìš”ì•½

- \( p \)ì™€ \( q \) ëª¨ë‘ ë†’ìœ¼ë©´ â†’ ğŸ‘ OK
- \( p \)ëŠ” ë†’ì€ë° \( q \)ëŠ” ë‚®ìœ¼ë©´ â†’ ğŸ˜¢ í˜ë„í‹° ë°œìƒ
- \( p \)ê°€ ë‚®ìœ¼ë©´ â†’ ğŸ«¤ ì˜í–¥ ì—†ìŒ
- \( D_{KL} = 0 \)ì´ë©´ â†’ \( p = q \)


## ğŸ” ì˜ˆì‹œ: KL(pâ€–q) vs KL(qâ€–p)

ë¶„í¬ ì˜ˆì‹œ:
- \( p = [\frac{1}{3}, \frac{1}{3}, \frac{1}{3}, \varepsilon] \)
- \( q = [\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4}] \)

ê³„ì‚° ê²°ê³¼:

- \( D_{KL}(p \| q) \)ì€ ì‘ìŒ (ê·¼ì‚¬ ì˜í•¨)
- \( D_{KL}(q \| p) \)ì€ í¼ (ê·¼ì‚¬ ì‹¤íŒ¨)

â†’ KLì€ ë¹„ëŒ€ì¹­ì´ë¯€ë¡œ, **ì˜ˆì¸¡ì´ ì •ë‹µ ë¶„í¬ë³´ë‹¤ ë„“ìœ¼ë©´** ë¬¸ì œ ì—†ìŒ  
   ë°˜ëŒ€ë¡œ **ì˜ˆì¸¡ì´ ì¢ì€ë° ì •ë‹µì´ ë„“ìœ¼ë©´** í° í˜ë„í‹° ë°œìƒ

---


# ğŸ“ KL Divergence: ê°€ìš°ì‹œì•ˆ ê°„ì˜ ê±°ë¦¬

## 1. ë‹¨ë³€ëŸ‰ Gaussian (\( \mathcal{N}(\mu, \sigma^2) \)):

\( p = \mathcal{N}(\mu_1, \sigma_1^2),\quad q = \mathcal{N}(\mu_2, \sigma_2^2) \)

$$
D_{KL}(p \| q) = \frac{1}{2} \left(
\frac{\sigma_1^2}{\sigma_2^2}
+ \frac{(\mu_2 - \mu_1)^2}{\sigma_2^2}
+ \log \frac{\sigma_2^2}{\sigma_1^2}
- 1
\right)
$$


## 2. ë‹¤ë³€ëŸ‰ Gaussian (\( \mathcal{N}(\mu, \Sigma) \)):

\( p = \mathcal{N}(\mu_1, \Sigma_1),\quad q = \mathcal{N}(\mu_2, \Sigma_2) \)

$$
D_{KL}(p \| q) = \frac{1}{2} \left[
\text{tr}(\Sigma_2^{-1} \Sigma_1)
+ (\mu_2 - \mu_1)^T \Sigma_2^{-1} (\mu_2 - \mu_1)
- D + \log \frac{|\Sigma_2|}{|\Sigma_1|}
\right]
$$

â†’ \( D \): ì°¨ì›ì˜ ìˆ˜


## â• ê¸°íƒ€ Divergence ì¢…ë¥˜ (More Divergences)

- **Bregman divergence (1967)**  
- **f-divergence (CsiszÃ¡r, Ali-Silvey, 1960s)**  
- **Î±-divergence (Chernoff, Amari, 1950s~)**

ì´ë“¤ì€ KLì„ ì¼ë°˜í™”í•˜ê±°ë‚˜, ë‹¤ë¥¸ ëª©ì ì— ë§ê²Œ ì„¤ê³„ëœ ê±°ë¦¬ ì¸¡ì • ë°©ì‹ì´ë‹¤.




---


# ğŸ¯ Maximum Likelihood & Kullback Matching

## ğŸ“Œ Likelihood

í™•ë¥  ë°€ë„ í•¨ìˆ˜ê°€ íŒŒë¼ë¯¸í„° \( \theta \)ì— ë”°ë¼ ì •í•´ì§„ë‹¤ê³  í•  ë•Œ,  
ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ likelihoodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

- ì¡°ê±´ë¶€ ëª¨ë¸ (supervised learning):

$$
\prod_{n=1}^{N} p(y_n \mid x_n, \theta)
$$

- ë¹„ì§€ë„ ëª¨ë¸ (unsupervised learning):

$$
\prod_{n=1}^{N} p(x_n \mid \theta)
$$


## ğŸ§® Maximum Likelihood Estimation (MLE)

### ì§€ë„ í•™ìŠµ:
$$
\theta_{\text{ML}} = \arg\max_\theta \sum_{n=1}^{N} \log p(y_n \mid x_n, \theta)
$$

### ë¹„ì§€ë„ í•™ìŠµ:
$$
\theta_{\text{ML}} = \arg\max_\theta \sum_{n=1}^{N} \log p(x_n \mid \theta)
$$


## ğŸ¯ ì§ê´€: ëª¨ë¸ ë¶„í¬ â‰ˆ ë°ì´í„° ë¶„í¬

MLEì€ ê²°êµ­ **ëª¨ë¸ ë¶„í¬ê°€ ì‹¤ì œ ë°ì´í„° ë¶„í¬ì™€ ê°€ê¹Œì›Œì§€ë„ë¡** \( \theta \)ë¥¼ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´:

$$
p(y \mid x, \theta) \approx p_{\text{data}}(y \mid x), \quad
p(x \mid \theta) \approx p_{\text{data}}(x)
$$


## ğŸ“ MLE = KL Divergence ìµœì†Œí™” ê´€ì  (Kullback Matching)

MLEì€ ì‚¬ì‹¤ìƒ **ëª¨ë¸ ë¶„í¬ê°€ ë°ì´í„° ë¶„í¬ì— ê°€ê¹Œì›Œì§€ë„ë¡**  
**KL Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •**ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥:

### ë°ì´í„°ì˜ ê²½í—˜ì  ë¶„í¬:

$$
\tilde{p}(x) = \frac{1}{N} \sum_{n=1}^N \delta(x - x_n)
$$

### ëª¨ë¸ ë¶„í¬:

$$
p(x \mid \theta)
$$


## ğŸ§® ëª©í‘œ ì‹: MLEì„ KL ê´€ì ìœ¼ë¡œ í‘œí˜„

MLEì€ ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤:

$$
\theta_{\text{ML}} = \arg\min_\theta D_{KL}\left(\tilde{p}(x) \| p(x \mid \theta)\right)
$$

ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì „ê°œí•˜ë©´:

$$
\begin{aligned}
\theta_{\text{ML}} 
&= \arg\min_\theta \int \tilde{p}(x) \log \frac{\tilde{p}(x)}{p(x \mid \theta)} dx \\
&= \arg\max_\theta \int \tilde{p}(x) \log p(x \mid \theta) dx \\
&= \arg\max_\theta \frac{1}{N} \sum_{n=1}^N \log p(x_n \mid \theta)
\end{aligned}
$$


## âœ… ê²°ë¡ 

MLEì€ ë‹¨ìˆœí•œ ìµœëŒ€í™”ê°€ ì•„ë‹Œ,  
**ë°ì´í„° ë¶„í¬ì™€ ëª¨ë¸ ë¶„í¬ ê°„ì˜ KL Divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •**ì´ë‹¤.

ë”°ë¼ì„œ MLEì€ **"Kullback-Leibler Matching"**ì˜ ì‹œê°ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆìœ¼ë©°,  
ì´ëŸ¬í•œ ê´€ì ì€ **Information Theoryì™€ Machine Learningì˜ ì—°ê²°ê³ ë¦¬**ë¥¼ ì œê³µí•œë‹¤.


---

# ğŸ”— Mutual Information (ìƒí˜¸ ì •ë³´ëŸ‰)

## ğŸ“˜ ì •ì˜

Mutual Informationì€ ë‘ í™•ë¥  ë³€ìˆ˜ \( X \)ì™€ \( Y \) ê°„ì˜ **ì •ë³´ ê³µìœ  ì •ë„**ë¥¼ ì¸¡ì •í•˜ëŠ” ê°’ì´ë‹¤.  
ì´ëŠ” ê²°í•© ë¶„í¬ \( p(x, y) \)ì™€ ì£¼ë³€ ë¶„í¬ \( p(x)p(y) \) ì‚¬ì´ì˜ KL Divergenceë¡œ ì •ì˜ëœë‹¤:

$$
I(X; Y) = D_{KL}(p(x, y) \| p(x)p(y)) = \mathbb{E}_{p(x, y)} \left[ \log \frac{p(x, y)}{p(x)p(y)} \right]
$$

---

## ğŸ“Š ì´ì‚° ë° ì—°ì† í™•ë¥  ë³€ìˆ˜ í‘œí˜„

- ì´ì‚° ë³€ìˆ˜ì˜ ê²½ìš°:

$$
I(X; Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
$$

- ì—°ì† ë³€ìˆ˜ì˜ ê²½ìš°:

$$
I(X; Y) = \int \int p(x, y) \log \frac{p(x, y)}{p(x)p(y)} dx\,dy
$$

---

## ğŸ¯ ì§ê´€ì  í•´ì„: ë¶ˆí™•ì‹¤ì„±ì˜ ê°ì†ŒëŸ‰

Mutual Informationì€ "ì–´ë–¤ ë³€ìˆ˜ í•˜ë‚˜ë¥¼ ì•Œì•˜ì„ ë•Œ, ë‹¤ë¥¸ ë³€ìˆ˜ì— ëŒ€í•œ **ë¶ˆí™•ì‹¤ì„±ì´ ì–¼ë§ˆë‚˜ ì¤„ì–´ë“œëŠ”ê°€**"ë¥¼ ì¸¡ì •í•œë‹¤.

$$
I(X; Y) = H(Y) - H(Y \mid X) = H(X) - H(X \mid Y)
$$

- \( H(Y) \): \( Y \)ì— ëŒ€í•œ ì „ì²´ ë¶ˆí™•ì‹¤ì„±  
- \( H(Y \mid X) \): \( X \)ë¥¼ ì•Œì•˜ì„ ë•Œ \( Y \)ì— ë‚¨ì•„ìˆëŠ” ë¶ˆí™•ì‹¤ì„±  
- \( I(X; Y) \): \( X \)ë¥¼ í†µí•´ ì¤„ì–´ë“  \( Y \)ì˜ ë¶ˆí™•ì‹¤ì„±

â†’ ì¦‰, **ì •ë³´ ê³µìœ  ì •ë„ = ë¶ˆí™•ì‹¤ì„± ê°ì†Œ ì •ë„**

---

## ğŸŸ  ë² ì§€ ë‹¤ì´ì–´ê·¸ë¨ í•´ì„

- ì› \( H(X) \), \( H(Y) \) ê°ê°ì€ ë‘ ë³€ìˆ˜ì˜ ì •ë³´ëŸ‰
- ê²¹ì¹œ ë¶€ë¶„ \( I(X; Y) \)ëŠ” ìƒí˜¸ ì •ë³´ëŸ‰
- ë‚˜ë¨¸ì§€ëŠ” ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼:  
  - \( H(X \mid Y) \), \( H(Y \mid X) \)

``` 
      H(X)
    ________
   |        | 
   | H(X|Y) |         H(Y)
   |_______ |_______ ______
            |       |
            | H(Y|X)|
            |_______|
            ^     
            I(X;Y)
```

## âœ… ì„±ì§ˆ

- \( I(X; Y) \geq 0 \)
- \( I(X; Y) = 0 \) âŸº \( X \perp Y \) (ë…ë¦½)
- ëŒ€ì¹­ì„±: \( I(X; Y) = I(Y; X) \)

## ğŸ“Œ ê²°ë¡ 

Mutual Informationì€ ì—”íŠ¸ë¡œí”¼, ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼, KL Divergenceì˜ ê´€ì  ëª¨ë‘ë¡œ ì •ì˜ ê°€ëŠ¥í•˜ë©°,  
**ë‘ ë³€ìˆ˜ ê°„ì˜ ì˜ì¡´ì„±, ì •ë³´ êµí™˜ëŸ‰, ë…ë¦½ì„± ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í•µì‹¬ ì§€í‘œ**ë¡œ ì“°ì¸ë‹¤.






---

# Vector Calculus and Optimization
- ë¯¸ë¶„ê°’ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµ
- ë”¥ëŸ¬ë‹ ë° ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì£¼ë¡œ ì‚¬ìš©

``` 
ì´ êµ¬ì¡°ëŠ” ë‹¨ìˆœí•œ ë‚˜ì—´ì´ ì•„ë‹ˆë¼:
	1.	ìŠ¤ì¹¼ë¼ ìµœì í™” â†’ ë²¡í„° ìµœì í™” â†’ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™”
	2.	ë¯¸ë¶„ ê°€ëŠ¥ì„± â†’ ë²¡í„° ë¯¸ë¶„ â†’ ìˆ˜ì¹˜ í•´ì„ í•„ìš”ì„±
	3.	ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê¹Œì§€ ì—°ê²°
```

## ğŸ“Œ ì˜ˆì œ 1: ë‹¨ìˆœ í•¨ìˆ˜ ìµœì í™”

### ë¬¸ì œ ì •ì˜:
í•¨ìˆ˜ \( f(x) = (x - 1)^2 + 3 \)ì˜ ìµœì†Œê°’ì„ ì°¾ê³ ì í•¨.

$$
\arg\min_x f(x) = \arg\min_x \left[(x - 1)^2 + 3\right]
$$

### í•´ì„:

ë¯¸ë¶„í•˜ì—¬ ìµœì†Ÿê°’ ì°¾ê¸°:

$$
\frac{d f(x)}{dx} = 2(x - 1) = 0 \quad \Rightarrow \quad x = 1
$$


## ğŸ“Œ ì˜ˆì œ 2: ì„ í˜• ë°©ì •ì‹ (Least Squares)

### Goal:
ë‹¤ìŒì„ ìµœì†Œí™”í•˜ëŠ” \( x \)ë¥¼ êµ¬í•˜ë¼:

$$
\arg\min_x \|Ax - b\|_2^2 \quad \text{(where } A \in \mathbb{R}^{m \times n},\ m > n \text{)}
$$

### í•´ì„:

ë²¡í„° ë¯¸ë¶„ì„ ì‚¬ìš©í•´ gradient 0ì´ ë˜ëŠ” \( x \)ë¥¼ ì°¾ëŠ”ë‹¤:

$$
\frac{d}{dx} \|Ax - b\|_2^2 = 0
$$

### í•´:

$$
x = (A^\top A)^{-1} A^\top b \quad \text{(Least Squares Solution)}
$$

â†’ ë²¡í„° ë¯¸ì ë¶„ í•„ìš”!


## ğŸ“Œ ì˜ˆì œ 3: Neural Network íŒŒë¼ë¯¸í„° ìµœì í™”

### Goal:
í•™ìŠµ ë°ì´í„° \( \{(x_n, y_n)\}_{n=1}^N \) ì— ëŒ€í•´, ì‹ ê²½ë§ \( f(x; \theta) \)ì˜ íŒŒë¼ë¯¸í„° \( \theta \)ë¥¼ íŠœë‹í•˜ë¼.

### ëª©ì í•¨ìˆ˜ (Square Loss):

$$
\arg\min_\theta \frac{1}{2N} \sum_{n=1}^{N} \left( y_n - f(x_n; \theta) \right)^2
$$


### í˜„ì‹¤ì  ê³ ë ¤:

- \( f(x; \theta) \)ëŠ” ë¹„ì„ í˜•ì´ë¯€ë¡œ **í•´ì„ì  í•´(closed-form)** ì¡´ì¬í•˜ì§€ ì•ŠìŒ
- ë”°ë¼ì„œ **ìˆ˜ì¹˜ì  ìµœì í™”(numerical optimization)** í•„ìš”


## ğŸ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜

- ë°˜ë³µ ìµœì í™” ë°©ì‹:

$$
\theta^{(0)} \to \theta^{(1)} \to \theta^{(2)} \to \cdots \to \theta^{(k)}
$$

- ì‚¬ìš©ë˜ëŠ” ë°©ë²•:
  - Gradient Descent (GD)
  - Stochastic Gradient Descent (SGD)
  - Newton's Method ë“±


# ğŸ”§ Derivative & Partial Derivative

## ğŸ“˜ 1. ë¯¸ë¶„ì˜ ì •ì˜ (Univariate Derivative)

ë‹¨ì¼ ë³€ìˆ˜ í•¨ìˆ˜ \( f(x) \)ì˜ ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤:

$$
\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}
$$


## âœï¸ ì˜ˆì œ: \( f(x) = x^2 \)

$$
\frac{df(x)}{dx} = \lim_{h \to 0} \frac{(x + h)^2 - x^2}{h}
= \lim_{h \to 0} \frac{2xh + h^2}{h}
= \lim_{h \to 0} (2x + h) = 2x
$$


## ğŸ”¢ ë¯¸ë¶„ ê¸°ë³¸ ê·œì¹™

- **ê³±ì˜ ë²•ì¹™ (Product Rule):**

$$
(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)
$$

- **ëª«ì˜ ë²•ì¹™ (Quotient Rule):**

$$
\left(\frac{f(x)}{g(x)}\right)' = \frac{f'(x)g(x) - f(x)g'(x)}{(g(x))^2}
$$

- **í•©ì˜ ë²•ì¹™ (Sum Rule):**

$$
(f(x) + g(x))' = f'(x) + g'(x)
$$

- **í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ (Chain Rule):**

$$
(g(f(x)))' = g'(f(x)) \cdot f'(x)
$$


## ğŸ§  ì˜ˆì œ: Chain Rule

í•¨ìˆ˜ \( h(x) = (2x + 1)^4 \) ì„ ë¯¸ë¶„í•˜ë¼.

- \( g(f) = f^4 \), \( f(x) = 2x + 1 \)

$$
h'(x) = g'(f(x)) \cdot f'(x) = 4(2x + 1)^3 \cdot 2 = 8(2x + 1)^3
$$


## ğŸ” ë¡œê·¸, ì§€ìˆ˜, ì—­ìˆ˜ ë¯¸ë¶„ ì˜ˆì‹œ

- \( f(x) = x \log x \)

$$
\frac{d}{dx}[x \log x] = \log x + 1
$$

- \( f(x) = e^{wx + b} \)

$$
\frac{d}{dx}[e^{wx + b}] = w e^{wx + b}
$$

- \( f(x) = \frac{1}{x} \)

$$
\frac{d}{dx}\left[\frac{1}{x}\right] = -\frac{1}{x^2}
$$


## ğŸŒ í¸ë¯¸ë¶„ (Partial Derivative)

ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ \( f(x_1, x_2, ..., x_d) \)ì˜ ë³€ìˆ˜ \( x_i \)ì— ëŒ€í•œ í¸ë¯¸ë¶„ ì •ì˜:

$$
\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, ..., x_i + h, ..., x_d) - f(x_1, ..., x_i, ..., x_d)}{h}
$$


## ğŸ“˜ ì˜ˆì œ: í¸ë¯¸ë¶„

í•¨ìˆ˜ \( f(x_1, x_2) = x_1^2 x_2 + x_1 x_2^2 \)

- \( \frac{\partial f}{\partial x_1} = 2x_1 x_2 + x_2^2 \)
- \( \frac{\partial f}{\partial x_2} = x_1^2 + 2x_1 x_2 \)


## ğŸ§© í¸ë¯¸ë¶„ ê¸°ë³¸ ê·œì¹™

- **ê³±ì˜ ë²•ì¹™:**

$$
\frac{\partial}{\partial x}[f(x)g(x)] = \frac{\partial f}{\partial x}g(x) + f(x)\frac{\partial g}{\partial x}
$$

- **í•©ì˜ ë²•ì¹™:**

$$
\frac{\partial}{\partial x}[f(x) + g(x)] = \frac{\partial f}{\partial x} + \frac{\partial g}{\partial x}
$$

- **Chain Rule (Multivariable):**  
  \( f : \mathbb{R}^m \to \mathbb{R} \), \( g : \mathbb{R}^n \to \mathbb{R}^m \)

$$
\frac{\partial}{\partial x} (f \circ g)(x) = \left[ \frac{\partial f}{\partial g} \right] \cdot \left[ \frac{\partial g}{\partial x} \right]
$$


## ğŸ“˜ ì˜ˆì œ: ë‹¤ë³€ìˆ˜ ì²´ì¸ë£°

í•¨ìˆ˜ \( f: \mathbb{R}^2 \to \mathbb{R}^2 \), \( g: \mathbb{R}^2 \to \mathbb{R} \)

- ì„±ë¶„ë³„ ê³„ì‚°:

$$
\frac{\partial (g \circ f)}{\partial x_1} = \frac{\partial g}{\partial f_1} \cdot \frac{\partial f_1}{\partial x_1} + \frac{\partial g}{\partial f_2} \cdot \frac{\partial f_2}{\partial x_1}
$$

$$
\frac{\partial (g \circ f)}{\partial x_2} = \frac{\partial g}{\partial f_1} \cdot \frac{\partial f_1}{\partial x_2} + \frac{\partial g}{\partial f_2} \cdot \frac{\partial f_2}{\partial x_2}
$$

- í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´:

$$
\left(\frac{d}{dx} (g \circ f)(x)\right)^\top = \left(\frac{\partial g}{\partial f}\right)^\top J_f
$$

â†’ ì—¬ê¸°ì„œ \( J_f \)ëŠ” \( f \)ì˜ Jacobian í–‰ë ¬



