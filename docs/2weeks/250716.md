# 11ì¼ì°¨ - 250716

# ğŸ” Linear Regression: Explainability

## ğŸ“Œ Feature Importance (t-statistic)

* ê³„ìˆ˜ì˜ ì¤‘ìš”ë„ëŠ” í•´ë‹¹ ê³„ìˆ˜ì˜ **í‘œì¤€ ì˜¤ì°¨ë¡œ ì •ê·œí™”ëœ ê°’**, ì¦‰ **t-í†µê³„ëŸ‰**ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤:

$$
t_j = \frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}
$$

* íŠ¹ì„±ì˜ ì¤‘ìš”ë„ëŠ” \$|t\_j|\$ ì˜ ì ˆëŒ“ê°’ì´ í´ìˆ˜ë¡ ì¤‘ìš”ë„ê°€ ë†’ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
\text{Importance of feature } j = |t_j| = \left| \frac{\hat{\theta}_j}{SE(\hat{\theta}_j)} \right|
$$

## ğŸš² Example: Bike Rentals Dataset for Regression

* ëª©í‘œ: **ë‚ ì”¨ì™€ ìš”ì¼ì— ë”°ë¼ ìì „ê±° ëŒ€ì—¬ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡**
* ì‚¬ìš©ëœ íŠ¹ì„±ë“¤:

  * ìì „ê±° ìˆ˜(cnt): target ê°’
  * ê³„ì ˆ(season): ë´„, ì—¬ë¦„, ê°€ì„, ê²¨ìš¸
  * ê³µíœ´ì¼ ì—¬ë¶€
  * ì£¼ë§ ì—¬ë¶€
  * ë‚ ì§œë¡œë¶€í„°ì˜ ê²½ê³¼ ì¼ìˆ˜(days\_since\_2011)
  * ë‚ ì”¨ ì •ë³´ (good, mist, rain/snow/storm ë“±)
  * ê¸°ì˜¨(Â°C), ìŠµë„(%), í’ì†(km/h)

Molnar's IML book ê¸°ë°˜ ì˜ˆì œ


## ğŸ“Š ì˜ˆì¸¡ ì˜ˆì‹œ í•´ì„

- íŠ¹ì • ì¼ìì˜ ì„ í˜• íšŒê·€ ê³„ìˆ˜ ì˜ˆì‹œ:

| Feature | Weight | SE | t |
|---------|--------|------|------|
| (Intercept) | 2399.4 | 238.3 | 10.1 |
| seasonSUMMER | 899.3 | 122.3 | 7.4 |
| seasonFALL | 138.2 | 161.7 | 0.9 |
| seasonWINTER | 425.6 | 110.8 | 3.8 |
| holidayHOLIDAY | -686.1 | 203.3 | 3.4 |
| workingdayWORKING DAY | 124.9 | 73.3 | 1.7 |
| weathersitMISTY | -379.4 | 87.6 | 4.3 |
| weathersitRAIN/SNOW/STORM | -1901.5 | 223.6 | 8.5 |
| temp | 110.7 | 7.0 | 15.7 |
| hum | -17.4 | 3.2 | 5.5 |
| windspeed | -42.5 | 6.9 | 6.2 |
| days_since_2011 | 4.9 | 0.2 | 28.5 |

- **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í•´ì„**: ê¸°ì˜¨ì´ 1ë„ ì˜¤ë¥´ë©´ ëŒ€ì—¬ ìˆ˜ìš”ê°€ 110.7 ì¦ê°€ (ë‹¤ë¥¸ ë³€ìˆ˜ ê³ ì •)
- **ë²”ì£¼í˜• ë³€ìˆ˜ í•´ì„**: ë‚˜ìœ ë‚ ì”¨ì¼ ê²½ìš° ëŒ€ì—¬ ìˆ˜ìš”ê°€ í‰ê· ì ìœ¼ë¡œ 1901.5ëª… ê°ì†Œ

## ğŸ“ˆ Effect Plot

$$
\text{Effect}_{j,n} = \theta_j x_{j,n}
$$

* ê° íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ë¥¼ ì‹œê°í™”
* ì˜ˆ: ê¸°ì˜¨ê³¼ ë‚ ì§œ ê²½ê³¼ ì¼ìˆ˜(days\_since\_2011)ê°€ í° ì˜í–¥ ë¯¸ì¹¨

## ğŸ‘¤ Explain Individual Predictions

íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤:

| Feature           | Value       |
| ----------------- | ----------- |
| season            | SPRING      |
| yr                | 2011        |
| mnth              | JAN         |
| holiday           | NO HOLIDAY  |
| weekday           | THU         |
| workingday        | WORKING DAY |
| weather           | GOOD        |
| temp              | 1.6043656   |
| hum               | 51.8261     |
| windspeed         | 6.000688    |
| cnt               | 1606        |
| days\_since\_2011 | 5           |

ì˜ˆì¸¡ê°’: 4504 / ì‹¤ì œê°’: 1606 â†’ **ê³ í¸í–¥(high bias)** ì˜ˆì‹œ

## ğŸ“‰ Individual Effect Plot

* ê° featureì˜ ì˜ˆì¸¡ ê¸°ì—¬ê°’ì„ ì‹œê°í™”í•œ ê·¸ë˜í”„
* ì‹¤ì œ ê°’, í‰ê·  ì˜ˆì¸¡, ì˜ˆì¸¡ ê¸°ì—¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ



## ğŸ§ª LASSO: Feature Selection

### LASSO with 2 Features

| Feature           | Weight |
| ----------------- | ------ |
| temp              | 52.33  |
| days\_since\_2011 | 2.15   |
| others            | 0.00   |

### LASSO with 5 Features

| Feature                 | Weight  |
| ----------------------- | ------- |
| seasonSPRING            | -389.99 |
| weather=RAIN/SNOW/STORM | -862.27 |
| temp                    | 85.58   |
| hum                     | -3.04   |
| days\_since\_2011       | 3.82    |

* í•„ìš”ì—†ëŠ” ë³€ìˆ˜ëŠ” 0ìœ¼ë¡œ ë§Œë“¤ê³ , í•µì‹¬ featureë§Œ ë‚¨ê¹€

ì¶œì²˜: Molnar's IML book


---


# Logistic Regression

- ê°œìš”
  - ì¤‘ìš”í•œ ì•Œê³ ë¦¬ì¦˜
  - Binary Classification - ì´ì§„ë¶„ë¥˜ë¥¼ ìœ„í•œê²ƒ

- ì¢…ë¥˜
  - Binary classification & logistic function
  - Logistic regression
    - Maximun likelihood formulation
    - Newton's method
    - Iterative re-weighted least squares (IRLS) algorithm
    - Generalized linear model
  - Softmax regression
  - Cross-entropy error
  - Multi-label learning



---

# Binary Classification

* ì´ì§„ ë¶„ë¥˜ ë¬¸ì œëŠ” ë‘ ê°œì˜ í´ë˜ìŠ¤ \$\mathcal{C} = {C\_1, C\_2}\$ ì¤‘ í•˜ë‚˜ì— ì†í•˜ë„ë¡ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.
* ì£¼ì–´ì§„ ì…ë ¥ \$\mathbf{x}\$ì— ëŒ€í•´ ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬í›„í™•ë¥ (posteriors)ì„ ê³„ì‚°í•©ë‹ˆë‹¤:

$$
P(C_1|\mathbf{x}) = \frac{P(\mathbf{x}|C_1)P(C_1)}{P(\mathbf{x})}
= \frac{P(\mathbf{x}|C_1)P(C_1)}{P(\mathbf{x}|C_1)P(C_1) + P(\mathbf{x}|C_2)P(C_2)}
= \frac{1}{1 + \frac{P(\mathbf{x}|C_2)P(C_2)}{P(\mathbf{x}|C_1)P(C_1)}}
= \frac{1}{1 + \exp \left( - \log \frac{P(\mathbf{x}|C_1)}{P(\mathbf{x}|C_2)} - \log \frac{P(C_1)}{P(C_2)} \right)}
$$


![img.png](250716_1.png)

![img.png](250716_2.png)

### ì˜ˆì œ
- Medical diagnosis: Disease or no disease
- Cancer diagnosis: Benign or mallignant
- Image classification: Cat or dog
- Image classification: Face or non-face
- Image classification: Foreground or background
- Sentiment analysis: Positive or negative
- Name entity recognition: Name or not
- Speech or audio: Voice or not


![img.png](250716_3.png)

---

#  Logistic Function

## ğŸ”„ Logistic Function í‘œí˜„

* ìœ„ ê²°ê³¼ëŠ” ì‹œê·¸ëª¨ì´ë“œ í˜•íƒœì˜ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤:

$$
P(C_1|\mathbf{x}) = \frac{1}{1 + e^{-\xi}}
$$

* ì—¬ê¸°ì„œ:

$$
\xi = \log \left( \frac{P(\mathbf{x}|C_1)}{P(\mathbf{x}|C_2)} \right) + \log \left( \frac{P(C_1)}{P(C_2)} \right)
$$

* ì²« ë²ˆì§¸ í•­ì€ likelihood ratio, ë‘ ë²ˆì§¸ í•­ì€ prior ratio


## ğŸ“ Multivariate Gaussian Model ê°€ì •

* í´ë˜ìŠ¤ ì¡°ê±´ë¶€ í™•ë¥  \$P(\mathbf{x}|C\_k)\$ë¥¼ ê³µí†µ ê³µë¶„ì‚° í–‰ë ¬ \$\Sigma\$ë¥¼ ê°€ì§„ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ë¡œ ê°€ì •:

$$
P(\mathbf{x}|C_i) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu_i)^T \Sigma^{-1} (\mathbf{x} - \mu_i) \right)
$$

* ì´ë¥¼ ëŒ€ì…í•˜ì—¬ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€ í˜•íƒœ ìœ ë„:

$$
P(C_1|\mathbf{x}) = \frac{1}{1 + \exp \left( - (\mathbf{w}^T \mathbf{x} + b) \right)}
$$

* ì—¬ê¸°ì„œ:

$$
\mathbf{w} = \Sigma^{-1} (\mu_1 - \mu_2), \quad
b = \frac{1}{2}(\mu_2^T \Sigma^{-1} \mu_2 - \mu_1^T \Sigma^{-1} \mu_1) + \log \left( \frac{P(C_1)}{P(C_2)} \right)
$$


## ğŸ“ˆ Logistic Functionì˜ ì„±ì§ˆ

* ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ \$\sigma(\xi) = \frac{1}{1 + e^{-\xi}}\$ ì˜ ì„±ì§ˆ:

  * \$\sigma(\xi) \to 0\$ as \$\xi \to -\infty\$
  * \$\sigma(\xi) \to 1\$ as \$\xi \to +\infty\$
  * \$\sigma(-\xi) = 1 - \sigma(\xi)\$
  * \$\frac{d}{d\xi} \sigma(\xi) = \sigma(\xi)(1 - \sigma(\xi))\$

---

# ğŸ” Logistic Regression

## ğŸ“˜ ê°œìš”

* **Logistic Regression**ì€ ëŒ€í‘œì ì¸ ì´ì§„ ë¶„ë¥˜(Binary Classification) ì•Œê³ ë¦¬ì¦˜
* ì¶œë ¥ê°’ \$y \in {0, 1}\$ ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥  \$P(y|\mathbf{x})\$ ë¥¼ ì§ì ‘ ëª¨ë¸ë§í•˜ëŠ” **Discriminative Model**
* í™•ë¥  ë¶„í¬ë¡œëŠ” **Bernoulli ë¶„í¬**, ì¶œë ¥ í•¨ìˆ˜ë¡œëŠ” **ì‹œê·¸ëª¨ì´ë“œ(ë¡œì§€ìŠ¤í‹±) í•¨ìˆ˜** ì‚¬ìš©

$$
\mathbb{E}[y | \mathbf{x}] = P(y = 1 | \mathbf{x}) = \sigma(\mathbf{w}^T \mathbf{x}) = \frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}}}
$$


## ğŸ§® Maximum Likelihood Estimation (MLE)

* í•™ìŠµ ë°ì´í„°: \${(\mathbf{x}*n, y\_n)}*{n=1}^N\$
* Bernoulli ë¶„í¬ ê¸°ë°˜ìœ¼ë¡œ likelihoodë¥¼ êµ¬ì„±í•˜ë©´:

$$
p(\mathbf{y}|\mathbf{X}, \mathbf{w}) = \prod_{n=1}^{N} \sigma(\mathbf{w}^T \mathbf{x}_n)^{y_n} (1 - \sigma(\mathbf{w}^T \mathbf{x}_n))^{1 - y_n}
$$

* ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜:

$$
\mathcal{L}(\mathbf{w}) = \sum_{n=1}^N \left[ y_n \log \hat{y}_n + (1 - y_n) \log(1 - \hat{y}_n) \right], \quad \hat{y}_n = \sigma(\mathbf{w}^T \mathbf{x}_n)
$$

> ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•¨


![img.png](250716_4.png)



## âš™ï¸ ìµœì í™”: Newton's Method & IRLS

- ë¯¸ë¶„ 1ë²ˆ: ê²½ì‚¬  Slope - Gradient Descent(ê²½ì‚¬ í•˜ê°•ë²•)
- ë¯¸ë¶„ 2ë²ˆ: Curvature 

### ğŸš€ Gradient Descent (ê²½ì‚¬ í•˜ê°•ë²•)

- **ëª©ì :** í•¨ìˆ˜ \( J(w) \)ë¥¼ ìµœì†Œí™”
- **ì—…ë°ì´íŠ¸ ê³µì‹:**
  \[
  w_{k+1} \leftarrow w_k - \alpha \nabla J(w_k)
  \]
- **ì„¤ëª…:**
  - \( \nabla J(w_k) \): 1ì°¨ ë¯¸ë¶„, ì¦‰ **ê¸°ìš¸ê¸° (slope)**
  - \( \alpha \): í•™ìŠµë¥  (learning rate)
  - ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ í•¨ìˆ˜ê°’ì´ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™
  - ë‹¨ìˆœí•˜ì§€ë§Œ ìˆ˜ë ´ ì†ë„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ


### ğŸ”¹ Newtonâ€™s Method (ë‰´í„´ ë°©ë²•)

- **ëª©ì :** ë” ë¹ ë¥´ê²Œ ìµœì ê°’ì— ìˆ˜ë ´
- **ì—…ë°ì´íŠ¸ ê³µì‹:**
$$
  \[
  w_{k+1} \leftarrow w_k - \alpha \left[ \nabla^2 J(w_k) \right]^{-1} \nabla J(w_k)
  \]
$$
- **ì„¤ëª…:**
  - $ ( \nabla J(w_k) ) $: 1ì°¨ ë¯¸ë¶„ â†’ ê¸°ìš¸ê¸°
  - $ ( \nabla^2 J(w_k) ) $: 2ì°¨ ë¯¸ë¶„ â†’ **ê³¡ë¥  (curvature)**, ì¦‰ í—¤ì‹œì•ˆ í–‰ë ¬
  - ê¸°ìš¸ê¸° + ê³¡ë¥  ëª¨ë‘ ê³ ë ¤í•˜ì—¬ ë” ì •í™•í•œ ë°©í–¥ê³¼ í¬ê¸°ë¡œ ì´ë™
  - ê³„ì‚° ë¹„ìš©ì€ ë†’ì§€ë§Œ ë¹ ë¥¸ ìˆ˜ë ´ ê°€ëŠ¥


* 2ì°¨ í…Œì¼ëŸ¬ ì „ê°œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì´ì°¨ ìµœì í™” ê¸°ë²•

$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - [\nabla^2 J(\mathbf{w}^{(t)})]^{-1} \nabla J(\mathbf{w}^{(t)})
$$

* $J(\mathbf{w})$ëŠ” ìŒì˜ ë¡œê·¸ ê°€ëŠ¥ë„(negative log-likelihood)

![img.png](250716_5.png)

### âœ… ì°¨ì´ì  ë¹„êµ

| í•­ëª©              | Gradient Descent                | Newton's Method                      |
|-------------------|----------------------------------|--------------------------------------|
| ì‚¬ìš© ë¯¸ë¶„         | 1ì°¨ ë¯¸ë¶„ (Gradient)             | 1ì°¨ + 2ì°¨ ë¯¸ë¶„ (Gradient + Hessian) |
| ìˆ˜ë ´ ì†ë„         | ëŠë¦¼                             | ë¹ ë¦„                                 |
| ê³„ì‚° ë¹„ìš©         | ë‚®ìŒ                             | ë†’ìŒ (í—¤ì‹œì•ˆ ì—­í–‰ë ¬ ê³„ì‚° í•„ìš”)       |
| ì ìš© ê°€ëŠ¥ì„±       | ê³ ì°¨ì› ë¬¸ì œì— ìœ ë¦¬              | ê³ ì°¨ì› ë¬¸ì œì—ì„œëŠ” ë¶€ë‹´ë  ìˆ˜ ìˆìŒ     |


### ğŸ”¹ Iterative Re-weighted Least Squares (IRLS)

* Newton's Methodë¥¼ ì„ í˜•íšŒê·€ í˜•íƒœë¡œ ê³„ì‚°
* ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ì‹œìŠ¤í…œìœ¼ë¡œ ì •ë¦¬ë¨:

$$
\mathbf{w}_{t+1} = (\mathbf{X}^T \mathbf{S}_t \mathbf{X})^{-1} \mathbf{X}^T \mathbf{S}_t \mathbf{z}_t
$$

* ì—¬ê¸°ì„œ:

  * $\mathbf{S}\_t$ëŠ” ëŒ€ê° í–‰ë ¬ë¡œ ì˜ˆì¸¡ê°’ ê¸°ë°˜ ê°€ì¤‘ì¹˜
  * $\mathbf{z}\_t$ëŠ” working response vector


## ğŸ§© Generalized Linear Model (GLM) ê´€ì 

* **GLM**ì€ ì„ í˜• ëª¨ë¸ì„ í™•ë¥  ë¶„í¬ ê´€ì ìœ¼ë¡œ í™•ì¥í•œ ê°œë…

* 3ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ë¨:

  1. ì¶œë ¥ ë³€ìˆ˜ì˜ ë¶„í¬ (ì§€ìˆ˜ ê°€ì¡±: ì˜ˆ. Bernoulli, Gaussian ë“±)
  2. ì„ í˜• ì˜ˆì¸¡ê¸° \$\eta = \mathbf{w}^T \mathbf{x}\$
  3. ë§í¬ í•¨ìˆ˜ \$g(\mathbb{E}\[y]) = \eta\$

* Logistic Regressionì€ ë‹¤ìŒ êµ¬ì„±ì— í•´ë‹¹:

  * ë¶„í¬: Bernoulli
  * ë§í¬ í•¨ìˆ˜: logit (\$\log \frac{\mu}{1 - \mu}\$)
  * ì—­ ë§í¬ í•¨ìˆ˜: ì‹œê·¸ëª¨ì´ë“œ


## ğŸ”„ ë‹¤ì¤‘ í´ë˜ìŠ¤ í™•ì¥: Softmax Regression

* í´ë˜ìŠ¤ê°€ \$K > 2\$ ê°œì¸ ê²½ìš° ì‚¬ìš©
* ê° í´ë˜ìŠ¤ \$k\$ì— ëŒ€í•´:

$$
p(y = k | \mathbf{x}) = \frac{\exp(\mathbf{w}_k^T \mathbf{x})}{\sum_{j=1}^K \exp(\mathbf{w}_j^T \mathbf{x})}
$$

* Cross-entropy lossë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ:

$$
\mathcal{L} = -\sum_{n=1}^N \sum_{k=1}^K y_{nk} \log p(y = k | \mathbf{x}_n)
$$

* Multiclass one-hot ë ˆì´ë¸” \$\mathbf{Y} \in \mathbb{R}^{N \times K}\$ë¥¼ ì‚¬ìš©
* Softmax íšŒê·€ë„ Newtonâ€™s methodë‚˜ IRLSë¡œ ìµœì í™” ê°€ëŠ¥

---

## ğŸ·ï¸ ê¸°íƒ€ í™•ì¥

* **Cross-Entropy Error**: Bernoulli ë˜ëŠ” softmax ì¶œë ¥ì— ê³µí†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜
* **Multi-label Learning**: ë…ë¦½ì ì¸ ì—¬ëŸ¬ binary classifierë¥¼ ë³‘ë ¬ë¡œ í•™ìŠµí•˜ëŠ” í˜•íƒœ


---

# Multi-Label Learning (ë©€í‹°ë ˆì´ë¸” ë¶„ë¥˜)
- ì…ë ¥ í•˜ë‚˜ì— ì—¬ëŸ¬ ë ˆì´ë¸”ì´ ë™ì‹œì— ë¶™ëŠ” ë¬¸ì œ (ì˜ˆ: dog, sand, sky)
- ì¶œë ¥ ë²¡í„° ì˜ˆ:
$$
  \[
  \mathbf{y} = [1, 0, 1, 1, 0]
  \]
$$
- ê° í´ë˜ìŠ¤ë§ˆë‹¤ ë…ë¦½ì ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ ìˆ˜í–‰
- Loss (Binary Cross Entropy for each label):
$$
  \[
  \mathcal{J} = \frac{1}{N} \sum_{n=1}^{N} \sum_{k=1}^{K} \left[ - y_{k,n} \log \hat{y}_{k,n} - (1 - y_{k,n}) \log (1 - \hat{y}_{k,n}) \right]
  \]
  where \( \hat{y}_{k,n} = \sigma(\mathbf{w}_k^\top \mathbf{x}_n) \)
$$
- ì¼ë¶€ í•­ëª©ì´ '?'ë¡œ ë¯¸ì •ì¸ ê²½ìš° ì œì™¸í•˜ê³  ê³„ì‚°




