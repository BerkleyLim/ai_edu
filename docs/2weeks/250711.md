# 7ì¼ì°¨ - 250711

# ğŸ§  Gradient, Jacobian & Hessian
## ğŸŒŸ ê°œìš”

- **Gradient (âˆ‡f)**: ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ \( f : \mathbb{R}^d \to \mathbb{R} \)ì˜ 1ì°¨ ë¯¸ë¶„ ë²¡í„°
- **Jacobian (J)**: ë²¡í„° í•¨ìˆ˜ \( f : \mathbb{R}^n \to \mathbb{R}^m \)ì˜ 1ì°¨ ë¯¸ë¶„ í–‰ë ¬
- **Hessian (H)**: ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì˜ 2ì°¨ ë¯¸ë¶„ (2ì°¨ í¸ë¯¸ë¶„) í–‰ë ¬

![img.png](250711_1.png)
![img.png](250711_2.png)

## ğŸ“ Gradient

### ì •ì˜:
$$
\nabla f(x) = \left[ \frac{\partial f}{\partial x_1}, \cdots, \frac{\partial f}{\partial x_d} \right]^T \in \mathbb{R}^d
$$

â†’ GradientëŠ” **í•¨ìˆ˜ê°€ ê°€ì¥ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥**  
â†’ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì—ì„œ ì¤‘ìš”í•œ ì—­í• 


### ì˜ˆì‹œ: \( f(\theta) = \theta^\top x \)

- \( f(\theta) = \sum \theta_i x_i \)
- ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ í¸ë¯¸ë¶„: \( \frac{\partial f}{\partial \theta_i} = x_i \)
- ê²°ê³¼:

$$
\nabla_\theta f(\theta) = x
$$


### ì˜ˆì‹œ: \( f(\theta) = \theta^\top A \theta \) (A symmetric)

- \( \frac{\partial f}{\partial \theta_k} = 2 \sum_j A_{kj} \theta_j \)
- ê²°ê³¼:

$$
\nabla_\theta f(\theta) = 2A\theta
$$


## ğŸŸ¦ Jacobian Matrix

### ì •ì˜:
ë²¡í„° í•¨ìˆ˜ \( f : \mathbb{R}^n \to \mathbb{R}^m \)ì˜ Jacobianì€:

$$
J = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

â†’ ê° í–‰ì€ ê° ì¶œë ¥ \( f_i \)ì˜ gradient


### ì˜ˆì‹œ: ì„ í˜• ë³€í™˜

í•¨ìˆ˜:
- \( y_1 = -2x_1 + x_2 \)
- \( y_2 = x_1 + x_2 \)

Jacobian:

$$
J = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2}
\end{bmatrix}
=
\begin{bmatrix}
-2 & 1 \\
1 & 1
\end{bmatrix}
$$


## ğŸ”· Hessian Matrix

### ì •ì˜:

ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ \( f : \mathbb{R}^d \to \mathbb{R} \)ì— ëŒ€í•´, Hessianì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
H = \nabla^2 f(x) = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right]_{i,j=1}^{d} \in \mathbb{R}^{d \times d}
$$

â†’ \( H = \frac{\partial}{\partial x} \left( \nabla f(x)^\top \right) \)

- ëŒ€ì¹­í–‰ë ¬ (symmetric)
- convexity íŒë³„ì— ì‚¬ìš©ë¨: \( H \succeq 0 \Rightarrow \) convex


## ğŸ” ì •ë¦¬ ìš”ì•½

| ê°œë… | ì •ì˜ì—­ | ì¹˜ì—­ | í¬ê¸° | ì˜ë¯¸ |
|------|--------|------|------|------|
| Gradient | \( \mathbb{R}^d \to \mathbb{R} \) | \( \mathbb{R}^d \) | \( d \times 1 \) | ë°©í–¥ê³¼ ê¸°ìš¸ê¸° |
| Jacobian | \( \mathbb{R}^n \to \mathbb{R}^m \) | \( \mathbb{R}^{m \times n} \) | \( m \times n \) | ë°©í–¥ë³„ í¸ë¯¸ë¶„ í–‰ë ¬ |
| Hessian | \( \mathbb{R}^d \to \mathbb{R} \) | \( \mathbb{R}^{d \times d} \) | \( d \times d \) | 2ì°¨ ê³¡ë¥  ì •ë³´ |


## ğŸ“Œ ì¶”ê°€ Tip

- GradientëŠ” Jacobianì˜ íŠ¹ìˆ˜í•œ ê²½ìš°  
- Hessianì€ Gradientì˜ Gradient  
- ìµœì í™” ì´ë¡ ì—ì„  GradientëŠ” ë°©í–¥, Hessianì€ ê³¡ë¥ 


# ğŸ“Œ ë³´ì¶©: Gradient, Jacobian, Hessianì˜ ì‹¤ì œ ì˜ë¯¸ ë° í™œìš©


## ğŸ“ Gradientì™€ ë°©í–¥ ë¯¸ë¶„ (Directional Derivative)

GradientëŠ” í•¨ìˆ˜ê°€ **ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ”ê°€**ë¥¼ ì•Œë ¤ì¤€ë‹¤.

ì–´ë–¤ ë‹¨ìœ„ë²¡í„° ë°©í–¥ \( \mathbf{v} \) ì— ëŒ€í•œ ë°©í–¥ ë¯¸ë¶„ì€:

$$
D_{\mathbf{v}} f(x) = \nabla f(x)^\top \mathbf{v}
$$

- Gradient ë°©í–¥: ê°€ì¥ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥
- -Gradient ë°©í–¥: ê°€ì¥ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥


## ğŸš€ Gradient Descent ì•Œê³ ë¦¬ì¦˜

ìµœì í™”ì—ì„œ ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” gradient í™œìš©:

$$
x^{(t+1)} = x^{(t)} - \eta \nabla f(x^{(t)})
$$

- \( \eta \): learning rate
- ëª©ì : í•¨ìˆ˜ \( f(x) \)ë¥¼ ìµœì†Œí™”
- GradientëŠ” **ì˜¤ë¥´ë§‰ ë°©í–¥**ì´ë¯€ë¡œ, ë°˜ëŒ€ë¡œ ì´ë™í•˜ë©´ ë¹ ë¥´ê²Œ ë‚´ë ¤ê°„ë‹¤


## ğŸ§­ Jacobianì˜ ê¸°í•˜ì  í•´ì„

Jacobian í–‰ë ¬ì€ ë²¡í„° í•¨ìˆ˜ \( f : \mathbb{R}^n \to \mathbb{R}^m \)ì´  
ì…ë ¥ ê³µê°„ì„ **ì–´ë–»ê²Œ ë³€í˜•ì‹œí‚¤ëŠ”ì§€**ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

íŠ¹íˆ ì¢Œí‘œê³„ ë³€í™˜ì—ì„œ ë©´ì /ë¶€í”¼ ë³€í™”ìœ¨ì€ ë‹¤ìŒìœ¼ë¡œ í‘œí˜„ë¨:

$$
\text{ë³€í™˜ í›„ ë¶€í”¼} = |\det J(x)| \cdot \text{ë³€í™˜ ì „ ë¶€í”¼}
$$

- \( \det J(x) \)ê°€ 0ì´ë©´ êµ­ì†Œì ìœ¼ë¡œ ì ‘í˜ì´ ì¼ì–´ë‚¨
- í™•ë¥  ë¶„í¬ ë³€í™˜, ì •ê·œí™” ë“±ì—ì„œ í•„ìˆ˜


## ğŸ“Š Hessianê³¼ ìµœì í™” ì¡°ê±´ (2ì°¨ ë„í•¨ìˆ˜ì˜ ì—­í• )

Hessian í–‰ë ¬ \( \nabla^2 f(x) \)ëŠ” í•¨ìˆ˜ì˜ **ê³¡ë¥ **ì„ ì¸¡ì •í•œë‹¤.

ìµœì í™” ë¬¸ì œì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ íŒë³„ì— ì‚¬ìš©ëœë‹¤:

| ì¡°ê±´ | ì˜ë¯¸ |
|------|------|
| \( \nabla^2 f(x) \succ 0 \) | Local minimum (strict) |
| \( \nabla^2 f(x) \succeq 0 \) | Local minimum (maybe saddle) |
| \( \nabla^2 f(x) \prec 0 \) | Local maximum |
| Hessian indefinite | Saddle point |


### ğŸ’¡ Newtonâ€™s Method

Hessianê³¼ Gradientë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” 2ì°¨ ìµœì í™” ì•Œê³ ë¦¬ì¦˜:

$$
x^{(t+1)} = x^{(t)} - [\nabla^2 f(x^{(t)})]^{-1} \nabla f(x^{(t)})
$$

- GradientëŠ” **ë°©í–¥**, Hessianì€ **ê³¡ë¥ (ì–¼ë§ˆë‚˜ ê°€íŒŒë¥¸ì§€)** ì œê³µ
- ì •í™•í•˜ì§€ë§Œ ë¹„ìš©ì´ í¼


## ğŸ§® Gradient vs Jacobian ë¹„êµ ìš”ì•½

| í•­ëª© | Gradient | Jacobian |
|------|----------|----------|
| í•¨ìˆ˜ | \( f : \mathbb{R}^d \to \mathbb{R} \) | \( f : \mathbb{R}^n \to \mathbb{R}^m \) |
| ê²°ê³¼ | \( \nabla f \in \mathbb{R}^d \) (ì—´ë²¡í„°) | \( J \in \mathbb{R}^{m \times n} \) |
| ì—­í•  | ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° | ë²¡í„° í•¨ìˆ˜ì˜ ëª¨ë“  í¸ë¯¸ë¶„ |
| í•´ì„ | ê°€ì¥ ë¹ ë¥¸ ì¦ê°€ ë°©í–¥ | ì „ì²´ ë²¡í„° í•„ë“œì˜ ë³€í™˜ |


## ğŸ” ì •ë¦¬ ìš”ì•½

- **Gradient**ëŠ” ê²½ì‚¬ ë°©í–¥
- **Jacobian**ì€ ë‹¤ë³€ìˆ˜ ë²¡í„° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ í–‰ë ¬í™”í•œ ê²ƒ
- **Hessian**ì€ ê³¡ë¥ ì„ ì¸¡ì •í•˜ê³  ìµœì í™” ì´ë¡ ì— í•„ìˆ˜
- ëª¨ë‘ê°€ **ë”¥ëŸ¬ë‹, ìµœì í™”, ë²¡í„° ë¯¸ì ë¶„**ì—ì„œ í•µì‹¬ ë„êµ¬



---


# ğŸ“ Taylor Series Expansion

## ğŸ”¹ Taylor Series (ë‹¨ë³€ìˆ˜)

ë§¤ìš° ë§¤ë„ëŸ¬ìš´ í•¨ìˆ˜ \( f \in C^\infty \), ì¦‰ ë¬´í•œ ë²ˆ ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜ \( f: \mathbb{R} \rightarrow \mathbb{R} \) ì— ëŒ€í•´,  
ì  \( x_0 \) ì£¼ë³€ì—ì„œì˜ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ ì „ê°œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
f(x) = \sum_{k=0}^{\infty} \frac{1}{k!} \left. \frac{d^k f(x)}{dx^k} \right|_{x = x_0} (x - x_0)^k
$$


### ğŸ“Œ ì˜ˆì‹œ: \( e^x \)ì˜ í…Œì¼ëŸ¬ ì „ê°œ (ì›ì  ê¸°ì¤€)

$$
e^x = \sum_{k=0}^\infty \frac{x^k}{k!} 
= 1 + x + \frac{x^2}{2} + \frac{x^3}{6} + \frac{x^4}{24} + \frac{x^5}{120} + \cdots
$$


## ğŸ”¹ 1ì°¨ í…Œì¼ëŸ¬ ê·¼ì‚¬ (ì„ í˜• ê·¼ì‚¬ / Linearization)

í•¨ìˆ˜ \( f : \mathbb{R}^d \to \mathbb{R} \)ì— ëŒ€í•´,  
ì  \( x_0 \) ê·¼ì²˜ì—ì„œì˜ 1ì°¨ í…Œì¼ëŸ¬ ì „ê°œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
f(x) \approx f(x_0) + \nabla f(x_0)^\top (x - x_0)
$$

- \( \nabla f(x_0) \): \( x_0 \)ì—ì„œì˜ gradient
- í•´ì„: **í•¨ìˆ˜ë¥¼ ì ‘í‰ë©´ìœ¼ë¡œ ê·¼ì‚¬**


## ğŸ”· 2ì°¨ í…Œì¼ëŸ¬ ê·¼ì‚¬ (Quadratic Approximation)

í•¨ìˆ˜ \( f : \mathbb{R}^d \to \mathbb{R} \)ì— ëŒ€í•´,  
ì  \( x_0 \) ê·¼ì²˜ì—ì„œì˜ 2ì°¨ í…Œì¼ëŸ¬ ì „ê°œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
f(x) \approx f(x_0) + \nabla f(x_0)^\top (x - x_0) 
+ \frac{1}{2}(x - x_0)^\top \nabla^2 f(x_0) (x - x_0)
$$

- \( \nabla^2 f(x_0) \): Hessian matrix at \( x_0 \)
- í•´ì„: **í•¨ìˆ˜ë¥¼ 2ì°¨ ê³¡ë©´(paraboloid)ìœ¼ë¡œ ê·¼ì‚¬**


## ğŸ’¡ í™œìš© ìš”ì•½

| ì°¨ìˆ˜ | ìˆ˜ì‹ í˜•íƒœ | ì˜ë¯¸ |
|------|-----------|------|
| 0ì°¨  | \( f(x_0) \) | ìƒìˆ˜ ê·¼ì‚¬ |
| 1ì°¨  | \( f(x_0) + \nabla f(x_0)^\top(x - x_0) \) | ì ‘ì„ /í‰ë©´ |
| 2ì°¨  | ìœ„ + \( \frac{1}{2}(x - x_0)^T H(x_0)(x - x_0) \) | ê³¡ë¥  í¬í•¨ ê·¼ì‚¬ |


## âœ¨ ì‹¤ì œ í™œìš© ì˜ˆì‹œ

- **ë¨¸ì‹ ëŸ¬ë‹**: ì†ì‹¤í•¨ìˆ˜ ê·¼ì‚¬, ë‰´í„´ ë°©ë²•
- **ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜**: ì ì • í•´ ê·¼ì‚¬
- **ê²½ì œí•™/í†µê³„í•™**: ë³µì¡í•œ í•¨ìˆ˜ì— ëŒ€í•œ ê·¼ì‚¬ì  ë¶„ì„


---


# ğŸ”§ Optimization: Gradient Descent
## ğŸ¯ ëª©í‘œ

ë§¤ê°œë³€ìˆ˜ \( \theta \)ì— ëŒ€í•´, ì£¼ì–´ì§„ ì†ì‹¤ í•¨ìˆ˜ \( \mathcal{J}(\theta) \)ë¥¼ **ìµœì†Œí™”**í•˜ëŠ” ë¡œì»¬ ìµœì†Ÿê°’ ì°¾ê¸°:

$$
\arg\min_{\theta} \mathcal{J}(\theta, X, y)
$$


## ğŸ“ˆ ì†ì‹¤ í•¨ìˆ˜ ì˜ˆì‹œ

- **ì„ í˜• íšŒê·€**:

$$
\mathcal{J}(\theta, X, y) = \frac{1}{2N} \sum_{n=1}^N (y_n - \theta^\top x_n)^2
$$

- **ë¡œì§€ìŠ¤í‹± íšŒê·€**:

$$
\mathcal{J}(\theta, X, y) = -\sum_{n=1}^N \left[y_n \log(\sigma(\theta^\top x_n)) + (1 - y_n) \log(1 - \sigma(\theta^\top x_n))\right]
$$

- **ë”¥ëŸ¬ë‹ ì¼ë°˜**:

$$
\mathcal{J}(\theta) = \frac{1}{2N} \sum_{n=1}^N (y_n - f(x_n; \theta))^2
$$


## ğŸ” Gradient Descent ì•Œê³ ë¦¬ì¦˜

íŒŒë¼ë¯¸í„° \( \theta \)ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë°˜ë³µì ìœ¼ë¡œ ê°±ì‹ :

$$
\theta_{k+1} = \theta_k - \alpha \cdot \left. \frac{\partial \mathcal{J}}{\partial \theta} \right|_{\theta = \theta_k}
$$

- \( \alpha > 0 \): í•™ìŠµë¥  (learning rate)
- í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ ë‚´ë ¤ê° (ìµœì†Ÿê°’ìœ¼ë¡œ ìˆ˜ë ´)


## ğŸ¨ ì‹œê°ì  ì§ê´€

- ê¸°ìš¸ê¸° < 0 â†’ ìš°ì¸¡ìœ¼ë¡œ ì´ë™  
- ê¸°ìš¸ê¸° > 0 â†’ ì¢Œì¸¡ìœ¼ë¡œ ì´ë™  
- local minimum ê·¼ì²˜ì—ì„œëŠ” ì§„ë™í•˜ê±°ë‚˜ ëŠë¦¬ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆìŒ


## ğŸ“ ìˆ˜ë ´ ì¡°ê±´: ê³ ì • step sizeì¼ ë•Œ

### Theorem:

í•¨ìˆ˜ \( \mathcal{J}(\theta) \)ê°€ convexì´ê³  gradientê°€ Lipschitz continuousë¼ë©´:

$$
\|\nabla \mathcal{J}(\theta_1) - \nabla \mathcal{J}(\theta_2)\| \leq L \|\theta_1 - \theta_2\|
$$

- ì¼ì •í•œ step size \( \alpha \leq \frac{1}{L} \)ì´ë©´, ë‹¤ìŒì´ ë³´ì¥ë¨:

$$
\mathcal{J}(\theta_k) - \mathcal{J}(\theta^*) \leq \frac{\|\theta_0 - \theta^*\|^2}{2 \alpha k}
$$

- ìˆ˜ë ´ ì†ë„: \( \mathcal{O}(1/k) \)


## ğŸ§® Proximal Regularization View (ì •ê·œí™” ê´€ì )

### 1ì°¨ ê·¼ì‚¬:

$$
\mathcal{J}(\theta) \approx \mathcal{J}(\theta_k) + \nabla \mathcal{J}(\theta_k)^\top (\theta - \theta_k)
$$

â†’ ì„ í˜• ê·¼ì‚¬ëŠ” **í•˜í•œ(bound)**ì´ ì—†ìœ¼ë¯€ë¡œ ìµœì†Œê°’ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

### ì •ê·œí™” ì¶”ê°€:

$$
\mathcal{J}(\theta_k) + \nabla \mathcal{J}(\theta_k)^\top (\theta - \theta_k) + \frac{1}{2\alpha} \|\theta - \theta_k\|^2
$$

ì´ë¥¼ ìµœì†Œí™”í•˜ë©´:

$$
\theta = \theta_k - \alpha \nabla \mathcal{J}(\theta_k)
$$

ì¦‰, **gradient descentì˜ ìˆ˜í•™ì  ìœ ë„ ë°©ì‹**


## ğŸ§ƒ Batch vs Mini-batch vs SGD

### âš™ï¸ ì „ì²´ ë°ì´í„° ê¸°ì¤€ (Full Batch):

$$
\theta_{k+1} = \theta_k - \alpha \left. \frac{\partial \mathcal{J}(\theta, X, y)}{\partial \theta} \right|_{\theta = \theta_k}
$$

- ì •í™•í•˜ì§€ë§Œ ëŠë¦¼
- ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì  (í° ë°ì´í„°ì…‹ì—ì„œ ì–´ë ¤ì›€)


### âš–ï¸ Mini-Batch Gradient Descent:

ì „ì²´ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ mini-batchë¡œ ë‚˜ëˆ„ì–´ ê° batch ê¸°ì¤€ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ê°±ì‹ :

$$
\mathcal{J}^{(m)}(\theta) = \frac{1}{M} \sum_{i=1}^{M} \mathcal{J}(\theta; x_i^{(m)}, y_i^{(m)})
$$

ì—…ë°ì´íŠ¸:

$$
\theta_{k+1} = \theta_k - \alpha \left. \frac{\partial \mathcal{J}^{(m)}(\theta)}{\partial \theta} \right|_{\theta = \theta_k}
$$


### âš¡ Stochastic Gradient Descent (SGD)

- Mini-batch ì‚¬ì´ì¦ˆ = 1
- ë§¤ìš° ë¹ ë¥´ë‚˜ **ë…¸ì´ì¦ˆ ë§ìŒ**
- ë§¤ ìŠ¤í…ë§ˆë‹¤ ë¹ ë¥´ê²Œ ë°˜ì‘í•˜ë©°, generalization ì˜ë¨


## ğŸ“Š ìˆ˜ë ´ ë¹„êµ

| ë°©ì‹        | ìˆ˜ë ´ ì†ë„ | ë©”ëª¨ë¦¬ | ë…¸ì´ì¦ˆ |
|-------------|------------|--------|--------|
| Full-Batch  | ëŠë¦¼       | ë†’ìŒ   | ì—†ìŒ   |
| Mini-Batch  | ì ë‹¹í•¨     | ì ë‹¹í•¨ | ì•½ê°„   |
| SGD         | ë¹ ë¦„       | ë§¤ìš° ì‘ìŒ | í¼     |


## ğŸ“Œ ìš”ì•½

- Gradient DescentëŠ” **ê¸°ìš¸ê¸° ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™**
- Fixed step sizeë¡œë„ ìˆ˜ë ´í•˜ë ¤ë©´ Lipschitz ì¡°ê±´ í•„ìš”
- Regularization ê´€ì ì—ì„œ GDëŠ” 1ì°¨ ê·¼ì‚¬ì˜ ì •ê·œí™” ìµœì†Œí™”
- Mini-batchì™€ SGDëŠ” ì‹¤ì œ ë”¥ëŸ¬ë‹ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë¨

---

# ğŸš€ Advanced Optimization Algorithms
## â±ï¸ Momentum

Gradient Descentì—ì„œ ê³¼ê±° ë°©í–¥ì„ ê´€ì„±ì²˜ëŸ¼ í™œìš©í•˜ì—¬ ì§„ë™ì„ ì¤„ì´ê³  ìˆ˜ë ´ì„ ë¹ ë¥´ê²Œ í•¨.

### Update Rule:

$$
v_{t} = \beta v_{t-1} + (1 - \beta)\nabla_\theta \mathcal{J}(\theta_t)  
$$

$$
\theta_{t+1} = \theta_t - \alpha v_t
$$

- \( \beta \in [0.5, 0.9] \): ê´€ì„± ê³„ìˆ˜  
- ë¹ ë¥´ê²Œ ì¢ì€ ê³¨ì§œê¸° íƒˆì¶œ ê°€ëŠ¥ (ì˜ˆ: ravines)


## ğŸŒ€ Nesterov Accelerated Gradient (NAG)

Momentumì˜ ë¯¸ë˜ê°’ì„ ë¯¸ë¦¬ ì˜ˆì¸¡í•˜ì—¬ gradientë¥¼ ì¸¡ì •.

### Update Rule:

$$
v_{t} = \beta v_{t-1} + \alpha \nabla_\theta \mathcal{J}(\theta_t - \beta v_{t-1})  
$$

$$
\theta_{t+1} = \theta_t - v_t
$$

- ê¸°ì¡´ Momentumë³´ë‹¤ overshooting ë°©ì§€
- ì‹¤ì œë¡œëŠ” ë” ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´


## ğŸ“‰ RMSProp

Gradientì˜ ì´ë™ í‰ê· ì˜ ë¶„ì‚°ìœ¼ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ì—¬ í•™ìŠµë¥  í­ì£¼ ë°©ì§€.

### Update Rule:

$$
s_t = \beta s_{t-1} + (1 - \beta)(\nabla_\theta \mathcal{J}(\theta_t))^2
$$

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{s_t + \epsilon}} \nabla_\theta \mathcal{J}(\theta_t)
$$

- \( \beta \approx 0.9 \), \( \epsilon \approx 10^{-8} \)
- ì‘ì€ gradientëŠ” ëŠë¦¬ê²Œ, í° gradientëŠ” ë¹ ë¥´ê²Œ ì§„í–‰ â†’ ì•ˆì •ì ì¸ ìˆ˜ë ´


## ğŸ§  Adam (Adaptive Moment Estimation)

Momentum + RMSPropì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜.

### Update Rule:

- 1ì°¨ ëª¨ë©˜í…€ (í‰ê· ):

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1)\nabla_\theta \mathcal{J}(\theta_t)
$$

- 2ì°¨ ëª¨ë©˜í…€ (ë¶„ì‚°):

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla_\theta \mathcal{J}(\theta_t))^2
$$

- í¸í–¥ ë³´ì •:

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

- íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸:

$$
\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

- ê¸°ë³¸ ì„¤ì •: \( \beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8} \)


## ğŸ“Š Optimizer ìš”ì•½

| ì•Œê³ ë¦¬ì¦˜      | ì¥ì                                   | ë‹¨ì                        |
|---------------|---------------------------------------|----------------------------|
| SGD           | ê°„ë‹¨, ì¼ë°˜í™” ì˜ë¨                    | ëŠë¦° ìˆ˜ë ´, ì§„ë™            |
| Momentum      | ë¹ ë¥¸ ìˆ˜ë ´, ì§„ë™ ì–µì œ                 | overshoot ê°€ëŠ¥ì„±           |
| NAG           | ì •í™•í•œ ë°©í–¥ ì˜ˆì¸¡, ì•ˆì •ì  ìˆ˜ë ´        | ì•½ê°„ ë³µì¡                  |
| RMSProp       | í•™ìŠµë¥  ì¡°ì ˆë¡œ ë¹ ë¥¸ ìˆ˜ë ´              | ìµœì ê°’ ê·¼ì²˜ì—ì„œ ì§„ë™ ê°€ëŠ¥  |
| Adam          | ìë™ ì¡°ì ˆ + ë¹ ë¥¸ ìˆ˜ë ´, ê°€ì¥ ë„ë¦¬ ì“°ì„ | overfit ê°€ëŠ¥ì„± ìˆìŒ        |


## ğŸ§© ì‹¤ì „ ì ìš© íŒ

- ë”¥ëŸ¬ë‹ì—ì„œëŠ” ëŒ€ë¶€ë¶„ **Adam**ìœ¼ë¡œ ì‹œì‘  
- **SGD + Momentum**ì€ regularization ì„±ëŠ¥ì´ ì¢‹ìŒ  
- **RMSProp**ì€ RNNì´ë‚˜ ì‹œê³„ì—´ì— ê°•ë ¥  
- í•™ìŠµë¥  \( \alpha \)ì€ ì—¬ì „íˆ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°


## ğŸ“ ì°¸ê³ 

- ë”¥ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ íŠœë‹ ì‹œ â†’ Optimizer ì™¸ì—ë„ **learning rate schedule**, **weight decay**, **batch size** ì¡°ì ˆì´ ì¤‘ìš”
- Optimizer ì„ íƒì€ ë°ì´í„°ì…‹ íŠ¹ì„±, ëª¨ë¸ êµ¬ì¡°, ëª©ì ì— ë”°ë¼ ë‹¬ë¼ì§


---


# ğŸ¯ Learning Rate Scheduling
## ğŸ”§ ì™œ Learning Rateë¥¼ ì¡°ì ˆí•´ì•¼ í•˜ë‚˜?

- ê³ ì •ëœ í•™ìŠµë¥ ì€ í›ˆë ¨ ì´ˆë°˜ì—” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ë‚˜,  
  **í›„ë°˜ë¶€ì—ëŠ” ë°œì‚°í•˜ê±°ë‚˜ ìµœì ì  ê·¼ì²˜ì—ì„œ ì§„ë™**í•  ìˆ˜ ìˆìŒ.
- í•™ìŠµë¥ ì„ **ì ì°¨ ì¤„ì´ë©´** ë” ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´ ê°€ëŠ¥.


## ğŸ“˜ ëŒ€í‘œì ì¸ Learning Rate Scheduler

### 1. Step Decay (ê³„ë‹¨ì‹ ê°ì†Œ)

$$
\alpha_t = \alpha_0 \cdot \gamma^{\left\lfloor \frac{t}{s} \right\rfloor}
$$

- ì¼ì • stepë§ˆë‹¤ í•™ìŠµë¥ ì„ \( \gamma \)ë°°ë¡œ ì¤„ì„.
- ì˜ˆ: 10 epochë§ˆë‹¤ 0.1ë°° ê°ì†Œ

> ì¥ì : ê°„ë‹¨í•˜ê³  ì§ê´€ì   
> ë‹¨ì : ë¶ˆì—°ì†ì ì¸ ë³€í™”ë¡œ ìˆ˜ë ´ ë¶ˆì•ˆì • ê°€ëŠ¥


### 2. Exponential Decay

$$
\alpha_t = \alpha_0 \cdot e^{-\lambda t}
$$

- ë§¤ epochë§ˆë‹¤ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ  
- ì—°ì†ì ì´ê³  ë¶€ë“œëŸ¬ìš´ ë³€í™”

> ì¥ì : ë” ìì—°ìŠ¤ëŸ½ê³  ì—°ì†ì ì¸ ê°ì†Œ  
> ë‹¨ì : ë„ˆë¬´ ë¹ ë¥´ê²Œ ì‘ì•„ì§€ë©´ í•™ìŠµ ë©ˆì¶¤


### 3. Cosine Annealing

$$
\alpha_t = \alpha_{min} + \frac{1}{2}(\alpha_0 - \alpha_{min}) \left(1 + \cos\left(\frac{t\pi}{T}\right)\right)
$$

- Cosine ê³¡ì„ ì²˜ëŸ¼ ê°ì†Œ â†’ ë¶€ë“œëŸ½ê³  ëŠë¦¬ê²Œ ì¤„ì–´ë“¦  
- \( T \): ì „ì²´ decay ì£¼ê¸°

> ì¥ì : ëŠë¦¬ê²Œ ì¤„ì–´ë“¤ì–´ ìˆ˜ë ´ ì•ˆì •ì„± ë†’ìŒ  
> ë‹¨ì : ì£¼ê¸° ê²°ì •ì´ í•„ìš”


### 4. Reduce on Plateau

- validation lossê°€ **ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´** í•™ìŠµë¥  ê°ì†Œ
- ì˜ˆ: 5 epoch ì—°ì†ìœ¼ë¡œ loss ê°ì†Œ X â†’ í•™ìŠµë¥  0.1ë°°

> ì¥ì : ìë™ ì¡°ì ˆ  
> ë‹¨ì : í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë§ìŒ (patience, factor ë“±)


### 5. Warm-up (ì´ˆê¸° í•™ìŠµë¥  ìƒìŠ¹)

- ì´ˆê¸° ëª‡ step ë™ì•ˆ í•™ìŠµë¥ ì„ **ì ì°¨ ì¦ê°€**ì‹œì¼œ ì•ˆì •í™”

$$
\alpha_t = \frac{t}{T_{warmup}} \cdot \alpha_0 \quad \text{for } t \leq T_{warmup}
$$

- ì´í›„ëŠ” Cosine Annealing ë˜ëŠ” Exponential Decayë¡œ ì „í™˜

> ì¥ì : ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œ ë¶ˆì•ˆì • ì´ˆê¸° ìˆ˜ë ´ ë°©ì§€  
> BERT, GPT ë“± ëŒ€ë¶€ë¶„ ì‚¬ìš©


## ğŸ§ª ì‹¤ì „ ì ìš© ì „ëµ

| ìƒí™© | ì¶”ì²œ Learning Rate |
|------|--------------------|
| ê¸°ë³¸ í•™ìŠµ | Step Decay, Reduce on Plateau |
| Fine-tuning | Cosine Annealing + Warm-up |
| ëŒ€ê·œëª¨ ëª¨ë¸ | Linear Warm-up â†’ Cosine Annealing |
| ê³ ì† ì‹¤í—˜ | Exponential Decay |


## ğŸ“ PyTorch ì˜ˆì‹œ

```python
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR

scheduler = StepLR(optimizer, step_size=10, gamma=0.1)
# or
scheduler = CosineAnnealingLR(optimizer, T_max=50)
```

## ğŸ’¡ ìš”ì•½
- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ì€ ìµœì í™”ì˜ í•µì‹¬!
- Cosine Annealing + Warm-upì€ í˜„ëŒ€ ë”¥ëŸ¬ë‹ì—ì„œ ì‚¬ì‹¤ìƒ í‘œì¤€
- overfitting ë°©ì§€ì—ë„ ë„ì›€ë¨




---

# â¹ï¸ Early Stopping & Weight Decay (L2 Regularization)
## ğŸ›‘ Early Stopping

### ğŸ“Œ ì •ì˜
> í•™ìŠµ ë„ì¤‘ **ê²€ì¦ ì„±ëŠ¥ì´ ë” ì´ìƒ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´**  
> ì¡°ê¸° ì¢…ë£Œí•˜ëŠ” ê¸°ë²•.

### ğŸ” ëª©ì 
- **ê³¼ì í•© ë°©ì§€ (Overfitting Prevention)**
- **ì‹œê°„/ìì› ì ˆì•½**


### ğŸ“ ì‘ë™ ë°©ì‹

1. ê²€ì¦ ì§€í‘œ(Validation loss ë˜ëŠ” Accuracy)ë¥¼ ëª¨ë‹ˆí„°ë§
2. `patience` ê¸°ê°„ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ í•™ìŠµ ì¢…ë£Œ

```python
# Pseudo-code
if val_loss < best_loss:
    best_loss = val_loss
    wait = 0
else:
    wait += 1
    if wait >= patience:
        stop_training()
```


### âš™ï¸ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| í•­ëª© | ì„¤ëª… |
|------|------|
| `patience` | ëª‡ epoch ë™ì•ˆ ê°œì„  ì—†ì„ ë•Œ ë©ˆì¶œì§€ |
| `monitor` | ëª¨ë‹ˆí„°ë§í•  ì§€í‘œ (`val_loss`, `val_acc`) |
| `mode` | `min` ë˜ëŠ” `max` |
| `restore_best_weights` | ë©ˆì¶”ê¸° ì§ì „ ìµœì  ê°€ì¤‘ì¹˜ë¡œ ë³µêµ¬í• ì§€ |


## ğŸ§ª PyTorch ì˜ˆì‹œ (Early Stopping êµ¬í˜„)

```python
class EarlyStopping:
    def __init__(self, patience=5):
        self.patience = patience
        self.counter = 0
        self.best_score = None

    def step(self, val_loss):
        if self.best_score is None or val_loss < self.best_score:
            self.best_score = val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                return True  # Stop training
        return False
```


## ğŸ‹ï¸ Weight Decay (L2 Regularization)

### ğŸ“Œ ì •ì˜
> ëª¨ë¸ì˜ **ê°€ì¤‘ì¹˜ í¬ê¸° ìì²´ì— íŒ¨ë„í‹°**ë¥¼ ë¶€ì—¬í•´ ê³¼ì í•© ë°©ì§€


### ğŸ“ ìˆ˜ì‹

$$
J(\theta) = \text{Loss}(\theta) + \lambda \|\theta\|_2^2
$$

- $begin:math:text$ \\lambda $end:math:text$: ê·œì œ ê°•ë„ (weight decay ê³„ìˆ˜)


### ğŸ¤– PyTorch ì ìš© ë°©ë²•

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
```

> ëŒ€ë¶€ë¶„ì˜ OptimizerëŠ” `weight_decay` íŒŒë¼ë¯¸í„° ë‚´ì¥


### ğŸ¯ ì°¨ì´ì : L2 vs Dropout vs Early Stopping

| ê¸°ë²• | ëª©ì  | ë°©ì‹ |
|------|------|------|
| L2 ê·œì œ | ëª¨ë¸ ë³µì¡ë„ ì–µì œ | ê°€ì¤‘ì¹˜ í¬ê¸° ì¤„ì„ |
| Dropout | êµ¬ì¡°ì  ê³¼ì í•© ë°©ì§€ | ì¼ë¶€ ë‰´ëŸ° ë¹„í™œì„±í™” |
| Early Stopping | ê³¼ì í•© ì‹œì  ì°¨ë‹¨ | ëª¨ë‹ˆí„°ë§ í›„ ì¢…ë£Œ |


## âœ… ìš”ì•½

- **Early Stopping**: validation loss ê¸°ì¤€ ì¡°ê¸° ì¢…ë£Œ  
- **Weight Decay**: ê³¼ë„í•œ ê°€ì¤‘ì¹˜ ì„±ì¥ ë°©ì§€  
- ë‘˜ ë‹¤ **ê³¼ì í•© ë°©ì§€ì— í•µì‹¬ì ì¸ ì—­í• **




---

# âš™ï¸ Newton's Method & Quasi-Newton

---

## ğŸ§  Newton's Method

### ğŸ“Œ ì •ì˜  
> Newtonâ€™s MethodëŠ” ëª©ì í•¨ìˆ˜ì˜ **ì´ì°¨ í…Œì¼ëŸ¬ ê·¼ì‚¬**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œê°’ì„ ì°¾ëŠ” ìµœì í™” ê¸°ë²•ì…ë‹ˆë‹¤.


### ğŸ“ Update Rule

$$
\theta_{k+1} = \theta_k - \alpha \left[ \nabla^2 J(\theta_k) \right]^{-1} \nabla J(\theta_k)
$$

- $begin:math:text$ \\alpha > 0 $end:math:text$: Step size (í•™ìŠµë¥ )
- $begin:math:text$ \\nabla^2 J(\\theta_k) $end:math:text$: í•´ì‹œì•ˆ í–‰ë ¬ (Hessian)
- **Positive definite** ì¡°ê±´ í•„ìš”


### ğŸ“˜ ìˆ˜ì‹ ìœ ë„ (2ì°¨ í…Œì¼ëŸ¬ ì „ê°œ ê¸°ë°˜)

$$
J(\theta) \approx J(\theta_k) + \nabla J(\theta_k)^T (\theta - \theta_k) + \frac{1}{2} (\theta - \theta_k)^T \nabla^2 J(\theta_k)(\theta - \theta_k)
$$

ì´ê±¸ ë¯¸ë¶„í•´ì„œ stationary point êµ¬í•˜ë©´:

$$
\nabla J(\theta_k) + \nabla^2 J(\theta_k)(\theta - \theta_k) = 0
$$

ë”°ë¼ì„œ:

$$
\theta = \theta_k - \left[ \nabla^2 J(\theta_k) \right]^{-1} \nabla J(\theta_k)
$$


### ğŸ“ˆ ìˆ˜ë ´ íŠ¹ì„±

- **Quadratic Convergence**: ì´ˆë°˜ ìˆ˜ë ´ì´ ë¹ ë¦„
- **Hessianì´ ì–‘ì˜ ì •ë¶€í˜¸ì¼ ê²½ìš°**ì—ë§Œ ìˆ˜ë ´ ë³´ì¥


## ğŸ–¼ï¸ ì‹œê°ì  ì˜ˆì‹œ

- (a) Convex í•¨ìˆ˜ â†’ ìˆ˜ë ´ ë³´ì¥  
- (b) Non-convex í•¨ìˆ˜ â†’ ìˆ˜ë ´ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ì¡´ì¬


## ğŸ§© Quasi-Newton Methods

### ğŸ“Œ ì •ì˜
> í•´ì‹œì•ˆ $begin:math:text$ \\nabla^2 J(\\theta) $end:math:text$ì„ **ì§ì ‘ ê³„ì‚°í•˜ì§€ ì•Šê³ **,  
> ì´ì „ ë‹¨ê³„ì˜ ì •ë³´ë¡œ ê·¼ì‚¬ í–‰ë ¬ $begin:math:text$ B_k $end:math:text$ë¥¼ êµ¬ì„±í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ë°©ì‹.


### ğŸ“ ì¡°ê±´

$$
B_{k+1}(s_k) = y_k \quad \text{where} \quad s_k = \theta_{k+1} - \theta_k,\quad y_k = \nabla J(\theta_{k+1}) - \nabla J(\theta_k)
$$

- **Positive-definite** ì¡°ê±´ í•„ìš”: $begin:math:text$ x^T B_k x > 0 $end:math:text$


## ğŸ”„ BFGS: Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno

### ğŸ¯ ëª©ì 

$$
\min_{B_{k+1}} \|B_{k+1} - B_k\|_F \quad \text{subject to} \quad B_{k+1}s_k = y_k
$$


### ğŸ“ ì—…ë°ì´íŠ¸ ê³µì‹ (BFGS)

$$
B_{k+1} = B_k + \frac{y_k y_k^T}{y_k^T s_k} - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}
$$

ë˜ëŠ”, **ì—­í–‰ë ¬ ë²„ì „** ì‚¬ìš©:

$$
B_{k+1}^{-1} = \left( I - \frac{s_k y_k^T}{y_k^T s_k} \right) B_k^{-1} \left( I - \frac{y_k s_k^T}{y_k^T s_k} \right) + \frac{s_k s_k^T}{y_k^T s_k}
$$


## âœ… ìš”ì•½

| ê¸°ë²• | íŠ¹ì§• | ìˆ˜ë ´ ì†ë„ | ê³„ì‚°ëŸ‰ |
|------|------|------------|--------|
| Newton's Method | ì •í™•í•œ í•´ì‹œì•ˆ ì‚¬ìš© | ë¹ ë¦„ (Quadratic) | ë¬´ê±°ì›€ |
| Quasi-Newton | ê·¼ì‚¬ í•´ì‹œì•ˆ ì‚¬ìš© | ì¤‘ê°„ | ê°€ë²¼ì›€ |
| BFGS | ëŒ€í‘œì ì¸ Quasi-Newton | ë¹ ë¦„ + íš¨ìœ¨ì  | âœ… |



---

# ğŸ§  L-BFGS & Conjugate Gradient Method
## ğŸª¶ L-BFGS (Limited-memory BFGS)

### ğŸ“Œ ì •ì˜  
> BFGSëŠ” í•´ì‹œì•ˆ ê·¼ì‚¬ë¥¼ ìœ„í•´ ì „ì²´ í–‰ë ¬ì„ ìœ ì§€í•´ì•¼ í•˜ì§€ë§Œ,  
> **L-BFGSëŠ” ìµœê·¼ $begin:math:text$ m $end:math:text$ê°œì˜ ì—…ë°ì´íŠ¸ë§Œ ì €ì¥í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì´ëŠ” ë°©ì‹**ì…ë‹ˆë‹¤.  
> íŠ¹íˆ **ê³ ì°¨ì› ìµœì í™” ë¬¸ì œ**ì—ì„œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.


### ğŸ” í•µì‹¬ ì•„ì´ë””ì–´

- **ì „ì²´ í•´ì‹œì•ˆ í–‰ë ¬ $begin:math:text$ B_k $end:math:text$ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ**
- **ìµœê·¼ $begin:math:text$ m $end:math:text$ê°œì˜ ë²¡í„° ìŒ $begin:math:text$ (s_i, y_i) $end:math:text$**ë§Œ ì‚¬ìš©í•˜ì—¬ ê·¼ì‚¬
  - $begin:math:text$ s_i = \\theta_{i+1} - \\theta_i $end:math:text$
  - $begin:math:text$ y_i = \\nabla J(\\theta_{i+1}) - \\nabla J(\\theta_i) $end:math:text$


### âš™ï¸ ì•Œê³ ë¦¬ì¦˜ ê°œìš”

1. ì´ì „ $begin:math:text$ m $end:math:text$ê°œì˜ $begin:math:text$ (s_i, y_i) $end:math:text$ ì €ì¥
2. Two-loop recursionìœ¼ë¡œ $begin:math:text$ B_k^{-1} \\nabla J(\\theta_k) $end:math:text$ ê³„ì‚°
3. ì—…ë°ì´íŠ¸:

$$
\theta_{k+1} = \theta_k - \alpha_k \cdot d_k
$$

where $begin:math:text$ d_k = B_k^{-1} \\nabla J(\\theta_k) $end:math:text$


### âœ… ì¥ì 

- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: $begin:math:text$ O(mn) $end:math:text$ ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš© (ê¸°ì¡´ BFGSëŠ” $begin:math:text$ O(n^2) $end:math:text$)
- ëŒ€ê·œëª¨ ë¬¸ì œì— ì í•©


## ğŸ§­ Conjugate Gradient Method (CG)

### ğŸ“Œ ì •ì˜  
> í•´ì‹œì•ˆì´ **symmetric positive-definite**ì¸ ê²½ìš°,  
> ìµœì†Œê°’ì„ ì°¾ê¸° ìœ„í•´ ì„ í˜• ì‹œìŠ¤í…œì„ **ë°˜ë³µì ìœ¼ë¡œ í‘¸ëŠ” ë°©ì‹**ì…ë‹ˆë‹¤.


### ğŸ” ì ìš© ì‹œì 

- ëª©ì  í•¨ìˆ˜ê°€ **2ì°¨ í•¨ìˆ˜(Quadratic Form)**:

$$
J(\theta) = \frac{1}{2} \theta^T A \theta - b^T \theta
$$


### ğŸ§® ì•Œê³ ë¦¬ì¦˜ ê°œìš”

ì´ˆê¸°ê°’ $begin:math:text$ \\theta_0 $end:math:text$, ì”ì°¨ $begin:math:text$ r_0 = b - A \\theta_0 $end:math:text$, ë°©í–¥ $begin:math:text$ p_0 = r_0 $end:math:text$

ë°˜ë³µë¬¸:

1.  
$$
\alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}
$$
2.  
$$
\theta_{k+1} = \theta_k + \alpha_k p_k
$$
3.  
$$
r_{k+1} = r_k - \alpha_k A p_k
$$
4.  
$$
\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}
$$
5.  
$$
p_{k+1} = r_{k+1} + \beta_k p_k
$$


### âœ… íŠ¹ì§•

- **Hessian ì§ì ‘ ì €ì¥ ë¶ˆí•„ìš”**
- **Memory íš¨ìœ¨ì„±**: ê³ ì°¨ì›ì— ì í•©
- **ì •í™•íˆ $begin:math:text$ n $end:math:text$íšŒ ë°˜ë³µí•˜ë©´ í•´ë¥¼ ì–»ìŒ (2ì°¨ í•¨ìˆ˜ì¼ ê²½ìš°)**


## ğŸ§® ìš”ì•½ ë¹„êµ

| ê¸°ë²• | ì €ì¥ ë¹„ìš© | Hessian í•„ìš” | ìš©ë„ |
|------|-----------|---------------|------|
| BFGS | $begin:math:text$ O(n^2) $end:math:text$ | ê·¼ì‚¬ ì €ì¥ | ì¼ë°˜ ëª©ì  |
| L-BFGS | $begin:math:text$ O(mn) $end:math:text$ | ìµœê·¼ $begin:math:text$ m $end:math:text$ê°œë§Œ ì €ì¥ | ëŒ€ê·œëª¨ ìµœì í™” |
| CG | ì ìŒ | ê³±ë§Œ í•„ìš” (explicit ì €ì¥ X) | 2ì°¨ í•¨ìˆ˜ì— ìµœì  |







