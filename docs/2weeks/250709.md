# 5일차 - 250709

## Statistical Independence (통계적 동립)

###  📘 통계적 독립 (Statistical Independence)

#### ✅ 개념

- 두 확률 변수 X와 Y가 서로 독립이라는 뜻은, 
- 한 변수가 어떤 값을 가져도 다른 변수의 확률에 영향을 주지 않는다는 것이에요.

#### ✅ 수식 조건

p(x, y) = p(x) \cdot p(y)

#### ✅ 의미하는 바
- X를 알고 있어도 Y의 확률은 그대로다:
p(y | x) = p(y)
- 반대로도 마찬가지:
p(x | y) = p(x)
- 두 변수의 합의 분산은 각 분산의 합이 된다:
\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)


### 📘 비상관성 (Uncorrelatedness)

#### ✅ 개념

- 두 변수가 상관관계가 없다는 건,
- X가 커지거나 작아진다고 해서 Y의 평균이 같이 오르내리지 않는다는 뜻이에요.

#### ✅ 수식 조건

\mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y]

즉, 두 변수의 곱에 대한 평균이
각 변수 평균의 곱과 같으면 “상관 없음”으로 간주해요.

#### 📌 관계 정리
- 독립 ⇒ 상관 없음 ✔
  -> 독립이면 무조건 상관도 없어요.
- 없음 ⇏ 독립 ✖
  -> 하지만 상관이 없다고 해서 독립이라고 볼 수는 없어요.
      (비선형 관계 등 다른 의존성이 있을 수 있음)


## Bernoulli (베르누이 분포)
- Discrete (이산) 
- 값의 종류: 0 또는 1 (Binary random variable)
- 예시: 동전 던지기 (앞/뒤), 정답/오답
- 특징: 성공확률 P에 따라 1 또는 0
- P(X = 1) = p,\quad P(X = 0) = 1 - p
![img.png](250709_1.png)


## Categorical (범주형 분포)
- Discrete (이산)
- 값의 종류: 여러 개의 정수 범주 (예: 0 ~K)
- 예시: 주사위, 과일 종류 분류, 클래스 분류
- 특징: 여러 범주 중 하나 선택

![img.png](250709_2.png)


## Gaussian(가우시안 분포)
- continuous (연속)
- 실수 전체 영역 R
- 예시: 키, 몸무게, 센서 오차, 시험 점수
- 특징: 평균과 분산으로 형태 결정


![img.png](250709_3.png)
- σ^2 : variance (분산)
- µ : mean (평균)


![img.png](250709_4.png)

- Σ : 
- µ : mean vector (평균 벡터)



### Correlation coefficient

![img.png](250709_5.png)

![img.png](250709_6.png)



### Properties of Gaussian Distribution (가우시안 분포 속성)

#### ✅ 주요 성질 요약

- 평균(mean)과 공분산(covariance)만으로 완전히 정의됨
- **중심극한정리(Central Limit Theorem)** 적용됨
- 선형 변환 후에도 여전히 Gaussian
- 주변 분포(Marginal)와 조건 분포(Conditional) 모두 Gaussian
- 공분산 행렬을 대각화(diagonalize)하는 선형 변환 존재  
  → **Whitening, Data Sphering**
- 같은 평균과 공분산을 갖는 분포 중 **최대 엔트로피(maximum entropy)** 가짐


#### ✅ 정규분포 벡터의 합

##### 두 독립 Gaussian 랜덤 벡터

- \( \mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x) \)
- \( \mathbf{y} \sim \mathcal{N}(\mu_y, \Sigma_y) \)

##### 합의 분포

- \( \mathbf{z} = \mathbf{x} + \mathbf{y} \)
- \( \mathbf{z} \sim \mathcal{N}(\mu_x + \mu_y, \Sigma_x + \Sigma_y) \)


#### ✅ 평균 계산

\[
\mathbb{E}[\mathbf{z}] = \mathbb{E}[\mathbf{x} + \mathbf{y}] = \mathbb{E}[\mathbf{x}] + \mathbb{E}[\mathbf{y}]
\]


#### ✅ 공분산 계산

\[
\begin{aligned}
\mathrm{Cov}(\mathbf{z}) 
&= \mathbb{E}[\mathbf{z} \mathbf{z}^\top] - \mathbb{E}[\mathbf{z}] \mathbb{E}[\mathbf{z}]^\top \\
&= \mathbb{E}[(\mathbf{x} + \mathbf{y})(\mathbf{x} + \mathbf{y})^\top] - (\mu_x + \mu_y)(\mu_x + \mu_y)^\top \\
&= \mathbb{E}[\mathbf{x}\mathbf{x}^\top] + \mathbb{E}[\mathbf{y}\mathbf{y}^\top] \\
&\quad + \mathbb{E}[\mathbf{x}\mathbf{y}^\top] + \mathbb{E}[\mathbf{y}\mathbf{x}^\top] - \text{cross terms} \\
&= \Sigma_x + \Sigma_y \quad \text{(독립일 때 cross term은 0)}
\end{aligned}
\]


---

# 📘 정보 이론 주제 정리 (Information Theory Topics)

- ▶️ **Information and Shannon entropy**  
  정보량과 셰넌 엔트로피

- ▶️ **Cross entropy**  
  크로스 엔트로피 (예측 분포와 실제 분포 간의 차이를 측정)

- ▶️ **Relative entropy (Kullback-Leibler divergence)**  
  상대 엔트로피 (쿨백-라이블러 발산)

- ▶️ **Kullback matching and maximum likelihood estimation**  
  쿨백 정합(KL 정합) 및 최대우도추정(MLE)

- ▶️ **Mutual information**  
  상호 정보량 (두 변수 간 공유되는 정보의 양)



## (Shannon Entropy) - 세넌 엔트로피
- Average information (평균 정보)


![img.png](250709_7.png)


### Information
![img.png](250709_8.png)


### Bit (binary digit)
![img.png](250709_9.png)


### Nat (Natural unit of information)
![img.png](250709_10.png)




