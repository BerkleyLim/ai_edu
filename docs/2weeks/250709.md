# 5ì¼ì°¨ - 250709

## Statistical Independence (í†µê³„ì  ë™ë¦½)

###  ğŸ“˜ í†µê³„ì  ë…ë¦½ (Statistical Independence)

#### âœ… ê°œë…

- ë‘ í™•ë¥  ë³€ìˆ˜ Xì™€ Yê°€ ì„œë¡œ ë…ë¦½ì´ë¼ëŠ” ëœ»ì€, 
- í•œ ë³€ìˆ˜ê°€ ì–´ë–¤ ê°’ì„ ê°€ì ¸ë„ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ í™•ë¥ ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ì—ìš”.

#### âœ… ìˆ˜ì‹ ì¡°ê±´
$$
p(x, y) = p(x) \cdot p(y)
$$

#### âœ… ì˜ë¯¸í•˜ëŠ” ë°”
- Xë¥¼ ì•Œê³  ìˆì–´ë„ Yì˜ í™•ë¥ ì€ ê·¸ëŒ€ë¡œë‹¤:
$$
p(y \mid x) = p(y)
$$
- ë°˜ëŒ€ë„ ë§ˆì°¬ê°€ì§€:
$$
p(x \mid y) = p(x)
$$
- ë‘ ë³€ìˆ˜ì˜ í•©ì˜ ë¶„ì‚°ì€ ê° ë¶„ì‚°ì˜ í•©ì´ ëœë‹¤:
$$
\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)
$$


### ğŸ“˜ ë¹„ìƒê´€ì„± (Uncorrelatedness)

#### âœ… ê°œë…

- ë‘ ë³€ìˆ˜ê°€ ìƒê´€ê´€ê³„ê°€ ì—†ë‹¤ëŠ” ê±´,
- Xê°€ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§„ë‹¤ê³  í•´ì„œ Yì˜ í‰ê· ì´ ê°™ì´ ì˜¤ë¥´ë‚´ë¦¬ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëœ»ì´ì—ìš”.

#### âœ… ìˆ˜ì‹ ì¡°ê±´

$$
\mathbb{E}[XY] = \mathbb{E}[X] \cdot \mathbb{E}[Y]
$$

ì¦‰, ë‘ ë³€ìˆ˜ì˜ ê³±ì— ëŒ€í•œ í‰ê· ì´
ê° ë³€ìˆ˜ í‰ê· ì˜ ê³±ê³¼ ê°™ìœ¼ë©´ â€œìƒê´€ ì—†ìŒâ€ìœ¼ë¡œ ê°„ì£¼í•´ìš”.

#### ğŸ“Œ ê´€ê³„ ì •ë¦¬
- ë…ë¦½ â‡’ ìƒê´€ ì—†ìŒ âœ”
  -> ë…ë¦½ì´ë©´ ë¬´ì¡°ê±´ ìƒê´€ë„ ì—†ì–´ìš”.
- ì—†ìŒ â‡ ë…ë¦½ âœ–
  -> í•˜ì§€ë§Œ ìƒê´€ì´ ì—†ë‹¤ê³  í•´ì„œ ë…ë¦½ì´ë¼ê³  ë³¼ ìˆ˜ëŠ” ì—†ì–´ìš”.
      (ë¹„ì„ í˜• ê´€ê³„ ë“± ë‹¤ë¥¸ ì˜ì¡´ì„±ì´ ìˆì„ ìˆ˜ ìˆìŒ)


## Bernoulli (ë² ë¥´ëˆ„ì´ ë¶„í¬)
- Discrete (ì´ì‚°) 
- ê°’ì˜ ì¢…ë¥˜: 0 ë˜ëŠ” 1 (Binary random variable)
- ì˜ˆì‹œ: ë™ì „ ë˜ì§€ê¸° (ì•/ë’¤), ì •ë‹µ/ì˜¤ë‹µ
- íŠ¹ì§•: ì„±ê³µí™•ë¥  Pì— ë”°ë¼ 1 ë˜ëŠ” 0
- ìˆ˜ì‹:
$$
P(X = 1) = p,\quad P(X = 0) = 1 - p
$$
![img.png](250709_1.png)


## Categorical (ë²”ì£¼í˜• ë¶„í¬)
- Discrete (ì´ì‚°)
- ê°’ì˜ ì¢…ë¥˜: ì—¬ëŸ¬ ê°œì˜ ì •ìˆ˜ ë²”ì£¼ (ì˜ˆ: 0 ~K)
- ì˜ˆì‹œ: ì£¼ì‚¬ìœ„, ê³¼ì¼ ì¢…ë¥˜ ë¶„ë¥˜, í´ë˜ìŠ¤ ë¶„ë¥˜
- íŠ¹ì§•: ì—¬ëŸ¬ ë²”ì£¼ ì¤‘ í•˜ë‚˜ ì„ íƒ

![img.png](250709_2.png)


## Gaussian(ê°€ìš°ì‹œì•ˆ ë¶„í¬)
- continuous (ì—°ì†)
- ì‹¤ìˆ˜ ì „ì²´ ì˜ì—­ R
- ì˜ˆì‹œ: í‚¤, ëª¸ë¬´ê²Œ, ì„¼ì„œ ì˜¤ì°¨, ì‹œí—˜ ì ìˆ˜
- íŠ¹ì§•: í‰ê· ê³¼ ë¶„ì‚°ìœ¼ë¡œ í˜•íƒœ ê²°ì •


![img.png](250709_3.png)
- Ïƒ^2 : variance (ë¶„ì‚°)
- Âµ : mean (í‰ê· )


![img.png](250709_4.png)

- Î£ : 
- Âµ : mean vector (í‰ê·  ë²¡í„°)



### Correlation coefficient

![img.png](250709_5.png)

![img.png](250709_6.png)



### Properties of Gaussian Distribution (ê°€ìš°ì‹œì•ˆ ë¶„í¬ ì†ì„±)

#### âœ… ì£¼ìš” ì„±ì§ˆ ìš”ì•½

- í‰ê· (mean)ê³¼ ê³µë¶„ì‚°(covariance)ë§Œìœ¼ë¡œ ì™„ì „íˆ ì •ì˜ë¨
- **ì¤‘ì‹¬ê·¹í•œì •ë¦¬(Central Limit Theorem)** ì ìš©ë¨
- ì„ í˜• ë³€í™˜ í›„ì—ë„ ì—¬ì „íˆ Gaussian
- ì£¼ë³€ ë¶„í¬(Marginal)ì™€ ì¡°ê±´ ë¶„í¬(Conditional) ëª¨ë‘ Gaussian
- ê³µë¶„ì‚° í–‰ë ¬ì„ ëŒ€ê°í™”(diagonalize)í•˜ëŠ” ì„ í˜• ë³€í™˜ ì¡´ì¬  
  â†’ **Whitening, Data Sphering**
- ê°™ì€ í‰ê· ê³¼ ê³µë¶„ì‚°ì„ ê°–ëŠ” ë¶„í¬ ì¤‘ **ìµœëŒ€ ì—”íŠ¸ë¡œí”¼(maximum entropy)** ê°€ì§


#### âœ… ì •ê·œë¶„í¬ ë²¡í„°ì˜ í•©

##### ë‘ ë…ë¦½ Gaussian ëœë¤ ë²¡í„°

- $$\mathbf{x} \sim \mathcal{N}(\mu_x, \Sigma_x)$$  
- $$\mathbf{y} \sim \mathcal{N}(\mu_y, \Sigma_y)$$  

##### í•©ì˜ ë¶„í¬
- $$\mathbf{z} = \mathbf{x} + \mathbf{y} \sim \mathcal{N}(\mu_x + \mu_y, \Sigma_x + \Sigma_y)$$


#### âœ… í‰ê·  ê³„ì‚°
$$
\mathbb{E}[\mathbf{z}] = \mathbb{E}[\mathbf{x}] + \mathbb{E}[\mathbf{y}]
$$


#### âœ… ê³µë¶„ì‚° ê³„ì‚°
$$
\begin{aligned}
\mathrm{Cov}(\mathbf{z}) &= \mathbb{E}[\mathbf{z} \mathbf{z}^\top] - \mathbb{E}[\mathbf{z}] \mathbb{E}[\mathbf{z}]^\top \\
&= \mathbb{E}[(\mathbf{x} + \mathbf{y})(\mathbf{x} + \mathbf{y})^\top] - (\mu_x + \mu_y)(\mu_x + \mu_y)^\top \\
&= \mathbb{E}[\mathbf{x}\mathbf{x}^\top] + \mathbb{E}[\mathbf{y}\mathbf{y}^\top] + \mathbb{E}[\mathbf{x}\mathbf{y}^\top] + \mathbb{E}[\mathbf{y}\mathbf{x}^\top] \\
&\quad - (\mu_x\mu_x^\top + \mu_y\mu_y^\top + \mu_x\mu_y^\top + \mu_y\mu_x^\top) \\
&= \Sigma_x + \Sigma_y \quad \text{(ë…ë¦½ì¼ ë•Œ cross termì€ 0)}
\end{aligned}
$$



---

# ğŸ“˜ ì •ë³´ ì´ë¡  ì£¼ì œ ì •ë¦¬ (Information Theory Topics)

- â–¶ï¸ **Information and Shannon entropy**  
  ì •ë³´ëŸ‰ê³¼ ì…°ë„Œ ì—”íŠ¸ë¡œí”¼

- â–¶ï¸ **Cross entropy**  
  í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (ì˜ˆì¸¡ ë¶„í¬ì™€ ì‹¤ì œ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •)

- â–¶ï¸ **Relative entropy (Kullback-Leibler divergence)**  
  ìƒëŒ€ ì—”íŠ¸ë¡œí”¼ (ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°)

- â–¶ï¸ **Kullback matching and maximum likelihood estimation**  
  ì¿¨ë°± ì •í•©(KL ì •í•©) ë° ìµœëŒ€ìš°ë„ì¶”ì •(MLE)

- â–¶ï¸ **Mutual information**  
  ìƒí˜¸ ì •ë³´ëŸ‰ (ë‘ ë³€ìˆ˜ ê°„ ê³µìœ ë˜ëŠ” ì •ë³´ì˜ ì–‘)



## (Shannon Entropy) - ì„¸ë„Œ ì—”íŠ¸ë¡œí”¼
- Average information (í‰ê·  ì •ë³´)

### âœ… ì •ì˜
$$
H(X) = -\sum_x p(x) \log p(x)
$$


![img.png](250709_7.png)


### Information
![img.png](250709_8.png)


### Bit (binary digit)
![img.png](250709_9.png)


### Nat (Natural unit of information)
![img.png](250709_10.png)



---


## ğŸ“˜ Mutual Information (ìƒí˜¸ ì •ë³´ëŸ‰)

- ë‘ í™•ë¥  ë³€ìˆ˜ X, Y ì‚¬ì´ì˜ **ì˜ì¡´ì„±**ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œ
- ì •ë³´ ì´ë¡ ì—ì„œ **ì •ë³´ ê³µìœ  ì •ë„**ë¥¼ ì˜ë¯¸

### âœ… ìˆ˜ì‹ ì •ì˜

$begin:math:display$
I(X; Y) = \\sum_{x, y} p(x, y) \\log \\frac{p(x, y)}{p(x)p(y)}
$end:math:display$

ë˜ëŠ” ì—°ì†í˜•ì—ì„œëŠ”,

$begin:math:display$
I(X; Y) = \\int \\int p(x, y) \\log \\left( \\frac{p(x, y)}{p(x)p(y)} \\right) dx\\, dy
$end:math:display$


### âœ… ì˜ë¯¸

- $begin:math:text$ I(X; Y) = 0 $end:math:text$ â‡” Xì™€ Yê°€ **ë…ë¦½**
- $begin:math:text$ I(X; Y) > 0 $end:math:text$ â‡” Xë¥¼ ì•Œë©´ Yì— ëŒ€í•´ ì¶”ê°€ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ìˆìŒ


### âœ… ì„±ì§ˆ ìš”ì•½

- í•­ìƒ 0 ì´ìƒ: $begin:math:text$ I(X; Y) \\geq 0 $end:math:text$
- ëŒ€ì¹­ì : $begin:math:text$ I(X; Y) = I(Y; X) $end:math:text$
- Mutual Informationì€ KL divergenceì˜ íŠ¹ìˆ˜í•œ í˜•íƒœ:

$begin:math:display$
I(X; Y) = D_{\\text{KL}}(p(x, y) \\| p(x)p(y))
$end:math:display$


## ğŸ“˜ Cross Entropy (êµì°¨ ì—”íŠ¸ë¡œí”¼)

- ì‹¤ì œ ë¶„í¬ $begin:math:text$ p(x) $end:math:text$ì™€ ì˜ˆì¸¡ ë¶„í¬ $begin:math:text$ q(x) $end:math:text$ ì‚¬ì´ì˜ ì°¨ì´
- ë¶„ë¥˜ ë¬¸ì œì˜ **ì†ì‹¤ í•¨ìˆ˜(Loss)**ë¡œ ìì£¼ ì‚¬ìš©

### âœ… ìˆ˜ì‹ ì •ì˜

$begin:math:display$
H(p, q) = - \\sum_x p(x) \\log q(x)
$end:math:display$

- ì‹¤ì œ ì •ë‹µ ë¶„í¬ëŠ” one-hot encodingì´ê³ , ì˜ˆì¸¡ ë¶„í¬ëŠ” softmaxì¼ ê²½ìš°ì— ë§ì´ ì‚¬ìš©ë¨


### âœ… ì˜ë¯¸

- **ì˜ˆì¸¡ì´ ì •ë‹µì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‚®ì€ ê°’**
- KL Divergenceì™€ ê´€ë ¨:

$begin:math:display$
H(p, q) = H(p) + D_{\\text{KL}}(p \\| q)
$end:math:display$

ì¦‰, cross entropyëŠ” "ì •í™•í•œ ì—”íŠ¸ë¡œí”¼ + ì¶”ê°€ë¡œ ë°œìƒí•˜ëŠ” ì˜¤ì°¨"ì˜ í•©


## ğŸ“˜ Kullback-Leibler Divergence (KL ë°œì‚°)

- ë‘ ë¶„í¬ $begin:math:text$ p(x), q(x) $end:math:text$ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì •ë³´ ì´ë¡  ì§€í‘œ
- í•œ ë¶„í¬ë¥¼ ë‹¤ë¥¸ ë¶„í¬ë¡œ ì–¼ë§ˆë‚˜ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ„

### âœ… ìˆ˜ì‹ ì •ì˜

$begin:math:display$
D_{\\text{KL}}(p \\| q) = \\sum_x p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right)
$end:math:display$


### âœ… ì„±ì§ˆ

- í•­ìƒ 0 ì´ìƒ: $begin:math:text$ D_{\\text{KL}}(p \\| q) \\geq 0 $end:math:text$
- ë¹„ëŒ€ì¹­: $begin:math:text$ D_{\\text{KL}}(p \\| q) \\neq D_{\\text{KL}}(q \\| p) $end:math:text$
- $begin:math:text$ D_{\\text{KL}}(p \\| q) = 0 $end:math:text$ â‡” $begin:math:text$ p = q $end:math:text$


## ğŸ“˜ Maximum Likelihood Estimation (MLE)

- ê´€ì¸¡ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” **ëª¨ìˆ˜ ì¶”ì • ë°©ë²•**

### âœ… ëª©í‘œ í•¨ìˆ˜

$begin:math:display$
\\theta^* = \\arg\\max_\\theta \\prod_{i=1}^n p(x_i \\mid \\theta)
$end:math:display$

ë˜ëŠ” ë¡œê·¸ ìš°ë„ (log-likelihood) ì‚¬ìš©:

$begin:math:display$
\\theta^* = \\arg\\max_\\theta \\sum_{i=1}^n \\log p(x_i \\mid \\theta)
$end:math:display$


### âœ… íŠ¹ì§•

- ê³„ì‚°ì´ ê°„ë‹¨í•˜ê³  ì§ê´€ì 
- ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ì¢‹ì€ ì¶”ì •
- Cross Entropy ìµœì†Œí™”ì™€ë„ ì—°ê²°ë¨


## ğŸ“˜ Central Limit Theorem (ì¤‘ì‹¬ê·¹í•œì •ë¦¬)

- **ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ë”ë¼ë„**, í‘œë³¸ í‰ê· ì˜ ë¶„í¬ëŠ” ì ì  **ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œì§**

### âœ… ìˆ˜ì‹ ìš”ì•½

$begin:math:display$
\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n (X_i - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)
$end:math:display$

- i.i.d. ì¡°ê±´ í•„ìš”
- í‰ê· ê³¼ ë¶„ì‚°ì´ ì¡´ì¬í•´ì•¼ í•¨

---


