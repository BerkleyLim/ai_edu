# 12ì¼ì°¨ - 250717

# ğŸ“˜ Feature (Variable) Selection ì •ë¦¬

Seungjin Choi

---

## 1. ì„ í˜• íšŒê³„ì™€ ê³¼ì í•© ë¬¸ì œ

* **ì„ í˜• íšŒê³„ ëª¨ë¸**:

  $$
  $$

y = w\_0 + w\_1 x\_1 + w\_2 x\_2 + \cdots + w\_D x\_D + \varepsilon

$$

- **ì˜ˆì¸¡ ì •í™•ë„ ë¬¸ì œ**  
- ê´€ì°°ì¹˜ ìˆ˜ $N \ll D$ â†’ ëª¨ë¸ì´ ê³¼ì í•©ë˜ì–´ ì˜ˆì¸¡ ì„±ëŠ¥ ì €í•˜
- $D > N$ì´ë©´ í•´ê°€ ë¬´í•œíˆ ë§ì•„ì§€ê³ , í…ŒìŠ¤íŒ…ì„¸íŠ¸ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ì•„ì§

- **ëª¨ë¸ í•´ì„ ê°€ëŠ¥ì„± ë¬¸ì œ**  
- ì—°ê´€ ì—†ëŠ” ë³€ìˆ˜ê°€ ë§ì„ ê²½ìš° ëª¨ë¸ í•´ì„ì´ Ã¬ \xec96bì•„ì§€ë©°
- ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ë©´ ë” ì§ê´€ì ì¸ ëª¨ë¸ì´ ê°€ëŠ¥

---

## 2. Linear Model Selection & Regularization

1. **Subset Selection**  
 - Best Subset Selection  
 - Forward Stepwise Selection  
 - Backward Stepwise Selection

2. **Regularization (Shrinkage)**  
 - Ridge Regression  
 - LASSO Regression

3. **ì°¨ì› ì¶•ì†Œ**  
 - Principal Component Regression (PCR)  
 - Partial Least Squares (PLS)

---

## 3. í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ ì¶”ì • (Test Error Estimation)

### 1) ê°„ì  ì¶”ì •
- í•™ìŠµ ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ bias ë³µì •:
- Mallowâ€™s $C_p$
- AIC (Akaike Information Criterion)
- BIC (Bayesian Information Criterion)
- Adjusted $R^2$

### 2) ì§ì ‘ ì¶”ì •
- Validation Set ë˜ëŠ” Cross Validation ì‚¬ìš©

---

## 4. Mallowâ€™s $C_p$

- ì •ì˜:
$$

C\_p = \frac{\text{SSE}}{\hat{\sigma}^2} + 2d - N

$$
$$

\text{or} \quad
C\_p = \frac{1}{N} \left( \text{SSE} + 2d \hat{\sigma}^2 \right)

$$

- ìš©ë„:
- í•™ìŠµ ì˜¤ì°¨ê°€ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ë¥¼ ê°€ì† ì¶”ì •í•˜ëŠ” ê²ƒì„ ë³µì •
- $\hat{\sigma}^2$: ì „ì²´ ë³€ìˆ˜(full model)ì—ì„œ ì¶”ì •í•œ ì˜¤ì°¨ ë²”ìˆ˜

---

## 5. AIC & BIC

- **AIC**:  
$$

\text{AIC} = -2 \log L + 2d

$$
$$

\text{(Gaussian ê°€ì • ì‹œ)} \quad
\text{AIC} = \frac{\text{SSE}}{\hat{\sigma}^2} + 2d

$$

- **BIC**:  
$$

\text{BIC} = -2 \log L + d \log N

$$

- $L$: ëª¨ë¸ì˜ likelihood ìµœëŒ€ê°’

---

## 6. $R^2$ ë° Adjusted $R^2$

- $R^2$:
$$

R^2 = 1 - \frac{\text{SSE}}{\text{SST}} = \frac{\text{SST} - \text{SSE}}{\text{SST}}

$$

- Adjusted $R^2$:
$$

\bar{R}^2 = R^2 - (1 - R^2) \cdot \frac{D}{N - D - 1}

$$
- ë³€ìˆ˜ ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚ ë•Œ $R^2$ëŠ” ë¬´ì¡°ê±´ ì¦ê°€í•˜ë¬´ë¡œ ë³µì • í•„ìš”

---

# ğŸ“Š Subset Selection Algorithms

---

## 7. Best Subset Selection

- ëª¨ë“  $2^D$ ì¡°í•©ì„ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ ì„ íƒ
- ê³„ì‚° ë¹„ìš© \ae09ì¦:
- $D=10$: 1024ê°œ
- $D=20$: ì•½ 100ë§Œ
- $D=40$: ì•½ 1ì¡° â†’ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥

### ì•Œê³ ë¦¬ì¦˜ ìš”ì•½

```
1. Null model (ë³€ìˆ˜ ì—†ìŒ)ë¡œ ì‹œì‘
2. d = 1 ~ D ì— ëŒ€í•´:
 - dê°œ ë³€ìˆ˜ ì¡°í•© ëª¨ë‘ í•™ìŠµ
 - ê·¸ ì¤‘ SSE ê°€ì¥ ë‚®ì€ ëª¨ë¸ Md ì„ íƒ
3. Md ì¤‘ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ 1ê°œ ì„ íƒ (Cp, AIC, BIC, Adjusted R^2 ê¸°ì¤€)
```

---

## 8. Forward Stepwise Selection

- ë³€ìˆ˜ ì—†ì´ ì‹œì‘ â†’ í•˜ë‚˜ì”© ì¶”ê°€
- ë§¤ ë‹¨ê³„ë§ˆë‹¤ ì„±ëŠ¥ í–¥ìƒì´ ê°€ì¥ í° ë³€ìˆ˜ë§Œ ì¶”ê°€
- ê³„ì‚°ëŸ‰: ì•½ $D^2$

### ì•Œê³ ë¦¬ì¦˜ ìš”ì•½

```
1. Null modelë¡œ ì‹œì‘
2. ë§¤ ë‹¨ê³„ë§ˆë‹¤:
 - ë‚¨ì€ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ì”© ì¶”ê°€
 - SSE ë˜ëŠ” R^2 ê°€ì¥ ê°ì†Œëœ ëª¨ë¸ ì„ íƒ
3. ìµœì¢… ì„ íƒ: Cp, AIC, BIC ë“±ìœ¼ë¡œ ê²°ì •
```

---

## 9. Backward Stepwise Selection

- Full model (ëª¨ë“  ë³€ìˆ˜ í¬í•¨)ì—ì„œ ì‹œì‘ â†’ ë³€ìˆ˜ í•˜ë‚˜ì”© ì œê±°
- ì„±ëŠ¥ì´ ê°€ì¥ ë³´ìœ ë˜ëŠ” ë³€ìˆ˜ ì œê±°

### ì•Œê³ ë¦¬ì¦˜ ìš”ì•½

```
1. Full model(MD)ë¡œ ì‹œì‘
2. ë§¤ ë‹¨ê³„ë§ˆë‹¤:
 - í•˜ë‚˜ì”© ë³€ìˆ˜ ì œê±°
 - SSE ê°€ì¥ ë‚®ì€ ëª¨ë¸ ì„ íƒ
3. ìµœì¢… ì„ íƒ: Cp, AIC, BIC, Adjusted R^2 ë“±ìœ¼ë¡œ íŒë‹¨
```

---

## ğŸ”º ì°¸ê³  ë¬¸í•™

- **Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2023)**  
*An Introduction to Statistical Learning, 2nd ed.*

- [ISLR2 Dataset ë¦¬ë·°](https://r4ds.github.io/bookclub-islr/datasets-provided-in-the-islr2-package.html)

$$
