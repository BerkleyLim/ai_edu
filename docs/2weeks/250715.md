# 10ì¼ì°¨ - 250715

# Review ë³´ê¸° - Linear Regression
![img.png](250715_1.png)

---

# ğŸ“Œ Maximum Likelihood ê´€ì ì—ì„œì˜ OLS

## ğŸ“˜ ëª¨ë¸ ê°€ì •

- ëª©í‘œ ë³€ìˆ˜ \( y_n \)ì€ ë‹¤ìŒê³¼ ê°™ì€ ì„ í˜• ëª¨ë¸ë¡œ ìƒì„±ëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤:

$$
y_n = \mathbf{w}^\top \phi(\mathbf{x}_n) + \epsilon_n
$$

- ì—¬ê¸°ì„œ:
  - \( \phi(\mathbf{x}_n) \): ì…ë ¥ \( \mathbf{x}_n \)ì— ëŒ€í•œ íŠ¹ì„± ë³€í™˜(feature transformation)
  - \( \epsilon_n \sim \mathcal{N}(0, \sigma^2) \): í‰ê·  0, ë¶„ì‚° \( \sigma^2 \)ì¸ ê°€ìš°ì‹œì•ˆ ì¡ìŒ
  - \( \mathbf{w} \): í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„° ë²¡í„°

- ë²¡í„° í˜•íƒœë¡œ í‘œí˜„í•˜ë©´:

$$
\mathbf{y} = \Phi \mathbf{w} + \boldsymbol{\epsilon}
$$

- í™•ë¥  ëª¨ë¸ ê´€ì ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë©ë‹ˆë‹¤:

$$
p(\mathbf{y} | \Phi, \mathbf{w}) = \mathcal{N}(\Phi \mathbf{w}, \sigma^2 \mathbf{I})
$$


## ğŸ“ˆ ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜ (Log-Likelihood)

- ë¡œê·¸ ê°€ëŠ¥ë„ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
\mathcal{L} = \log p(\mathbf{y} | \Phi, \mathbf{w}) = \sum_{n=1}^{N} \log p(y_n | \phi(\mathbf{x}_n), \mathbf{w})
$$

- ì´ë¥¼ ì •ë¦¬í•˜ë©´:

$$
\mathcal{L} = -\frac{N}{2} \log \sigma^2 - \frac{N}{2} \log 2\pi - \frac{1}{2\sigma^2} \mathcal{J}_{LS}
$$

- ì—¬ê¸°ì„œ \( \mathcal{J}_{LS} \): OLSì—ì„œì˜ ì œê³±í•© ì˜¤ì°¨(Sum of Squared Errors)

### ì°¸ì¡°
![img.png](250715_2.png)

### likelihoodê°€ ìˆëŠ”ë° log-likelihoodë¥¼ ë³„ë„ë¡œ ì“°ëŠ” ì´ìœ ?
![img_1.png](250715_7.png)




## ğŸ§  ìµœëŒ€ìš°ë„ ì¶”ì • (Maximum Likelihood Estimation, MLE)

- MLEëŠ” ë¡œê·¸ ê°€ëŠ¥ë„ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„° \( \mathbf{w} \)ë¥¼ ì°¾ëŠ” ê²ƒ:

$$
\mathbf{w}_{ML} = \arg\max_{\mathbf{w}} \log p(\mathbf{y} | \Phi, \mathbf{w})
$$

- ê²°ê³¼ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê²°ë¡ ì— ë„ë‹¬í•©ë‹ˆë‹¤:

$$
\mathbf{w}_{ML} = \mathbf{w}_{OLS}
$$

> âœ… ì¦‰, **ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ê°€ì •** í•˜ì—ì„œ MLEëŠ” OLS í•´ì™€ ë™ì¼í•©ë‹ˆë‹¤.


---


# ğŸ“Œ Regularization

- ëª¨ë“  ë”¥ëŸ¬ë‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ ê¹”ê³  ì‹œì‘í•œë‹¤!!
- **Ridge Regression**: $\ell_2$ norm regularization  
- **LASSO**: $\ell_1$ norm regularization


## â“ Why Regularization?

- ëª©ì : ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ë‚˜ì˜¨ê²ƒ!!

> ë³µì¡í•œ ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©(overfitting)ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> Regularizationì€ ì´ëŸ° **ê³¼ì í•© ë¬¸ì œë¥¼ ì™„í™”**í•˜ì—¬ ë” **ì¼ë°˜í™” ê°€ëŠ¥í•œ ëª¨ë¸**ì„ í•™ìŠµí•˜ê²Œ ë„ì™€ì¤ë‹ˆë‹¤.

ì˜ˆì‹œë¡œ, ê³ ì°¨ ë‹¤í•­ì‹ì„ ì‚¬ìš©í•œ ëª¨ë¸ì€ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì™„ë²½í•˜ê²Œ ë§ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œëŠ” ì˜ˆì¸¡ì´ í¬ê²Œ ë²—ì–´ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì ì ˆí•œ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ì€ ì•½ê°„ì˜ ì˜¤ì°¨ë¥¼ ê°ìˆ˜í•˜ë”ë¼ë„ ë³´ë‹¤ ë¶€ë“œëŸ½ê³  ì¼ë°˜í™”ëœ í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ê²Œ ë©ë‹ˆë‹¤.


![img.png](250715_3.png)
> Regularizationì€ í•™ìŠµëœ ëª¨ë¸ì˜ **ì¼ë°˜í™” ì„±ëŠ¥**(generalization)ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.



## âš™ï¸ Regularization ê°œë…

### ğŸ§  ëª©ì 
- ì…ë ¥ $\mathbf{x}$ì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ ì¶”ë¡ í•˜ê³ ì í•©ë‹ˆë‹¤.
- í•™ìŠµ ë°ì´í„°:  
  $$
  \mathcal{D} = \{ (\mathbf{x}_n, y_n) \}_{n=1}^{N}
  $$

### ğŸ§® ì†ì‹¤ í•¨ìˆ˜
- ì¼ë°˜ì ì¸ ì†ì‹¤ í•¨ìˆ˜: $\ell(f(\mathbf{x}_n; \mathbf{w}), y_n)$  
- ì˜ˆ: Least Squares (LS) ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\sum_{n=1}^N \ell(f(\mathbf{x}_n; \mathbf{w}), y_n) = \frac{1}{2N} \|\mathbf{y} - \Phi \mathbf{w} \|^2
$$


## ğŸ§© Regularizer ì¶”ê°€

ëª¨ë¸ì˜ **ë³µì¡ë„(complexity)** ë¥¼ ì œí•œí•˜ê¸° ìœ„í•´, ì†ì‹¤ í•¨ìˆ˜ì— íŒ¨ë„í‹° í•­ì„ ì¶”ê°€í•©ë‹ˆë‹¤:

$$
\underbrace{\sum_{n=1}^N \ell(f(\mathbf{x}_n; \mathbf{w}), y_n)}_{\text{loss}} + \lambda \underbrace{R(f)}_{\text{regularizer}}
$$

- $\lambda$: ì •ê·œí™” í•­ì˜ ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°**


## ğŸ” ëª¨ë¸ ë³µì¡ë„ì™€ ê³„ìˆ˜ ë³€í™” ì˜ˆì‹œ

- ë‹¤ìŒ ê·¸ë¦¼ì€ ëª¨ë¸ ì°¨ìˆ˜ $M$ì— ë”°ë¥¸ íšŒê·€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:

| ëª¨ë¸ ì°¨ìˆ˜ | ì˜ˆì‹œ |
|-----------|------|
| $M = 1$   | (a) ì„ í˜• íšŒê·€ (ê³¼ì†Œì í•©) |
| $M = 3$   | (b) ì ì ˆí•œ ëª¨ë¸ |
| $M = 9$   | (c) ê³¼ì í•© |
| ê³„ìˆ˜ ë³€í™” | (d) íšŒê·€ ê³„ìˆ˜ í¬ê¸°ê°€ ë§¤ìš° ì»¤ì§ |

![img.png](250715_4.png)

## ğŸ“Š íšŒê·€ ê³„ìˆ˜ ë¹„êµ (ì˜ˆì‹œ)

| ê³„ìˆ˜ | $M=0$ | $M=1$ | $M=3$ | $M=9$ |
|------|-------|--------|--------|--------|
| $w_0$ | 0.1861 | 1.0977 | 0.0880 | -8.1 |
| $w_1$ |       | -1.8913 | 9.9135 | 401.2 |
| $w_2$ |       |        | -29.8721 | -6326.3 |
| $w_3$ |       |        | 20.1642 | 49778.9 |
| $w_4$ |       |        |         | -222555.2 |
| $w_5$ |       |        |         | 599603.0 |
| $w_6$ |       |        |         | -990507.7 |
| $w_7$ |       |        |         | 980248.7 |
| $w_8$ |       |        |         | -523726.3 |
| $w_9$ |       |        |         | 122122.1 |

> ë†’ì€ ì°¨ìˆ˜ì¼ìˆ˜ë¡ ê³„ìˆ˜ì˜ í¬ê¸°ê°€ ê¸‰ê²©íˆ ì»¤ì§€ë©°, ì´ëŠ” ê³¼ì í•©(overfitting)ì˜ ì›ì¸ì´ ë©ë‹ˆë‹¤.


---


# ğŸ“˜ Ridge Regression

![img.png](250715_5.png)

- ëŒë‹¤ $ \lambda $ ê°€ ì»¤ì§ˆìˆ˜ë¡ ê°•í•œ regularization

## ğŸ“€ ì •ì˜

Ridge Regressionì€ \$\ell\_2\$ norm ì •ê·œí™”ë¥¼ ì ìš©í•œ ì„ í˜• íšŒê·€ì…ë‹ˆë‹¤.

ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 + \frac{\lambda}{2} \|\mathbf{w}\|^2
$$

í•´ë‹¹ ì‹ìœ¼ë¡œë¶€í„° í•´ëŠ” ì•„ë˜ì™€ ê°™ì´ ìœ ë„ë©ë‹ˆë‹¤:

$$
\mathbf{w}_{\text{ridge}} = \left( \Phi^\top \Phi + \lambda I \right)^{-1} \Phi^\top \mathbf{y}
$$

ë˜ëŠ” ë‹¤ìŒê²ƒê³¼ ê°™ì€ ì œì•½ì¡°ê±´ í•˜ì˜ ìµœì í™” ë¬¸ì œë¡œ í‘œí˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 \quad \text{subject to} \quad \|\mathbf{w}\|^2 \leq \delta
$$

* \$\lambda\$: ì •ê·œí™” ê°•ë„ (í¬ë©´ ì œì•½ì´ ê°•í•¨)
* \$\delta\$: \$\ell\_2\$ ì œì•½ ì¡°ê±´ì˜ ê²½ê³„

![img.png](250715_6.png)

## ğŸ“ˆ ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ ëª¨ë¸ ë³€í™”

* **$ \log\_{10} \lambda = \infty $**: ë‹¨ìˆœí•œ ì§€ì„  (\uac00ìš´ì¹˜ ê±°ì˜ 0)
* **$ \log\_{10} \lambda = -10 $**: \uc77cë°˜ì ì¸ \uacf5ì„ 
* **$ \lambda = 0 $**: ì •ê·œí™” ì—†ìŒ â†’ ê³¼ì í•© ë°œìƒ


## íŒŒë¼ë¯¸í„° ê°’ ë¹„êµ (Ridge ì ìš© ì „í›„)

| í•­ëª©       | OLS ì¶”ì •ì¹˜   | Ridge ì¶”ì •ì¹˜ |
| -------- | --------- | --------- |
| \$w\_0\$ | -8.1      | -0.2059   |
| \$w\_1\$ | 401.2     | 0.6986    |
| \$w\_2\$ | -6326.3   | -0.8355   |
| \$w\_3\$ | 49778.9   | 0.1882    |
| \$w\_4\$ | -222555.2 | -0.0394   |
| \$w\_5\$ | 599603.0  | -0.1488   |
| \$w\_6\$ | -990507.7 | -0.0717   |
| \$w\_7\$ | 980248.7  | 0.1706    |
| \$w\_8\$ | -523726.3 | -0.0984   |
| \$w\_9\$ | 122122.1  | 0.0165    |

> Ridgeë¥¼ ì ìš©í•˜ë©´ ê¸°ë³¸ê¸° ê³„ìˆ˜ ê°’ì´ ê°€ë³€í•´ì§„ë‹¤.


## í˜¸í™˜ì  ì„¤ëª…

RidgeëŠ” ë‹¤ìŒ ë‘ ê²ƒì„ ë™ì‹œì— ìµœì†Œí™”í•©ë‹ˆë‹¤:

* LS ì†ì‹¤: $ |\mathbf{y} - \Phi \mathbf{w}|^2 $
* ì •ê·œí™” í•­: $ |\mathbf{w}|^2 $

ê·¸ëŠ” **ë™ê³ ì„ (Contour)** í˜•íƒœë¡œ ì‹¤ì‹œê°„í™”ë  ìˆ˜ ìˆê³ ,\nìš©ì  ì¡°ê±´ ë‚´ë¶€ì—ì„œ ì†ì‹¤ì´ ìµœì†Œì¸ ì§€ì ì„ ì°¾ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.


![img.png](250715_8.png)

## í•™ìŠµ ì•Œê³ ë¦¬ë“œë§ (Pseudocode)

```
Algorithm 1: Ridge Regression

Input: X âˆˆ â„^{nÃ—d}, y âˆˆ â„^n, Î» âˆˆ â„

1. Initialize Î¦ â† X,  y â† y
2. Compute w â† (Î¦áµ€Î¦ + Î»I)â»Â¹ Î¦áµ€y
3. Return w
```


## íŒŒì¼ ì„¤ëª…: Prostate Cancer Data

* íƒ€ê²Ÿ: PSA(ì „ë§ì„  íŠ¹ì´ í• ì•„ì›ƒ) ìˆ˜ì¹˜ ì˜ˆì¸¡
* íŠ¹ì„±:

  * `lcavol`: log cancer volume
  * `lweight`: log prostate weight
  * `age`: age of patient
  * `lbph`: log of benign prostatic hyperplasia
  * `svi`: seminal vesicle invasion
  * `lcp`: log of capsular penetration
  * `gleason`: Gleason score
  * `pgg45`: percent of Gleason 4 or 5

í¬í•¨ ë°ì´í„° ì¡°ì§:

* [http://statweb.stanford.edu/\~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
* [https://hastie.su.domains/CASI\_files/DATA/prostate.data](https://hastie.su.domains/CASI_files/DATA/prostate.data)


## ğŸ“ˆ Regularization Path [Hastie, Tibshirani, Friedman]

* \$\lambda\$ì˜ ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ ê° ê³„ìˆ˜ \$w\_i\$ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´
* Ridge regressionì˜ ê°€ìš´ì¹˜ ì´ˆê³¼ ê²°ê³¼ ê°„ì˜ ê´€ê³„ ì‹œê°í™”

![img.png](250715_9.png)

![img.png](250715_10.png)


## ğŸ” Cross-Validation for Choosing $Î»$

![img.png](250715_11.png)

> \$\lambda\$ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ëª©í‘œì— ê´€ê°í•˜ê¸° ë•Œë¬¸ì—,
> ëŒ€ì²´ì ìœ¼ë¡œ **k-fold êµì°¨ ê²€ì‚¬**ë¡œ ì ì ˆí•œ ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.

ë³µì†Œ:

* ë°ì´í„°ë¥¼ Kê°œ ë¶„í• 
* í•˜ë‚˜ëŠ” validation, ë‚˜ë¨¸ì§€ëŠ” training
* ë‹¤ì–‘í•œ \$\lambda\$ ê°’ì— ëŒ€í•´ validation error ê³„ì‚°
* errorê°€ ê°€ì¥ ì ì€ ê°’ì„ ì„ íƒ

ê²°ê³¼:

```
Training:  1 2 3 4 5
Fold 1 â†’ Train: 2~5, Test: 1
Fold 2 â†’ Train: 1,3~5, Test: 2
...
```

ìµœì¢… ì„ íƒ: ê²€ì¦ ì˜¤ë¥˜ê°€ ìµœì†Œì¸ \$\lambda\$

---


# ğŸ“˜ LASSO (Least Absolute Shrinkage and Selection Operator)

## âœ¨ í•µì‹¬ ì•„ì´ë””ì–´

LASSOëŠ” ì•„ë˜ì˜ ëª©ì  í•¨ìˆ˜ë¡œ ì •ì˜ë©ë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 + \lambda \|\mathbf{w}\|_1
$$

ë˜ëŠ” ë‹¤ìŒ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìµœì í™” ë¬¸ì œë¡œ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 \quad \text{s.t.} \quad \|\mathbf{w}\|_1 \leq \delta
$$

* \$\ell\_1\$ ì •ê·œí™”ë¥¼ í†µí•´ í¬ì†Œì„±(sparsity)ì„ ìœ ë„í•©ë‹ˆë‹¤.

![img.png](250715_12.png)


## ğŸ” í•´ì„ì  ì¥ì 

* **ì˜ˆì¸¡ ì„±ëŠ¥**: ëª¨ë¸ ë³µì¡ë„ë¥¼ ì œí•œí•˜ì—¬ ê³¼ì í•© ë°©ì§€
* **í•´ì„ ê°€ëŠ¥ì„±**: ì¼ë¶€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë³€ìˆ˜ ì„ íƒì´ ê°€ëŠ¥í•¨

## ğŸ“ ì‹œê°ì  ì„¤ëª…

* LASSOëŠ” ì œì•½ ì¡°ê±´ì´ ë‹¤ì´ì•„ëª¬ë“œ ëª¨ì–‘ (\$\ell\_1\$ norm ball)
* ê²½ê³„ì—ì„œ ìµœì ê°’ì´ ë°œìƒ â†’ ì¼ë¶€ ê³„ìˆ˜ëŠ” ì •í™•íˆ 0ì´ ë¨

## ğŸ“ˆ Regularization Path ë¹„êµ

![img.png](250715_13.png)

* Ridge: ì—°ì†ì ìœ¼ë¡œ ìˆ˜ë ´, ëª¨ë“  ë³€ìˆ˜ í¬í•¨
* LASSO: ì¼ë¶€ ê³„ìˆ˜ê°€ ì •í™•íˆ 0ìœ¼ë¡œ ìˆ˜ë ´ â†’ ë³€ìˆ˜ ì„ íƒ ê¸°ëŠ¥ í¬í•¨

## ğŸ§® Gradient ê³„ì‚°

ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\nabla_w \left( \frac{1}{2} \|y - Xw\|^2 + \lambda \|w\|_1 \right)
$$

* \$|w|\_1\$ì€ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•œ ì§€ì  ì¡´ì¬ â†’ coordinate descent ì‚¬ìš©

## ğŸ” Coordinate Descent

ê° íŒŒë¼ë¯¸í„° \$w\_j\$ì— ëŒ€í•´ ë°˜ë³µì ìœ¼ë¡œ ìµœì í™”:

$$
w_j \leftarrow S\left( \frac{1}{N} \sum_{i=1}^N x_{ij} (y_i - \hat{y}^{(-j)}_i), \lambda \right)
$$

## ğŸ”» Soft Thresholding

$$
S(z, \lambda) = \text{sign}(z) \cdot \max(|z| - \lambda, 0)
$$

* ì…ë ¥ê°’ \$z\$ê°€ \$\lambda\$ë³´ë‹¤ ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ìˆ˜ì¶•
* í° ê°’ì€ \$\lambda\$ë§Œí¼ ê°ì†Œ

## ğŸ§® Subgradient ê³„ì‚°

\$\ell\_1\$ normì˜ ì„œë¸Œê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\partial |w_j| = \begin{cases}
1 & w_j > 0 \\
-1 & w_j < 0 \\
[-1, 1] & w_j = 0
\end{cases}
$$

ì´ ê°œë…ì„ í†µí•´ soft-thresholdingì´ ë„ì¶œë¨

## ğŸ”« Shooting Algorithm

LASSOì˜ coordinate descentëŠ” shooting ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³ ë„ ë¶ˆë¦¼

**Papers**:

* Fu, W. (1998). "Penalized regression: The bridge versus the LASSO"
* Wu and Lange (2008). "Coordinate descent algorithms for LASSO penalized regression"

Pseudocode (Shooting Algorithm):

```
Algorithm: Coordinate Descent for Sparse Regression
Input: X, y, Î», initialize w = 0
repeat until convergence:
  for j = 1 to p:
    compute partial residual r^{(j)}
    update w_j â† S( ... ) using soft-threshold
return w
```

---


# ğŸ¯ Bias-Variance Dilemma

![img.png](250715_14.png)

![img.png](250715_15.png)

## ğŸ“ Mean Squared Error (MSE)

* ì œê³± ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
\ell(y, f(x)) = (y - f(x))^2
$$

* í‰ê·  ì œê³± ì˜¤ì°¨(MSE)ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

$$
\mathbb{E}_{(x,y)} \left[ (y - f(x))^2 \right] = \int (y - f(x))^2 p(x, y) dx dy
$$

* \$y = f^\*(x) + \varepsilon\$ ìœ¼ë¡œ ê°€ì •í•˜ë©´:

$$
\mathbb{E}_{(x,y)}[(y - f(x))^2] = \underbrace{(f(x) - \mathbb{E}[y])^2}_{\text{bias}^2} + \underbrace{\text{Var}(y)}_{\text{noise}}
$$


## ğŸ§© Bias-Variance Decomposition

íŠ¹ì • \$x\$ì— ëŒ€í•œ ì†ì‹¤ì˜ ê¸°ëŒ“ê°’ì€ ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´ë©ë‹ˆë‹¤:

$$
\mathbb{E}_{\mathcal{D}} \left[ (f(x; \mathcal{D}) - y)^2 \right] = \left( \mathbb{E}_{\mathcal{D}}[f(x; \mathcal{D})] - f^*(x) \right)^2 + \mathbb{E}_{\mathcal{D}}\left[ \left( f(x; \mathcal{D}) - \mathbb{E}_{\mathcal{D}}[f(x; \mathcal{D})] \right)^2 \right] + \sigma^2
$$

* ì²« ë²ˆì§¸ í•­: bias\$^2\$ (ì˜ˆì¸¡ í‰ê· ê³¼ ì§„ì§œ í•¨ìˆ˜ \$f^\*\$ì˜ ì°¨ì´)
* ë‘ ë²ˆì§¸ í•­: variance (ëª¨ë¸ì˜ ë¶ˆì•ˆì •ì„±)
* ì„¸ ë²ˆì§¸ í•­: irreducible noise (ë°ì´í„°ì˜ ê³ ìœ  ë…¸ì´ì¦ˆ)

**ìš”ì•½:**

$$
\text{Expected loss} = \text{Bias}^2 + \text{Variance} + \text{Noise}
$$


## âš–ï¸ Bias-Variance Trade-off

* ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì¦ê°€í•˜ë©´:

  * BiasëŠ” ê°ì†Œ (ëª¨ë¸ì´ ë” ì •í™•í•˜ê²Œ ì˜ˆì¸¡)
  * VarianceëŠ” ì¦ê°€ (ëª¨ë¸ì´ ê³¼ì í•©ë¨)

* ë°˜ëŒ€ë¡œ ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ë©´:

  * BiasëŠ” ì»¤ì§
  * VarianceëŠ” ì‘ì•„ì§

> ì´ìƒì ì¸ ëª¨ë¸ì€ ì´ ë‘˜ ì‚¬ì´ì˜ ê· í˜•(Bias-Variance Trade-off)ì´ ê°€ì¥ ì˜ ë§ëŠ” ì§€ì ì„ ì°¾ëŠ” ê²ƒ


## ğŸ”¢ ì˜ˆì‹œ ì‹œë®¬ë ˆì´ì…˜

* \$N=25\$ì¸ ìƒ˜í”Œì„ ê°€ì§„ 100ê°œì˜ ë°ì´í„°ì…‹ ìƒì„±
* ì§„ì§œ í•¨ìˆ˜: \$f^\*(x) = \sin(2 \pi x)\$
* ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ 24ê°œ Gaussian basisë¡œ ëª¨ë¸ \$\hat{f}(x)\$ í•™ìŠµ

### Bias ê³„ì‚°:

$$
\text{bias}^2 = \frac{1}{n} \sum_{i=1}^n (\mathbb{E}[\hat{f}(x_i)] - f^*(x_i))^2
$$

### Variance ê³„ì‚°:

$$
\text{variance} = \frac{1}{n} \sum_{i=1}^n \mathbb{E} \left[ (\hat{f}(x_i) - \mathbb{E}[\hat{f}(x_i)])^2 \right]
$$


## ğŸ“Š ì‹œê°ì  ë¹„êµ

| ëª¨ë¸ ìœ í˜•                    | ì˜ˆì¸¡ ë¶„í¬        | Bias      | Variance      |
| ------------------------ | ------------ | --------- | ------------- |
| Rigid Model (1st row)    | ë‹¨ìˆœí•˜ê³  ë¶€ë“œëŸ¬ìš´ ì»¤ë¸Œ | High Bias | Low Variance  |
| Flexible Model (2nd row) | ë³µì¡í•˜ê³  ì„¸ì„¸í•œ ì»¤ë¸Œ  | Low Bias  | High Variance |

> ë³µì¡í•œ ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì—ëŠ” ì˜ ë§ì§€ë§Œ, ì¼ë°˜í™” ì„±ëŠ¥ì€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ

ì¶œì²˜: Bishop's PRML


---

# ğŸ” Linear Regression: Explainability

![img.png](250715_16.png)

## ğŸ“Œ Feature Importance (t-statistic)

* ê³„ìˆ˜ì˜ ì¤‘ìš”ë„ëŠ” í•´ë‹¹ ê³„ìˆ˜ì˜ **í‘œì¤€ ì˜¤ì°¨ë¡œ ì •ê·œí™”ëœ ê°’**, ì¦‰ **t-í†µê³„ëŸ‰**ìœ¼ë¡œ ì¸¡ì •í•©ë‹ˆë‹¤:

$$
t_j = \frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}
$$

* íŠ¹ì„±ì˜ ì¤‘ìš”ë„ëŠ” \$|t\_j|\$ ì˜ ì ˆëŒ“ê°’ì´ í´ìˆ˜ë¡ ì¤‘ìš”ë„ê°€ ë†’ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


## ğŸš² Example: Bike Rentals Dataset for Regression

* ëª©í‘œ: **ë‚ ì”¨ì™€ ìš”ì¼ì— ë”°ë¼ ìì „ê±° ëŒ€ì—¬ ìˆ˜ìš”ë¥¼ ì˜ˆì¸¡**
* ì‚¬ìš©ëœ íŠ¹ì„±ë“¤:

  * ìì „ê±° ìˆ˜(cnt): target ê°’
  * ê³„ì ˆ(season): ë´„, ì—¬ë¦„, ê°€ì„, ê²¨ìš¸
  * ê³µíœ´ì¼ ì—¬ë¶€
  * ì£¼ë§ ì—¬ë¶€
  * ë‚ ì§œë¡œë¶€í„°ì˜ ê²½ê³¼ ì¼ìˆ˜(days\_since\_2011)
  * ë‚ ì”¨ ì •ë³´ (good, mist, rain/snow/storm ë“±)
  * ê¸°ì˜¨(Â°C), ìŠµë„(%), í’ì†(km/h)

ë°ì´í„° ì¶œì²˜: *Molnar's IML book*


## ğŸ“Š ì˜ˆì¸¡ ì˜ˆì‹œ í•´ì„

* íŠ¹ì • ì¼ìì˜ ì„ í˜• íšŒê·€ ê³„ìˆ˜ ì˜ˆì‹œ:

| Feature                 | Weight  | SE    | t    |
| ----------------------- | ------- | ----- | ---- |
| temp                    | 110.7   | 3.4   | 32.5 |
| weather=RAIN/SNOW/STORM | -1901.5 | 282.6 | -6.7 |

* ìˆ˜ì¹˜í˜• ë³€ìˆ˜(temp): ê¸°ì˜¨ì´ 1ë„ ì˜¤ë¥´ë©´ ëŒ€ì—¬ ìˆ˜ìš”ê°€ 110.7 ì¦ê°€
* ë²”ì£¼í˜• ë³€ìˆ˜(weather): ë‚ ì”¨ê°€ ë‚˜ì  ê²½ìš° -1901.5ëª… ê°ì†Œ (ë‹¤ë¥¸ ì¡°ê±´ ë™ì¼ ì‹œ)


## ğŸ“ˆ Effect Plot: \$Effect\_{j,n} = \theta\_j x\_{j,n}\$

* ê° íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í–ˆëŠ”ì§€ë¥¼ ì‹œê°í™”
* ì˜ˆ: temp, days\_since\_2011 íŠ¹ì„±ì´ ê°€ì¥ í° ì˜í–¥ ë¯¸ì¹¨


## ğŸ§â€â™‚ï¸ Explain Individual Predictions

* íŠ¹ì • ì¸ìŠ¤í„´ìŠ¤ì˜ ì…ë ¥ ê°’:

| Feature           | Value       |
| ----------------- | ----------- |
| season            | SPRING      |
| yr                | 2011        |
| mnth              | JAN         |
| holiday           | NO HOLIDAY  |
| weekday           | THU         |
| workingday        | WORKING DAY |
| weather           | GOOD        |
| temp              | 1.6043656   |
| hum               | 51.8261     |
| windspeed         | 6.000688    |
| cnt               | 1606        |
| days\_since\_2011 | 5           |

â†’ ì˜ˆì¸¡ê°’: 4504, ì‹¤ì œê°’: 1606 â†’ ì˜¤ì°¨ê°€ í¼ (high bias ì‚¬ë¡€)


## ğŸ“ˆ Individual Effect Plot

* ê° íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ê¸°ì—¬í•œ ê°’ (ë¶‰ì€ ì„  ê¸°ì¤€ìœ¼ë¡œ ì˜í–¥ ë°©í–¥ ë° í¬ê¸° í‘œì‹œ)


## âš™ï¸ LASSO: Feature Selection ì‚¬ë¡€

### ğŸ“‰ LASSO with 2 Features

| Feature           | Weight |
| ----------------- | ------ |
| temp              | 52.33  |
| days\_since\_2011 | 2.15   |
| others            | 0.00   |

### ğŸ“Š LASSO with 5 Features

| Feature                 | Weight  |
| ----------------------- | ------- |
| seasonSPRING            | -389.99 |
| weather=RAIN/SNOW/STORM | -862.27 |
| temp                    | 85.58   |
| hum                     | -3.04   |
| days\_since\_2011       | 3.82    |

â†’ ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ì œê±°í•˜ê³  ì¤‘ìš”í•œ featureë§Œ ë‚¨ê¸°ëŠ” ë°©ì‹


ì¶œì²˜: Molnar's IML book


---


# ë²ˆì™¸: ğŸ§¬ Elastic Net

## ğŸ’¡ ì •ì˜

Elastic Netì€ Ridge(\$\ell\_2\$)ì™€ LASSO(\$\ell\_1\$)ë¥¼ ê²°í•©í•œ ì •ê·œí™” ë°©ë²•ì…ë‹ˆë‹¤:

$$
\min_{\mathbf{w}} \; \frac{1}{2} \|\mathbf{y} - \Phi \mathbf{w}\|^2 + \lambda_1 \|\mathbf{w}\|_1 + \lambda_2 \|\mathbf{w}\|_2^2
$$

* \$\lambda\_1\$: LASSO (í¬ì†Œì„± ìœ ë„)
* \$\lambda\_2\$: Ridge (ê³„ìˆ˜ ì•ˆì •í™”)

## âœ… ì¥ì 

* ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity)ì— ê°•ê±´í•¨
* ë³€ìˆ˜ ì„ íƒê³¼ ê³„ìˆ˜ ì¶•ì†Œë¥¼ ë™ì‹œì— ìˆ˜í–‰ ê°€ëŠ¥
* LASSOê°€ ë³€ìˆ˜ ìˆ˜ë³´ë‹¤ ê´€ì¸¡ì¹˜ê°€ ì ì„ ë•Œ ë¶ˆì•ˆì •í•œ ì  ë³´ì™„

## ğŸ” í‘œí˜„ ë°©ì‹ (í˜¼í•© ë¹„ìœ¨)

ë³´í†µ í•˜ë‚˜ì˜ ì •ê·œí™” ê³„ìˆ˜ \$\lambda\$ì™€ í˜¼í•© ë¹„ìœ¨ \$\alpha\$ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œí˜„:

$$
\lambda \left( \alpha \|\mathbf{w}\|_1 + (1 - \alpha) \|\mathbf{w}\|_2^2 \right)
$$

* \$\alpha = 1\$ â†’ LASSO
* \$\alpha = 0\$ â†’ Ridge
* \$0 < \alpha < 1\$ â†’ Elastic Net

## ğŸ“ˆ ì‚¬ìš© ì˜ˆì‹œ

* sklearnì˜ `ElasticNetCV`ëŠ” êµì°¨ ê²€ì¦ìœ¼ë¡œ ìµœì ì˜ \$\lambda\$, \$\alpha\$ë¥¼ ì°¾ìŒ

## ğŸ“Œ ì •ë¦¬

| ê¸°ë²•          | ì •ê·œí™” í•­                                       | íŠ¹ì§•            |
| ----------- | ------------------------------------------- | ------------- |
| Ridge       | $\|w\|\_2^2\$                               | ê³„ìˆ˜ ì „ì²´ ì¶•ì†Œ, ì•ˆì •í™” |
| LASSO       | $\|w\|\_1\$                                 | í¬ì†Œì„±, ë³€ìˆ˜ ì„ íƒ    |
| Elastic Net | \$\alpha \|w\|\_1 + (1-\alpha) \|w\|\_2^2\$ | ë‘ ì¥ì  ê²°í•©       |





