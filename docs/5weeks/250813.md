# 30일차 - 250813

# Transformer Models 강의 정리

## 1. Transformer 개요

Transformer는 **자연어 처리(NLP)**, **시계열 예측**, **이미지 처리** 등
다양한 분야에서 사용되는 딥러닝 모델입니다. - **구성**: Encoder +
Decoder - **핵심 모듈** - Positional Encoding (위치 정보 추가) -
Multi-Head Attention (다양한 시각에서 정보 추출) - Position-wise
Feedforward Network (각 위치별 비선형 변환) - **특징** - 순차 데이터
처리에서 뛰어난 성능 - RNN/LSTM보다 병렬 연산 가능 → 학습 속도 향상 -
멀리 떨어진 토큰 간의 의존성도 효율적으로 학습


## 2. Self-Attention 메커니즘

입력 시퀀스에서 **Query, Key, Value**를 모두 동일한 시퀀스로부터
생성합니다. - 각 단어(토큰)는 다른 모든 단어와의 연관성을 계산하여
새로운 표현을 만듭니다. - **수식** $$
  y_i = \sum_{j=1}^T W_{i,j} x_j
  $$ $$
  W_{i,j} = \frac{\exp(q_i k_j^\top)}{\sum_{\ell=1}^T \exp(q_i k_\ell^\top)}
  $$ - 선형 변환을 거쳐 얻은 Query, Key, Value: $$
  q_i = W_q x_i,\quad k_j = W_k x_j,\quad v_j = W_v x_j
  $$

**이해 포인트**:\
Self-Attention은 "문장 내에서 어떤 단어가 어떤 단어를 참고해야
하는가?"를 학습하는 과정입니다.


## 3. Scaled Dot-Product Attention

Query와 Key의 내적을 통해 유사도를 구하고, 이를 softmax로 확률화한 뒤
Value에 가중합을 적용합니다. - **정의** $$
  Attention(Q, K, V) = softmax\left( \frac{QK^\top}{\sqrt{D_k}} \right) V
  $$ - 스케일링(√D_k로 나누기)은 값이 너무 커져서 softmax가 기울어지는
문제를 방지합니다.


## 4. Multi-Head Attention

Self-Attention을 여러 개 병렬로 수행하여 다양한 관점에서 패턴을
학습합니다. - **정의** $$
  MultiHead(Q, K, V) = Concat(\text{head}_1, \dots, \text{head}_h) W^O
  $$ - 각 head는 다른 매개변수를 사용해 서로 다른 특성을 학습


## 5. Positional Encoding

Transformer는 순서 정보가 없으므로 위치 정보를 추가해야 합니다. -
**Sinusoidal Encoding 수식** $$
  PE_{pos,2i} = \sin\left( \frac{pos}{10000^{2i/D_{model}}} \right)
  $$ $$
  PE_{pos,2i+1} = \cos\left( \frac{pos}{10000^{2i/D_{model}}} \right)
  $$ - **특징** - 각 위치에 대해 고유한 벡터 제공 - 길이가 다른
문장에서도 상대적 거리 유지 가능 - 수학적으로 무한 길이 일반화 가능


## 6. RoPE (Rotary Position Embedding)

-   절대 위치 정보를 **회전 변환**으로 인코딩
-   상대 위치 정보까지 자연스럽게 포함
-   장점:
    1.  임의 길이 시퀀스 확장 가능
    2.  상대 거리 증가 시 의존성 점진적 감소
    3.  계산 효율성 우수


## 7. Time Series용 Transformer

-   **Encoder**: 과거 데이터를 입력받아 특징 추출
-   **Decoder**: 미래 값을 자기회귀 방식으로 예측
-   LSTM 기반보다 병렬 연산이 유리하고 긴 의존성 학습이 쉬움


## 8. Linformer

-   Self-Attention의 **O(N²)** 복잡도를 **O(N)**으로 줄이는 방법
-   Key와 Value를 저차원으로 투영 (Random Projection) $$
    \text{head}_i = Attention(QW^Q_i, E_iKW^K_i, F_iVW^V_i)
    $$


## 9. Set Transformer

-   **순서 무관(Set) 데이터**를 처리
-   **Permutation-Invariant** 특성
-   주요 모듈:
    -   SAB (Set Attention Block)
    -   ISAB (Induced SAB)
    -   PMA (Pooling by Multihead Attention)
-   모든 순열 불변 함수를 근사할 수 있는 보편 근사기


## 10. BERT

-   Encoder만 사용하는 Transformer 기반 언어 모델
-   **Pre-training**
    1.  Masked Language Model (MLM): 문장 내 일부 단어를 \[MASK\]로
        가리고 예측
    2.  Next Sentence Prediction (NSP): 두 문장이 연속인지 판별
-   **Fine-tuning**: 사전학습 후 다운스트림 태스크(분류, QA 등)에 맞게
    조정


## 11. Vision Transformer (ViT)

-   이미지를 패치 단위로 나누고 각 패치를 토큰처럼 처리
-   절차:
    1.  이미지를 P×P 패치로 분할
    2.  각 패치를 벡터로 변환 후 위치 정보 추가
    3.  Transformer Encoder에 입력
    4.  \[class\] 토큰 출력값을 최종 이미지 표현으로 사용


**요약**:\
Transformer는 "Attention"이라는 개념을 기반으로, 순서 있는 데이터뿐
아니라 순서 없는 데이터, 시계열, 이미지까지 폭넓게 활용 가능한 범용
딥러닝 아키텍처입니다.
