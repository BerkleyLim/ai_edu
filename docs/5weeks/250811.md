# 26ì¼ì°¨ - 250811

# Recurrent Neural Networks (RNN)

## 1. ì‹œí€€ìŠ¤ ë°ì´í„° ì˜ˆì‹œ

ì‹œí€€ìŠ¤: $ x_1, x_2, \dots, x_T $

- **ìŒì„± ì¸ì‹:** ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸  
  _(ì˜¤ë””ì˜¤ ì‹ í˜¸ì˜ íŒŒí˜•ì´ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ë¯€ë¡œ ìˆœì„œ ì •ë³´ ì¤‘ìš”)_
- **ê¸°ê³„ ë²ˆì—­:** í…ìŠ¤íŠ¸ â†’ í…ìŠ¤íŠ¸  
  _(ë¬¸ì¥ì˜ ì•ë’¤ ë¬¸ë§¥ì´ ë²ˆì—­ ê²°ê³¼ì— ì§ì ‘ ì˜í–¥ì„ ë¯¸ì¹¨)_
- **ê°ì„± ë¶„ì„:** í…ìŠ¤íŠ¸ â†’ ë ˆì´ë¸” (ê¸ì •, ë¶€ì •, ì¤‘ë¦½)  
  _(â€œì¢‹ë‹¤â€ë¼ëŠ” ë‹¨ì–´ ì•ë’¤ì— ë”°ë¼ ê¸/ë¶€ì • ì˜ë¯¸ê°€ ë‹¬ë¼ì§)_
- **ìŒì•… ìƒì„±:** ì´ë‹ˆì…œ â†’ ìŒì•…  
- **ë¹„ë””ì˜¤ ë¶„ë¥˜:** ë¹„ë””ì˜¤ â†’ ë ˆì´ë¸”  
  _(ì—°ì†ëœ í”„ë ˆì„ì˜ íŒ¨í„´ì„ ì¸ì‹í•´ì•¼ í•¨)_
- **ê°œì²´ëª… ì¸ì‹ (NER):** í…ìŠ¤íŠ¸ â†’ ì—”í‹°í‹° ë ˆì´ë¸”  

## 2. ì‹œí€€ìŠ¤ ëª¨ë¸ë§

ëª©í‘œ: ì‹œí€€ìŠ¤ì˜ í™•ë¥  ê³„ì‚°  
$$
P(x_1, x_2, \dots, x_T)
$$

ì˜ˆì‹œ:

- **ì˜¬ë°”ë¥¸ ë¬¸ì¥ ì„ íƒ:**  
$$
P(\text{I, am, going, home}) > P(\text{I, am, going, house})
$$
(â€œgoingâ€ ë’¤ì—ëŠ” â€œhomeâ€ì´ ë” ìì—°ìŠ¤ëŸ¬ì›€)

- **ì£¼ì‹ ì˜ˆì¸¡:**  
$$
P(100, 110, 120, 130, 140) \; ? \; \gtrless \; P(100, 110, 120, 130, 120)
$$
(ë§ˆì§€ë§‰ ê°’ì´ ë–¨ì–´ì§ˆì§€ ìœ ì§€/ìƒìŠ¹í• ì§€ ì˜ˆì¸¡)

- **ì¼ë°˜ì  ì˜ˆì¸¡ ë¬¸ì œ:**  
$$
P(x_1, x_2, x_3, x_4, \ ?)
$$

ğŸ’¡ **í¬ì¸íŠ¸:** RNNì€ ìˆœì°¨ì  í™•ë¥  ëª¨ë¸ë¡œ, í˜„ì¬ ì‹œì  ì˜ˆì¸¡ì€ ê³¼ê±° ì •ë³´ì— ì˜ì¡´.

## 3. ì…ë ¥-ì¶œë ¥ í˜•íƒœ

- **Vector to Vector:** ê³ ì • ì…ë ¥ â†’ ê³ ì • ì¶œë ¥  
- **Sequence to Sequence:** ê°€ë³€ ì…ë ¥ ê¸¸ì´ â†’ ê°€ë³€ ì¶œë ¥ ê¸¸ì´

## 4. ê°•ì˜ ëª©ì°¨

1. Vanilla RNN  
2. BPTT (Backpropagation Through Time)  
3. Bidirectional RNN  
4. LSTM  
5. í•™ìŠµ ê¸°ë²•: Teacher-forcing & Scheduled sampling  
6. Sequence-to-Sequence Learning (Encoder & Decoder)  

## 5. Vanilla RNN

### Feedforward Neural Net (ìˆœë°©í–¥ ì‹ ê²½ë§)
$$
y_t = \phi(W_{yh} h_t + b_y)  
$$
$$
h_t = \phi(W_{hx} x_t + b_h)
$$

- ì…ë ¥ ê°„ ë…ë¦½ì„± ê°€ì • (ì‹œí€€ìŠ¤ ê´€ê³„ ë°˜ì˜ ë¶ˆê°€)  
- ì…ë ¥ ê¸¸ì´ ê³ ì • í•„ìš”

- **í•œê³„**: ë…ë¦½ì„± ê°€ì •, ê³ ì • ê¸¸ì´ ì…ë ¥

### Vanilla RNN ìˆ˜ì‹

$$
y_t = \phi(V h_t + b_y)  
$$
$$
h_t = \phi(U x_t + W h_{t-1} + b_h)
$$

- **ì‹œê°„ ì¢…ì†ì„± í•™ìŠµ**: $h_t$ëŠ” ì´ì „ ìƒíƒœ $(h_{t-1})$ë¥¼ ë°˜ì˜  
- **ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥**  
- **ìˆœì„œ ì •ë³´ ìœ ì§€**  
- **íŒŒë¼ë¯¸í„° ê³µìœ **: $(U, W, V)$ëŠ” ëª¨ë“  ì‹œì ì—ì„œ ë™ì¼

**ì§ê´€**
- $(h_t)$: í˜„ì¬ê¹Œì§€ ë³¸ ì •ë³´ì˜ ìš”ì•½
- $(W h_{t-1})$: ì´ì „ ì •ë³´ì˜ ì˜í–¥
- $(U x_t)$: í˜„ì¬ ì…ë ¥ì˜ ì˜í–¥

## 6. RNNì˜ ì…ì¶œë ¥ êµ¬ì¡° ì˜ˆì‹œ

- **One-to-Many**: ì´ë¯¸ì§€ ìº¡ì…˜ ìƒì„±  
- **Many-to-One**: ê°ì„± ë¶„ì„, ì•¡í‹°ë¹„í‹° ì¸ì‹  
- **Many-to-Many**: ê°œì²´ëª… ì¸ì‹, ê¸°ê³„ ë²ˆì—­(Encoder-Decoder), NER
- **Encoder-Decoder**: ê¸°ê³„ ë²ˆì—­

## 7. BPTT (Backpropagation Through Time)

### Gradient Flow ë¬¸ì œ
- **Exploding Gradient**:  
  $(\sigma_{\max}(W) > 1)$ì¼ ë•Œ  â†’ ê¸°ìš¸ê¸° ë¬´í•œíˆ ì»¤ì§
- **Vanishing Gradient**:  
  $(\sigma_{\max}(W) < 1)$ì¼ ë•Œ   â†’ ê¸°ìš¸ê¸° ê±°ì˜ 0

### í•´ê²°ì±…

**Exploding Gradient**
- **Truncated BPTT**: ì¼ë¶€ ì‹œì ê¹Œì§€ë§Œ ì—­ì „íŒŒ
- **Gradient Clipping**: ê¸°ìš¸ê¸° í¬ê¸° ì œí•œ
- **Adaptive Optimizer**: Adam, RMSProp

**Vanishing Gradient**
- ReLU, LeakyReLU ì‚¬ìš©
- ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Identity)
- LSTM / GRU ì‚¬ìš©

### Gradient Clipping (By Norm)
$$
g \leftarrow \max\left(1, \frac{\delta}{\|g\|}\right) g
$$

$(\delta)$ : ë¯¸ë¦¬ ì •í•œ ì„ê³„ê°’

## 8. Truncated BPTT (TBPTT)
- $( k_1 )$: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê°„ê²©  
- $( k_2 )$: ì—­ì „íŒŒ ê¸¸ì´  
- ê¸´ ì‹œí€€ìŠ¤ í•™ìŠµ ì‹œ ë©”ëª¨ë¦¬/ì‹œê°„ ì ˆì•½

## 9. Bidirectional RNN
- ìˆœë°©í–¥ + ì—­ë°©í–¥ RNN ê²°í•©
- ì–‘ë°©í–¥ ë¬¸ë§¥ ì •ë³´ í™œìš©
   _(ì˜ˆ: ë‹¨ì–´ ì˜ë¯¸ íŒŒì•… ì‹œ ë‹¤ìŒ ë‹¨ì–´ ì •ë³´ë„ ë°˜ì˜)_


## 10. LSTM (Long Short-Term Memory)

ëª©í‘œ: ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ, Gradient ë¬¸ì œ ì™„í™”

### LSTM êµ¬ì¡°
- **ì…ë ¥ ê²Œì´íŠ¸** $(i_t)$  
- **ì¶œë ¥ ê²Œì´íŠ¸** $(o_t)$ 
- **ë§ê° ê²Œì´íŠ¸** $(f_t)$ 
- ì…€ ìƒíƒœ $(c_t)$ ì—…ë°ì´íŠ¸

$$
g_t = \tanh(W_g x_t + W_{gh} h_{t-1} + b_g)
$$
$$
i_t = \sigma(W_i x_t + W_{ih} h_{t-1} + b_i)
$$
$$
f_t = \sigma(W_f x_t + W_{fh} h_{t-1} + b_f)
$$
$$
o_t = \sigma(W_o x_t + W_{oh} h_{t-1} + b_o)
$$
$$
c_t = f_t \odot c_{t-1} + i_t \odot g_t
$$
$$
h_t = o_t \odot \tanh(c_t)
$$

ğŸ’¡ **í¬ì¸íŠ¸**: $(c_t)$ëŠ” ê°€ì¤‘ì¹˜ ê³± ì—†ì´ ê²Œì´íŠ¸ë§Œ ê±°ì¹˜ë¯€ë¡œ gradient ì†Œì‹¤ì´ ì™„í™”ë¨.



## 11. LSTM Autoencoder
- ì‹œí€€ìŠ¤ë¥¼ ë‚®ì€ ì°¨ì›ì˜ ë²¡í„°ë¡œ ì••ì¶• â†’ ì¬ìƒì„±
- ë¹„ì§€ë„ í•™ìŠµì— í™œìš© ê°€ëŠ¥
- ì˜ˆ: ë¹„ë””ì˜¤ í”„ë ˆì„ ì‹œí€€ìŠ¤ ì••ì¶•


## 12. í•™ìŠµ ê¸°ë²•

### Teacher Forcing
- í•™ìŠµ ì‹œ ì •ë‹µ í† í° $(y_t)$ ì‚¬ìš©
- ì¶”ë¡  ì‹œ ëª¨ë¸ ì¶œë ¥ $( \hat{y}_t )$ ì‚¬ìš©

### Scheduled Sampling
- $(y_t)$ ì‚¬ìš© í™•ë¥  $(\epsilon_i)$ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ
- ì ì§„ì  Teacher Forcing ê°ì†Œ


## 13. Attention Mechanism

### ê¸°ë³¸ ì•„ì´ë””ì–´
- ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ì‹œì (hidden state)ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬  
- ì»¨í…ìŠ¤íŠ¸ ë²¡í„° \(c_t\):
$$
c_t = \sum_{t'=1}^{T_x} \alpha_{t,t'} h_{t'}
$$


## 14. Visual Attention (ì´ë¯¸ì§€ ìº¡ì…˜)
- CNN â†’ ì´ë¯¸ì§€ Feature Map ì¶”ì¶œ $(a_1, \dots, a_L)$
- Attention RNNì´ ìœ„ì¹˜ë³„ ê°€ì¤‘ì¹˜ë¡œ ë‹¨ì–´ ìƒì„±


## 15. í˜„ì—… ì ìš© íŒ

1. **ì§§ì€ ì‹œí€€ìŠ¤**: Vanilla RNNë„ ê°€ëŠ¥  
2. **ê¸´ ì‹œí€€ìŠ¤**: LSTM/GRU í•„ìˆ˜  
3. **ì‹¤ì‹œê°„ ì²˜ë¦¬**: TBPTT + Gradient Clipping  
4. **ê³ ì • ê¸¸ì´ ì¶œë ¥**: Encoder-Decoder êµ¬ì¡°  
5. **ë©€í‹°ëª¨ë‹¬ ì…ë ¥**: CNN + RNN ê²°í•© (ì˜ˆ: ì´ë¯¸ì§€+í…ìŠ¤íŠ¸)

